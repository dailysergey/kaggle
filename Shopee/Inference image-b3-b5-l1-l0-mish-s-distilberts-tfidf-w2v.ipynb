{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bibliographic-corner",
   "metadata": {
    "papermill": {
     "duration": 0.02049,
     "end_time": "2021-05-08T10:58:41.306438",
     "exception": false,
     "start_time": "2021-05-08T10:58:41.285948",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Version changes:\n",
    "* use eca-nfnet-l1 more powerful\n",
    "* use 3 models for text [Roberta - xlma-roberta - distilbert-indonesian]\n",
    "* use 4 model for iamges [ eca-nfnet-l0, eca-nfnet-l1, efficientnet_b3, efficientnet_b5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "czech-finish",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-05-08T10:58:41.350352Z",
     "iopub.status.busy": "2021-05-08T10:58:41.349778Z",
     "iopub.status.idle": "2021-05-08T10:58:41.352798Z",
     "shell.execute_reply": "2021-05-08T10:58:41.352248Z"
    },
    "papermill": {
     "duration": 0.02682,
     "end_time": "2021-05-08T10:58:41.352918",
     "exception": false,
     "start_time": "2021-05-08T10:58:41.326098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "human-peter",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T10:58:41.396945Z",
     "iopub.status.busy": "2021-05-08T10:58:41.396348Z",
     "iopub.status.idle": "2021-05-08T10:58:50.205783Z",
     "shell.execute_reply": "2021-05-08T10:58:50.204698Z"
    },
    "papermill": {
     "duration": 8.833853,
     "end_time": "2021-05-08T10:58:50.205930",
     "exception": false,
     "start_time": "2021-05-08T10:58:41.372077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import math\n",
    "import random \n",
    "import os \n",
    "import cv2\n",
    "import timm\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "import albumentations as A \n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import transformers\n",
    "from transformers import (BertTokenizer, BertModel,DistilBertTokenizer, DistilBertModel)\n",
    "\n",
    "#torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "import gc\n",
    "import cudf\n",
    "import cuml\n",
    "import cupy\n",
    "from cuml.feature_extraction.text import TfidfVectorizer\n",
    "from cuml.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "antique-cosmetic",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T10:58:50.250611Z",
     "iopub.status.busy": "2021-05-08T10:58:50.249955Z",
     "iopub.status.idle": "2021-05-08T10:58:50.252851Z",
     "shell.execute_reply": "2021-05-08T10:58:50.252456Z"
    },
    "papermill": {
     "duration": 0.027283,
     "end_time": "2021-05-08T10:58:50.252957",
     "exception": false,
     "start_time": "2021-05-08T10:58:50.225674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    img_size = 512\n",
    "    batch_size = 12\n",
    "    seed = 2021\n",
    "\n",
    "    bert_hidden_size = 768\n",
    "    \n",
    "    device = 'cuda'\n",
    "    classes = 11014\n",
    "    \n",
    "    model_name1 = 'eca_nfnet_l1'\n",
    "    model_path1 = '../input/effb-shopee/arcface_512x512_nfnet_l0(mish)_ep15.pt'\n",
    "    \n",
    "    model_name2 = 'efficientnet_b3'\n",
    "    model_path2 = '../input/shopee-pytorch-models/arcface_512x512_eff_b3.pt'\n",
    "    \n",
    "    #model_name3 = 'dm_nfnet_f0'\n",
    "    #model_path3 = '../input/shopeepytorchselftrained/arcface_512x512_dm_nfnet_f0(mish).pt'\n",
    "\n",
    "    model_name3 = 'tf_efficientnet_b5_ns'\n",
    "    model_path3 = '../input/shopee-pytorch-models/arcface_512x512_eff_b5_.pt'\n",
    "    \n",
    "    model_name4 = 'eca_nfnet_l0'\n",
    "    model_path4 = '../input/shopee-pytorch-models/arcface_512x512_nfnet_l0 (mish).pt'\n",
    "    max_length = 30\n",
    "    scale = 30 \n",
    "    margin = 0.5\n",
    "    num_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "demonstrated-association",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T10:58:50.302664Z",
     "iopub.status.busy": "2021-05-08T10:58:50.302131Z",
     "iopub.status.idle": "2021-05-08T10:58:50.313293Z",
     "shell.execute_reply": "2021-05-08T10:58:50.312892Z"
    },
    "papermill": {
     "duration": 0.039955,
     "end_time": "2021-05-08T10:58:50.313398",
     "exception": false,
     "start_time": "2021-05-08T10:58:50.273443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this submission notebook will compute CV score, but commit notebook will not\n"
     ]
    }
   ],
   "source": [
    "NUM_WORKERS = 4\n",
    "BATCH_SIZE = 16\n",
    "SEED = 2021\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "CHECK_SUB = False\n",
    "GET_CV = False\n",
    "\n",
    "test = pd.read_csv('../input/shopee-product-matching/test.csv')\n",
    "if len(test)>3: GET_CV = False\n",
    "else: print('this submission notebook will compute CV score, but commit notebook will not')\n",
    "\n",
    "\n",
    "# INFO HOW TO LOAD \n",
    "# https://stackoverflow.com/questions/64001128/load-a-pre-trained-model-from-disk-with-huggingface-transformers\n",
    "# WATCH BELOW HOW GETTING EMBEDDING\n",
    "\n",
    "################################################# № 1 MODEL & MODEL PATH ####################################################################\n",
    "\n",
    "transformer_model_1 = '../input/sentence-transformer-models/stsb-roberta-base/0_Transformer'\n",
    "\n",
    "TEXT_MODEL_PATH_1 = '../input/best-selftrained-lang-models/roberta-base_best_loss_num_epochs_15_arcface.bin'\n",
    "\n",
    "\n",
    "################################################# № 2 MODEL & MODEL PATH ####################################################################\n",
    "\n",
    "transformer_model_2 = '../input/sentence-transformer-models/paraphrase-xlm-r-multilingual-v1/0_Transformer'\n",
    "\n",
    "TEXT_MODEL_PATH_2 = '../input/best-multilingual-model/sentence_transfomer_xlm_best_loss_num_epochs_25_arcface.bin'\n",
    "\n",
    "################################################# № 3 MODEL & MODEL PATH ####################################################################\n",
    "\n",
    "transformer_model_3 = '../input/distilbert-base-indonesian'\n",
    "\n",
    "TEXT_MODEL_PATH_3 = '../input/best-selftrained-lang-models/distilbert-base-indonesian_best_loss_num_epochs_15_arcface.bin'\n",
    "\n",
    "\n",
    "################################################ Metric Loss and its params #######################################################\n",
    "loss_module = 'arcface'#'softmax'\n",
    "scale = 30.0\n",
    "m = 0.5 \n",
    "ls_eps = 0.0\n",
    "easy_margin = False\n",
    "\n",
    "\n",
    "model_params = {\n",
    "    'n_classes':11014,\n",
    "    'model_name': transformer_model_1,\n",
    "    'use_fc':False,\n",
    "    'fc_dim':512,\n",
    "    'dropout':0.0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "affecting-information",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T10:58:50.359820Z",
     "iopub.status.busy": "2021-05-08T10:58:50.359171Z",
     "iopub.status.idle": "2021-05-08T10:58:50.361974Z",
     "shell.execute_reply": "2021-05-08T10:58:50.361576Z"
    },
    "papermill": {
     "duration": 0.028455,
     "end_time": "2021-05-08T10:58:50.362075",
     "exception": false,
     "start_time": "2021-05-08T10:58:50.333620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_dataset():\n",
    "    if GET_CV:\n",
    "        df = pd.read_csv('../input/shopee-product-matching/train.csv')\n",
    "        tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n",
    "        df['matches'] = df['label_group'].map(tmp)\n",
    "        df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n",
    "        if CHECK_SUB:\n",
    "            df = pd.concat([df, df], axis = 0)\n",
    "            df.reset_index(drop = True, inplace = True)\n",
    "        df_cu = cudf.DataFrame(df)\n",
    "        image_paths = '../input/shopee-product-matching/train_images/' + df['image']\n",
    "    else:\n",
    "        df = pd.read_csv('../input/shopee-product-matching/test.csv')\n",
    "        df_cu = cudf.DataFrame(df)\n",
    "        image_paths = '../input/shopee-product-matching/test_images/' + df['image']\n",
    "    return df, df_cu, image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "drawn-motel",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T10:58:50.414105Z",
     "iopub.status.busy": "2021-05-08T10:58:50.413517Z",
     "iopub.status.idle": "2021-05-08T10:58:57.260109Z",
     "shell.execute_reply": "2021-05-08T10:58:57.258838Z"
    },
    "papermill": {
     "duration": 6.878273,
     "end_time": "2021-05-08T10:58:57.260245",
     "exception": false,
     "start_time": "2021-05-08T10:58:50.381972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_2255846744</td>\n",
       "      <td>0006c8e5462ae52167402bac1c2e916e.jpg</td>\n",
       "      <td>ecc292392dc7687a</td>\n",
       "      <td>Edufuntoys - CHARACTER PHONE ada lampu dan mus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_3588702337</td>\n",
       "      <td>0007585c4d0f932859339129f709bfdc.jpg</td>\n",
       "      <td>e9968f60d2699e2c</td>\n",
       "      <td>(Beli 1 Free Spatula) Masker Komedo | Blackhea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_4015706929</td>\n",
       "      <td>0008377d3662e83ef44e1881af38b879.jpg</td>\n",
       "      <td>ba81c17e3581cabe</td>\n",
       "      <td>READY Lemonilo Mie instant sehat kuah dan goreng</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        posting_id                                 image       image_phash  \\\n",
       "0  test_2255846744  0006c8e5462ae52167402bac1c2e916e.jpg  ecc292392dc7687a   \n",
       "1  test_3588702337  0007585c4d0f932859339129f709bfdc.jpg  e9968f60d2699e2c   \n",
       "2  test_4015706929  0008377d3662e83ef44e1881af38b879.jpg  ba81c17e3581cabe   \n",
       "\n",
       "                                               title  \n",
       "0  Edufuntoys - CHARACTER PHONE ada lampu dan mus...  \n",
       "1  (Beli 1 Free Spatula) Masker Komedo | Blackhea...  \n",
       "2   READY Lemonilo Mie instant sehat kuah dan goreng  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df,df_cu,image_paths = read_dataset()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-coaching",
   "metadata": {
    "papermill": {
     "duration": 0.020376,
     "end_time": "2021-05-08T10:58:57.301201",
     "exception": false,
     "start_time": "2021-05-08T10:58:57.280825",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# [Word2Vec](https://www.kaggle.com/medvedew/shopee-only-w2v-cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "honest-extreme",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-05-08T10:58:57.354280Z",
     "iopub.status.busy": "2021-05-08T10:58:57.349558Z",
     "iopub.status.idle": "2021-05-08T10:58:58.728890Z",
     "shell.execute_reply": "2021-05-08T10:58:58.728148Z"
    },
    "papermill": {
     "duration": 1.407553,
     "end_time": "2021-05-08T10:58:58.729017",
     "exception": false,
     "start_time": "2021-05-08T10:58:57.321464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncorpus = tokenize_corpus(list(df[\\'title\\']))\\n\\nmodel = Word2Vec(\\n        sentences=corpus,\\n        vector_size=200, \\n        window=20, \\n        min_count=1, \\n        sg=1, #skip-gram\\n        negative=10, \\n        epochs=1000, \\n        seed=SEED,\\n        workers=10\\n        )\\n\\ndef plot_vectors(vectors, labels, how=\\'tsne\\', ax=None):\\n    if how == \\'tsne\\':\\n        projections = TSNE().fit_transform(vectors)\\n    elif how == \\'svd\\':\\n        projections = TruncatedSVD().fit_transform(vectors)\\n    x = projections[:, 0]\\n    y = projections[:, 1]\\n    ax.scatter(x, y)\\n    for cur_x, cur_y, cur_label in zip(x, y, labels):\\n        ax.annotate(cur_label, (cur_x, cur_y))\\n        \\ndef n_grams(ngram, data):\\n    freq_dict = defaultdict(int)\\n    for text in data:\\n        tokens = [w for w in text.lower().split() if w != \" \" if w not in stopwords]\\n        ngrams = zip(*[tokens[i:] for i in range(ngram)])\\n        list_grams = [\" \".join(ngram) for ngram in ngrams]\\n        for word in list_grams:\\n            freq_dict[word] += 1\\n    df_ngram =  pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])   \\n    df_ngram.columns = [\"word\", \"wordcount\"]\\n    return df_ngram \\n\\ndf_3_grams = n_grams(3, df[\\'title\\']) \\nprint(df_3_grams.head(20))\\n\\ntest_words = [\\'jam\\', \\'tangan\\', \\'wanita\\', \\'xiaomi\\',\\'redmi\\',\\'note\\', \\'somebymi\\', \\'yuja\\', \\'niacin\\', \\'100\\', \\'ml\\']\\ngensim_words = [w for w in test_words if w in model.wv.index_to_key]\\ngensim_vectors = np.stack([model.wv[w] for w in gensim_words])\\n\\nfig, ax = plt.subplots()\\nfig.set_size_inches((10, 10))\\nplot_vectors(gensim_vectors, test_words, how=\\'svd\\', ax=ax)\\n\\n\\ndel train\\ndel test \\ndel corpus\\ngc.collect()\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import gc\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "import random\n",
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from collections import defaultdict\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "\n",
    "TOKEN_RE = re.compile(r'[\\w]+')\n",
    "\n",
    "def tokenize_text_simple_regex(txt, min_token_size=2):\n",
    "    txt = str(txt).lower()\n",
    "    all_tokens = TOKEN_RE.findall(txt)\n",
    "    return [wordnet_lemmatizer.lemmatize(token, pos=\"v\") for token in all_tokens if len(token) >= min_token_size]\n",
    "\n",
    "def tokenize_corpus(texts, tokenizer=tokenize_text_simple_regex, **tokenizer_kwargs):\n",
    "    return [tokenizer(text, **tokenizer_kwargs) for text in texts]\n",
    "'''\n",
    "corpus = tokenize_corpus(list(df['title']))\n",
    "\n",
    "model = Word2Vec(\n",
    "        sentences=corpus,\n",
    "        vector_size=200, \n",
    "        window=20, \n",
    "        min_count=1, \n",
    "        sg=1, #skip-gram\n",
    "        negative=10, \n",
    "        epochs=1000, \n",
    "        seed=SEED,\n",
    "        workers=10\n",
    "        )\n",
    "\n",
    "def plot_vectors(vectors, labels, how='tsne', ax=None):\n",
    "    if how == 'tsne':\n",
    "        projections = TSNE().fit_transform(vectors)\n",
    "    elif how == 'svd':\n",
    "        projections = TruncatedSVD().fit_transform(vectors)\n",
    "    x = projections[:, 0]\n",
    "    y = projections[:, 1]\n",
    "    ax.scatter(x, y)\n",
    "    for cur_x, cur_y, cur_label in zip(x, y, labels):\n",
    "        ax.annotate(cur_label, (cur_x, cur_y))\n",
    "        \n",
    "def n_grams(ngram, data):\n",
    "    freq_dict = defaultdict(int)\n",
    "    for text in data:\n",
    "        tokens = [w for w in text.lower().split() if w != \" \" if w not in stopwords]\n",
    "        ngrams = zip(*[tokens[i:] for i in range(ngram)])\n",
    "        list_grams = [\" \".join(ngram) for ngram in ngrams]\n",
    "        for word in list_grams:\n",
    "            freq_dict[word] += 1\n",
    "    df_ngram =  pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])   \n",
    "    df_ngram.columns = [\"word\", \"wordcount\"]\n",
    "    return df_ngram \n",
    "\n",
    "df_3_grams = n_grams(3, df['title']) \n",
    "print(df_3_grams.head(20))\n",
    "\n",
    "test_words = ['jam', 'tangan', 'wanita', 'xiaomi','redmi','note', 'somebymi', 'yuja', 'niacin', '100', 'ml']\n",
    "gensim_words = [w for w in test_words if w in model.wv.index_to_key]\n",
    "gensim_vectors = np.stack([model.wv[w] for w in gensim_words])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches((10, 10))\n",
    "plot_vectors(gensim_vectors, test_words, how='svd', ax=ax)\n",
    "\n",
    "\n",
    "del train\n",
    "del test \n",
    "del corpus\n",
    "gc.collect()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "textile-lease",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T10:58:58.778191Z",
     "iopub.status.busy": "2021-05-08T10:58:58.776599Z",
     "iopub.status.idle": "2021-05-08T10:58:58.778861Z",
     "shell.execute_reply": "2021-05-08T10:58:58.779262Z"
    },
    "papermill": {
     "duration": 0.02916,
     "end_time": "2021-05-08T10:58:58.779382",
     "exception": false,
     "start_time": "2021-05-08T10:58:58.750222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    len_y_pred = y_pred.apply(lambda x: len(x)).values\n",
    "    len_y_true = y_true.apply(lambda x: len(x)).values\n",
    "    f1 = 2 * intersection / (len_y_pred + len_y_true)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "toxic-prisoner",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T10:58:58.826071Z",
     "iopub.status.busy": "2021-05-08T10:58:58.825447Z",
     "iopub.status.idle": "2021-05-08T10:58:58.830744Z",
     "shell.execute_reply": "2021-05-08T10:58:58.830303Z"
    },
    "papermill": {
     "duration": 0.030726,
     "end_time": "2021-05-08T10:58:58.830853",
     "exception": false,
     "start_time": "2021-05-08T10:58:58.800127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_torch(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "structured-exclusion",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T10:58:58.879035Z",
     "iopub.status.busy": "2021-05-08T10:58:58.877232Z",
     "iopub.status.idle": "2021-05-08T10:58:58.879643Z",
     "shell.execute_reply": "2021-05-08T10:58:58.880063Z"
    },
    "papermill": {
     "duration": 0.028091,
     "end_time": "2021-05-08T10:58:58.880180",
     "exception": false,
     "start_time": "2021-05-08T10:58:58.852089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combine_predictions(row):\n",
    "    x = np.concatenate([row['image_predictions'], row['text_predictions'],row['text_predictions_sbert'],row['text_predictions_w2v']])\n",
    "    return ' '.join( np.unique(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "further-whole",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T10:58:58.929187Z",
     "iopub.status.busy": "2021-05-08T10:58:58.928682Z",
     "iopub.status.idle": "2021-05-08T10:58:58.932428Z",
     "shell.execute_reply": "2021-05-08T10:58:58.932003Z"
    },
    "papermill": {
     "duration": 0.031236,
     "end_time": "2021-05-08T10:58:58.932532",
     "exception": false,
     "start_time": "2021-05-08T10:58:58.901296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_image_predictions(df, embeddings,threshold = 0.0):\n",
    "    \n",
    "    if len(df) > 3:\n",
    "        KNN = 50\n",
    "    else : \n",
    "        KNN = 3\n",
    "    \n",
    "    model = NearestNeighbors(n_neighbors = KNN, metric = 'cosine')\n",
    "    model.fit(embeddings)\n",
    "    distances, indices = model.kneighbors(embeddings)\n",
    "    \n",
    "    predictions = []\n",
    "    for k in tqdm(range(embeddings.shape[0])):\n",
    "        idx = np.where(distances[k,] < threshold)[0]\n",
    "        ids = indices[k,idx]\n",
    "        posting_ids = df['posting_id'].iloc[ids].values\n",
    "        predictions.append(posting_ids)\n",
    "        \n",
    "    del model, distances, indices\n",
    "    gc.collect()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "obvious-respondent",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T10:58:58.978893Z",
     "iopub.status.busy": "2021-05-08T10:58:58.978337Z",
     "iopub.status.idle": "2021-05-08T10:58:58.981945Z",
     "shell.execute_reply": "2021-05-08T10:58:58.982305Z"
    },
    "papermill": {
     "duration": 0.028757,
     "end_time": "2021-05-08T10:58:58.982436",
     "exception": false,
     "start_time": "2021-05-08T10:58:58.953679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_test_transforms():\n",
    "\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(CFG.img_size,CFG.img_size,always_apply=True),\n",
    "            A.Normalize(),\n",
    "        ToTensorV2(p=1.0)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "spoken-dining",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T10:58:59.032757Z",
     "iopub.status.busy": "2021-05-08T10:58:59.031114Z",
     "iopub.status.idle": "2021-05-08T10:58:59.033382Z",
     "shell.execute_reply": "2021-05-08T10:58:59.033796Z"
    },
    "papermill": {
     "duration": 0.030047,
     "end_time": "2021-05-08T10:58:59.033908",
     "exception": false,
     "start_time": "2021-05-08T10:58:59.003861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    \n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def fetch_loss():\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "medieval-celebrity",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T10:58:59.087131Z",
     "iopub.status.busy": "2021-05-08T10:58:59.085811Z",
     "iopub.status.idle": "2021-05-08T10:58:59.088152Z",
     "shell.execute_reply": "2021-05-08T10:58:59.088604Z"
    },
    "papermill": {
     "duration": 0.033409,
     "end_time": "2021-05-08T10:58:59.088718",
     "exception": false,
     "start_time": "2021-05-08T10:58:59.055309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ShopeeTextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv):\n",
    "        self.csv = csv.reset_index()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.csv.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.csv.iloc[index]\n",
    "        \n",
    "        text = row.title\n",
    "        \n",
    "        text = TOKENIZER(text,\n",
    "                         return_attention_mask=False,\n",
    "                         return_token_type_ids=False,\n",
    "                         padding='max_length',\n",
    "                         truncation=True,\n",
    "                         max_length=64)\n",
    "        \n",
    "        input_ids = text['input_ids']\n",
    "        \n",
    "        input_ids,labels = self.prepare_mlm_input_and_labels(np.array(input_ids))\n",
    "\n",
    "        input_ids = torch.tensor(input_ids,dtype=torch.long)\n",
    "        labels = torch.tensor(labels,dtype=torch.long)\n",
    "    \n",
    "        return input_ids,labels\n",
    "    \n",
    "    def prepare_mlm_input_and_labels(self,X):\n",
    "        # 15% BERT masking\n",
    "        inp_mask = np.random.rand(*X.shape)<0.15 \n",
    "        # do not mask special tokens\n",
    "        inp_mask[X<=2] = False\n",
    "        # set targets to -1 by default, it means ignore\n",
    "        labels = -100 * np.ones(X.shape, dtype=int)\n",
    "        # set labels for masked tokens\n",
    "        labels[inp_mask] = X[inp_mask]\n",
    "        \n",
    "        # prepare input\n",
    "        X_mlm = np.copy(X)\n",
    "        # set input to [MASK] which is the last token for the 90% of tokens\n",
    "        # this means leaving 10% unchanged\n",
    "        inp_mask_2mask = inp_mask  & (np.random.rand(*X.shape)<0.90)\n",
    "        X_mlm[inp_mask_2mask] = mask_tok\n",
    "\n",
    "        # set 10% to a random token\n",
    "        inp_mask_2random = inp_mask_2mask  & (np.random.rand(*X.shape) < 1/9)\n",
    "        X_mlm[inp_mask_2random] = np.random.randint(3, CONFIG.vocab_size, inp_mask_2random.sum())\n",
    "\n",
    "        return X_mlm, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "driven-pearl",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T10:58:59.138133Z",
     "iopub.status.busy": "2021-05-08T10:58:59.136827Z",
     "iopub.status.idle": "2021-05-08T10:58:59.139282Z",
     "shell.execute_reply": "2021-05-08T10:58:59.139705Z"
    },
    "papermill": {
     "duration": 0.029689,
     "end_time": "2021-05-08T10:58:59.139820",
     "exception": false,
     "start_time": "2021-05-08T10:58:59.110131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ShopeeDatasetText(Dataset):\n",
    "    def __init__(self, csv):\n",
    "        self.csv = csv.reset_index()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.csv.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.csv.iloc[index]\n",
    "        \n",
    "        text = row.title\n",
    "        \n",
    "        text = TOKENIZER(text, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "        input_ids = text['input_ids'][0]\n",
    "        attention_mask = text['attention_mask'][0]  \n",
    "        \n",
    "        return input_ids, attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "collected-compilation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T10:58:59.188596Z",
     "iopub.status.busy": "2021-05-08T10:58:59.188013Z",
     "iopub.status.idle": "2021-05-08T10:58:59.191691Z",
     "shell.execute_reply": "2021-05-08T10:58:59.191278Z"
    },
    "papermill": {
     "duration": 0.030414,
     "end_time": "2021-05-08T10:58:59.191794",
     "exception": false,
     "start_time": "2021-05-08T10:58:59.161380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ShopeeDataset(Dataset):\n",
    "    def __init__(self, image_paths, transforms=None):\n",
    "\n",
    "        self.image_paths = image_paths\n",
    "        self.augmentations = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.image_paths.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.augmentations:\n",
    "            augmented = self.augmentations(image=image)\n",
    "            image = augmented['image']       \n",
    "    \n",
    "        return image,torch.tensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "conditional-better",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T10:58:59.244365Z",
     "iopub.status.busy": "2021-05-08T10:58:59.243849Z",
     "iopub.status.idle": "2021-05-08T10:58:59.247872Z",
     "shell.execute_reply": "2021-05-08T10:58:59.247434Z"
    },
    "papermill": {
     "duration": 0.034765,
     "end_time": "2021-05-08T10:58:59.247970",
     "exception": false,
     "start_time": "2021-05-08T10:58:59.213205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ShopeeNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_classes,\n",
    "                 model_name='bert-base-uncased',\n",
    "                 use_fc=False,\n",
    "                 fc_dim=512,\n",
    "                 dropout=0.0):\n",
    "        \"\"\"\n",
    "        :param n_classes:\n",
    "        :param model_name: name of model from pretrainedmodels\n",
    "            e.g. resnet50, resnext101_32x4d, pnasnet5large\n",
    "        :param pooling: One of ('SPoC', 'MAC', 'RMAC', 'GeM', 'Rpool', 'Flatten', 'CompactBilinearPooling')\n",
    "        :param loss_module: One of ('arcface', 'cosface', 'softmax')\n",
    "        \"\"\"\n",
    "        super(ShopeeNet, self).__init__()\n",
    "\n",
    "        self.transformer = transformers.AutoModel.from_pretrained(model_name)\n",
    "        final_in_features = self.transformer.config.hidden_size\n",
    "        \n",
    "        self.use_fc = use_fc\n",
    "    \n",
    "        if use_fc:\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "            self.fc = nn.Linear(final_in_features, fc_dim)\n",
    "            self.bn = nn.BatchNorm1d(fc_dim)\n",
    "            self._init_params()\n",
    "            final_in_features = fc_dim\n",
    "\n",
    "\n",
    "    def _init_params(self):\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "        nn.init.constant_(self.fc.bias, 0)\n",
    "        nn.init.constant_(self.bn.weight, 1)\n",
    "        nn.init.constant_(self.bn.bias, 0)\n",
    "\n",
    "    def forward(self, input_ids,attention_mask):\n",
    "        feature = self.extract_feat(input_ids,attention_mask)\n",
    "        return F.normalize(feature)\n",
    "\n",
    "    def extract_feat(self, input_ids,attention_mask):\n",
    "        x = self.transformer(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        \n",
    "        features = x[0]\n",
    "        features = features[:,0,:]\n",
    "\n",
    "        if self.use_fc:\n",
    "            features = self.dropout(features)\n",
    "            features = self.fc(features)\n",
    "            features = self.bn(features)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "becoming-metro",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T10:58:59.305990Z",
     "iopub.status.busy": "2021-05-08T10:58:59.304646Z",
     "iopub.status.idle": "2021-05-08T10:58:59.307063Z",
     "shell.execute_reply": "2021-05-08T10:58:59.307491Z"
    },
    "papermill": {
     "duration": 0.037996,
     "end_time": "2021-05-08T10:58:59.307606",
     "exception": false,
     "start_time": "2021-05-08T10:58:59.269610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_neighbours_cos_sim(df,embeddings):\n",
    "    '''\n",
    "    When using cos_sim use normalized features else use normal features\n",
    "    '''\n",
    "    embeddings = cupy.array(embeddings)\n",
    "    \n",
    "    if GET_CV:\n",
    "        thresholds = list(np.arange(0.65,0.95,0.05))\n",
    "\n",
    "        scores = []\n",
    "        for threshold in thresholds:\n",
    "            \n",
    "################################################# Code for Getting Preds #########################################\n",
    "            preds = []\n",
    "            CHUNK = 1024*4\n",
    "\n",
    "            print('Finding similar titles...for threshold :',threshold)\n",
    "            CTS = len(embeddings)//CHUNK\n",
    "            if len(embeddings)%CHUNK!=0: CTS += 1\n",
    "\n",
    "            for j in range( CTS ):\n",
    "                a = j*CHUNK\n",
    "                b = (j+1)*CHUNK\n",
    "                b = min(b,len(embeddings))\n",
    "\n",
    "                cts = cupy.matmul(embeddings,embeddings[a:b].T).T\n",
    "\n",
    "                for k in range(b-a):\n",
    "                    IDX = cupy.where(cts[k,]>threshold)[0]\n",
    "                    o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n",
    "                    o = ' '.join(o)\n",
    "                    preds.append(o)\n",
    "######################################################################################################################\n",
    "            df['pred_matches'] = preds\n",
    "            df['f1'] = f1_score(df['matches'], df['pred_matches'])\n",
    "            score = df['f1'].mean()\n",
    "            print(f'Our f1 score for threshold {threshold} is {score}')\n",
    "            scores.append(score)\n",
    "            \n",
    "        thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n",
    "        max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n",
    "        best_threshold = max_score['thresholds'].values[0]\n",
    "        best_score = max_score['scores'].values[0]\n",
    "        print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n",
    "            \n",
    "    else:\n",
    "        preds = []\n",
    "        CHUNK = 1024*4\n",
    "        threshold = 0.85\n",
    "\n",
    "        print('Finding similar texts...for threshold :',threshold)\n",
    "        CTS = len(embeddings)//CHUNK\n",
    "        if len(embeddings)%CHUNK!=0: CTS += 1\n",
    "\n",
    "        for j in range( CTS ):\n",
    "            a = j*CHUNK\n",
    "            b = (j+1)*CHUNK\n",
    "            b = min(b,len(embeddings))\n",
    "            print('chunk',a,'to',b)\n",
    "\n",
    "            cts = cupy.matmul(embeddings,embeddings[a:b].T).T\n",
    "\n",
    "            for k in range(b-a):\n",
    "                IDX = cupy.where(cts[k,]>threshold)[0]\n",
    "                o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n",
    "                preds.append(o)\n",
    "                    \n",
    "    return df, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "worst-raising",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T10:58:59.382323Z",
     "iopub.status.busy": "2021-05-08T10:58:59.363987Z",
     "iopub.status.idle": "2021-05-08T10:58:59.384802Z",
     "shell.execute_reply": "2021-05-08T10:58:59.384369Z"
    },
    "papermill": {
     "duration": 0.055584,
     "end_time": "2021-05-08T10:58:59.384899",
     "exception": false,
     "start_time": "2021-05-08T10:58:59.329315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "credit : https://github.com/HuangYG123/CurricularFace/blob/8b2f47318117995aa05490c05b455b113489917e/head/metrics.py#L70\n",
    "'''\n",
    "\n",
    "def l2_norm(input, axis = 1):\n",
    "    norm = torch.norm(input, 2, axis, True)\n",
    "    output = torch.div(input, norm)\n",
    "\n",
    "    return output\n",
    "\n",
    "class CurricularFace(nn.Module):\n",
    "    def __init__(self, in_features, out_features, s = 30, m = 0.50):\n",
    "        super(CurricularFace, self).__init__()\n",
    "\n",
    "        print('Using Curricular Face')\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.m = m\n",
    "        self.s = s\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.threshold = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "        self.kernel = nn.Parameter(torch.Tensor(in_features, out_features))\n",
    "        self.register_buffer('t', torch.zeros(1))\n",
    "        nn.init.normal_(self.kernel, std=0.01)\n",
    "\n",
    "    def forward(self, embbedings, label):\n",
    "        embbedings = l2_norm(embbedings, axis = 1)\n",
    "        kernel_norm = l2_norm(self.kernel, axis = 0)\n",
    "        cos_theta = torch.mm(embbedings, kernel_norm)\n",
    "        cos_theta = cos_theta.clamp(-1, 1)  # for numerical stability\n",
    "        with torch.no_grad():\n",
    "            origin_cos = cos_theta.clone()\n",
    "        target_logit = cos_theta[torch.arange(0, embbedings.size(0)), label].view(-1, 1)\n",
    "\n",
    "        sin_theta = torch.sqrt(1.0 - torch.pow(target_logit, 2))\n",
    "        cos_theta_m = target_logit * self.cos_m - sin_theta * self.sin_m #cos(target+margin)\n",
    "        mask = cos_theta > cos_theta_m\n",
    "        final_target_logit = torch.where(target_logit > self.threshold, cos_theta_m, target_logit - self.mm)\n",
    "\n",
    "        hard_example = cos_theta[mask]\n",
    "        with torch.no_grad():\n",
    "            self.t = target_logit.mean() * 0.01 + (1 - 0.01) * self.t\n",
    "        cos_theta[mask] = hard_example * (self.t + hard_example)\n",
    "        cos_theta.scatter_(1, label.view(-1, 1).long(), final_target_logit)\n",
    "        output = cos_theta * self.s\n",
    "        return output, nn.CrossEntropyLoss()(output,label)\n",
    "\n",
    "class ArcMarginProduct(nn.Module):\n",
    "    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.scale = scale\n",
    "        self.margin = margin\n",
    "        self.ls_eps = ls_eps  # label smoothing\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(margin)\n",
    "        self.sin_m = math.sin(margin)\n",
    "        self.th = math.cos(math.pi - margin)\n",
    "        self.mm = math.sin(math.pi - margin) * margin\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        # --------------------------- convert label to one-hot ---------------------------\n",
    "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
    "        one_hot = torch.zeros(cosine.size(), device='cuda')\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n",
    "        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.scale\n",
    "\n",
    "        return output\n",
    "\n",
    "class ShopeeModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, model_name,\n",
    "        n_classes = CFG.classes,\n",
    "        fc_dim = 512,\n",
    "        margin = CFG.margin,\n",
    "        scale = CFG.scale,\n",
    "        use_fc = True,\n",
    "        pretrained = False):\n",
    "\n",
    "\n",
    "        super(ShopeeModel,self).__init__()\n",
    "        print('Building Model Backbone for {} model'.format(model_name))\n",
    "\n",
    "        if model_name == \"curricular_face_eca_nfnet_l1\":\n",
    "            self.backbone = timm.create_model(\"eca_nfnet_l1\", pretrained=pretrained)\n",
    "        else:\n",
    "            self.backbone = timm.create_model(model_name, pretrained=pretrained)\n",
    "\n",
    "        if model_name == 'resnext50_32x4d':\n",
    "            final_in_features = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "\n",
    "        elif model_name == 'efficientnet_b3':\n",
    "            final_in_features = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "\n",
    "        elif model_name == 'tf_efficientnet_b5_ns':\n",
    "            final_in_features = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "        \n",
    "        elif \"nfnet\" in model_name:\n",
    "            final_in_features = self.backbone.head.fc.in_features\n",
    "            self.backbone.head.fc = nn.Identity()\n",
    "            self.backbone.head.global_pool = nn.Identity()\n",
    "            \n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self.use_fc = use_fc\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.0)\n",
    "        self.fc = nn.Linear(final_in_features, fc_dim)\n",
    "        self.bn = nn.BatchNorm1d(fc_dim)\n",
    "        self._init_params()\n",
    "        final_in_features = fc_dim\n",
    "        \n",
    "        if model_name == \"curricular_face_eca_nfnet_l1\":\n",
    "            self.final = CurricularFace(final_in_features, \n",
    "                                   n_classes, \n",
    "                                   s=scale, \n",
    "                                   m=margin)\n",
    "        else:\n",
    "            self.final = ArcMarginProduct(\n",
    "                final_in_features,\n",
    "                n_classes,\n",
    "                scale = scale,\n",
    "                margin = margin,\n",
    "                easy_margin = False,\n",
    "                ls_eps = 0.0\n",
    "            )\n",
    "\n",
    "    def _init_params(self):\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "        nn.init.constant_(self.fc.bias, 0)\n",
    "        nn.init.constant_(self.bn.weight, 1)\n",
    "        nn.init.constant_(self.bn.bias, 0)\n",
    "\n",
    "    def forward(self, image, label):\n",
    "        feature = self.extract_feat(image)\n",
    "        #logits = self.final(feature,label)\n",
    "        return feature\n",
    "\n",
    "    def extract_feat(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.backbone(x)\n",
    "        x = self.pooling(x).view(batch_size, -1)\n",
    "\n",
    "        if self.use_fc:\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc(x)\n",
    "            x = self.bn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dietary-liability",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T10:58:59.438124Z",
     "iopub.status.busy": "2021-05-08T10:58:59.437463Z",
     "iopub.status.idle": "2021-05-08T10:58:59.439928Z",
     "shell.execute_reply": "2021-05-08T10:58:59.440321Z"
    },
    "papermill": {
     "duration": 0.03353,
     "end_time": "2021-05-08T10:58:59.440450",
     "exception": false,
     "start_time": "2021-05-08T10:58:59.406920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Mish_func(torch.autograd.Function):\n",
    "    \n",
    "    \"\"\"from: https://github.com/tyunist/memory_efficient_mish_swish/blob/master/mish.py\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = i * torch.tanh(F.softplus(i))\n",
    "        ctx.save_for_backward(i)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        i = ctx.saved_variables[0]\n",
    "  \n",
    "        v = 1. + i.exp()\n",
    "        h = v.log() \n",
    "        grad_gh = 1./h.cosh().pow_(2) \n",
    "\n",
    "        # Note that grad_hv * grad_vx = sigmoid(x)\n",
    "        #grad_hv = 1./v  \n",
    "        #grad_vx = i.exp()\n",
    "        \n",
    "        grad_hx = i.sigmoid()\n",
    "\n",
    "        grad_gx = grad_gh *  grad_hx #grad_hv * grad_vx \n",
    "        \n",
    "        grad_f =  torch.tanh(F.softplus(i)) + i * grad_gx \n",
    "        \n",
    "        return grad_output * grad_f \n",
    "\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    def forward(self, input_tensor):\n",
    "        return Mish_func.apply(input_tensor)\n",
    "\n",
    "\n",
    "def replace_activations(model, existing_layer, new_layer):\n",
    "    \n",
    "    \"\"\"A function for replacing existing activation layers\"\"\"\n",
    "    \n",
    "    for name, module in reversed(model._modules.items()):\n",
    "        if len(list(module.children())) > 0:\n",
    "            model._modules[name] = replace_activations(module, existing_layer, new_layer)\n",
    "\n",
    "        if type(module) == existing_layer:\n",
    "            layer_old = module\n",
    "            layer_new = new_layer\n",
    "            model._modules[name] = layer_new\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "brilliant-citizenship",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T10:58:59.492665Z",
     "iopub.status.busy": "2021-05-08T10:58:59.491947Z",
     "iopub.status.idle": "2021-05-08T10:58:59.494621Z",
     "shell.execute_reply": "2021-05-08T10:58:59.494202Z"
    },
    "papermill": {
     "duration": 0.032099,
     "end_time": "2021-05-08T10:58:59.494724",
     "exception": false,
     "start_time": "2021-05-08T10:58:59.462625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_image_embeddings(image_paths, model_name, model_path, is_mish = False):\n",
    "    embeds = []\n",
    "    \n",
    "    model = ShopeeModel(model_name=model_name)\n",
    "    model.eval()\n",
    "    \n",
    "    if is_mish:\n",
    "        model = replace_activations(model, torch.nn.SiLU, Mish())    \n",
    "\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model = model.to(CFG.device)\n",
    "    \n",
    "\n",
    "    image_dataset = ShopeeDataset(image_paths=image_paths,transforms=get_test_transforms())\n",
    "    image_loader = torch.utils.data.DataLoader(\n",
    "        image_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img,label in tqdm(image_loader): \n",
    "            img = img.cuda()\n",
    "            label = label.cuda()\n",
    "            feat = model(img,label)\n",
    "            image_embeddings = feat.detach().cpu().numpy()\n",
    "            embeds.append(image_embeddings)\n",
    "    \n",
    "    \n",
    "    del model\n",
    "    image_embeddings = np.concatenate(embeds)\n",
    "    print(f'Our image embeddings shape is {image_embeddings.shape}')\n",
    "    del embeds\n",
    "    gc.collect()\n",
    "    return image_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "generic-record",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T10:58:59.551160Z",
     "iopub.status.busy": "2021-05-08T10:58:59.550497Z",
     "iopub.status.idle": "2021-05-08T10:58:59.553271Z",
     "shell.execute_reply": "2021-05-08T10:58:59.552764Z"
    },
    "papermill": {
     "duration": 0.036499,
     "end_time": "2021-05-08T10:58:59.553369",
     "exception": false,
     "start_time": "2021-05-08T10:58:59.516870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ShopeeNetExtend(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_classes,\n",
    "                 model_name='bert-base-uncased',\n",
    "                 pooling='mean_pooling',\n",
    "                 use_fc=False,\n",
    "                 fc_dim=512,\n",
    "                 dropout=0.0,\n",
    "                 loss_module='softmax',\n",
    "                 scale=30.0,\n",
    "                 margin=0.50,\n",
    "                 ls_eps=0.0,\n",
    "                 theta_zero=0.785):\n",
    "        \"\"\"\n",
    "        :param n_classes:\n",
    "        :param model_name: name of model from pretrainedmodels\n",
    "            e.g. resnet50, resnext101_32x4d, pnasnet5large\n",
    "        :param pooling: One of ('SPoC', 'MAC', 'RMAC', 'GeM', 'Rpool', 'Flatten', 'CompactBilinearPooling')\n",
    "        :param loss_module: One of ('arcface', 'cosface', 'softmax')\n",
    "        \"\"\"\n",
    "        super(ShopeeNetExtend, self).__init__()\n",
    "\n",
    "        self.transformer = transformers.AutoModel.from_pretrained(model_name)\n",
    "        final_in_features = self.transformer.config.hidden_size\n",
    "        \n",
    "        self.pooling = pooling\n",
    "        self.use_fc = use_fc\n",
    "    \n",
    "        if use_fc:\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "            self.fc = nn.Linear(final_in_features, fc_dim)\n",
    "            self.bn = nn.BatchNorm1d(fc_dim)\n",
    "            self.relu = nn.ReLU()\n",
    "            self._init_params()\n",
    "            final_in_features = fc_dim\n",
    "\n",
    "        self.loss_module = loss_module\n",
    "        if loss_module == 'arcface':\n",
    "            self.final = ArcMarginProduct(final_in_features, n_classes,\n",
    "                                          scale=scale, margin=margin, easy_margin=False, ls_eps=ls_eps)\n",
    "        else:\n",
    "            self.final = nn.Linear(final_in_features, n_classes)\n",
    "\n",
    "    def _init_params(self):\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "        nn.init.constant_(self.fc.bias, 0)\n",
    "        nn.init.constant_(self.bn.weight, 1)\n",
    "        nn.init.constant_(self.bn.bias, 0)\n",
    "\n",
    "    def forward(self, input_ids,attention_mask, label):\n",
    "        feature = self.extract_feat(input_ids,attention_mask)\n",
    "        if self.loss_module == 'arcface':\n",
    "            logits = self.final(feature, label)\n",
    "        else:\n",
    "            logits = self.final(feature)\n",
    "        return logits\n",
    "\n",
    "    def extract_feat(self, input_ids,attention_mask):\n",
    "        x = self.transformer(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        \n",
    "        features = x[0]\n",
    "        features = features[:,0,:]\n",
    "\n",
    "        if self.use_fc:\n",
    "            features = self.dropout(features)\n",
    "            features = self.fc(features)\n",
    "            features = self.bn(features)\n",
    "            features = self.relu(features)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "pleased-practice",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T10:58:59.606056Z",
     "iopub.status.busy": "2021-05-08T10:58:59.605346Z",
     "iopub.status.idle": "2021-05-08T10:58:59.608144Z",
     "shell.execute_reply": "2021-05-08T10:58:59.607651Z"
    },
    "papermill": {
     "duration": 0.032544,
     "end_time": "2021-05-08T10:58:59.608240",
     "exception": false,
     "start_time": "2021-05-08T10:58:59.575696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_text_embeddings(df,model_params, transformer_model_path, ext = False):\n",
    "    embeds = []\n",
    "    if ext :\n",
    "        model = ShopeeNetExtend(**model_params)\n",
    "    else:\n",
    "        model = ShopeeNet(**model_params)\n",
    "    model.eval()\n",
    "    \n",
    "    model.load_state_dict(dict(list(torch.load(transformer_model_path).items())[:-1]))\n",
    "    model = model.to(device)\n",
    "\n",
    "    # change datasets for different Models\n",
    "    text_dataset = ShopeeDatasetText(df)\n",
    "    text_loader = torch.utils.data.DataLoader(\n",
    "        text_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        num_workers=NUM_WORKERS\n",
    "    )\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask in tqdm(text_loader): \n",
    "            input_ids = input_ids.cuda()\n",
    "            attention_mask = attention_mask.cuda()\n",
    "            feat = model(input_ids, attention_mask)\n",
    "            text_embeddings = feat.detach().cpu().numpy()\n",
    "            embeds.append(text_embeddings)\n",
    "    \n",
    "    \n",
    "    del model\n",
    "    text_embeddings = np.concatenate(embeds)\n",
    "    print(f'Our text embeddings shape is {text_embeddings.shape}')\n",
    "    del embeds\n",
    "    gc.collect()\n",
    "    return text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "filled-michigan",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T10:58:59.661014Z",
     "iopub.status.busy": "2021-05-08T10:58:59.660216Z",
     "iopub.status.idle": "2021-05-08T10:58:59.663249Z",
     "shell.execute_reply": "2021-05-08T10:58:59.662713Z"
    },
    "papermill": {
     "duration": 0.032521,
     "end_time": "2021-05-08T10:58:59.663363",
     "exception": false,
     "start_time": "2021-05-08T10:58:59.630842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_text_predictions(df, max_features = 25_000):\n",
    "    \n",
    "    model = TfidfVectorizer( binary = True, max_features = max_features)\n",
    "    text_embeddings = model.fit_transform(df_cu['title']).toarray()\n",
    "    preds = []\n",
    "    CHUNK = 1024*4\n",
    "\n",
    "    print('Finding similar titles...')\n",
    "    CTS = len(df)//CHUNK\n",
    "    if len(df)%CHUNK!=0: CTS += 1\n",
    "    for j in range( CTS ):\n",
    "\n",
    "        a = j*CHUNK\n",
    "        b = (j+1)*CHUNK\n",
    "        b = min(b,len(df))\n",
    "        print('chunk',a,'to',b)\n",
    "\n",
    "        # COSINE SIMILARITY DISTANCE\n",
    "        cts = cupy.matmul( text_embeddings, text_embeddings[a:b].T).T\n",
    "\n",
    "        for k in range(b-a):\n",
    "            IDX = cupy.where(cts[k,]> 0.75)[0]\n",
    "            o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n",
    "            preds.append(o)\n",
    "    \n",
    "    del model,text_embeddings\n",
    "    gc.collect()\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-voice",
   "metadata": {
    "papermill": {
     "duration": 0.022547,
     "end_time": "2021-05-08T10:58:59.708562",
     "exception": false,
     "start_time": "2021-05-08T10:58:59.686015",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# IMAGE INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "relevant-voltage",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T10:58:59.759429Z",
     "iopub.status.busy": "2021-05-08T10:58:59.758831Z",
     "iopub.status.idle": "2021-05-08T10:59:12.887979Z",
     "shell.execute_reply": "2021-05-08T10:59:12.888713Z"
    },
    "papermill": {
     "duration": 13.15766,
     "end_time": "2021-05-08T10:59:12.888904",
     "exception": false,
     "start_time": "2021-05-08T10:58:59.731244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Model Backbone for eca_nfnet_l1 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our image embeddings shape is (3, 512)\n",
      "Building Model Backbone for efficientnet_b3 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our image embeddings shape is (3, 512)\n",
      "Building Model Backbone for tf_efficientnet_b5_ns model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our image embeddings shape is (3, 512)\n",
      "Building Model Backbone for eca_nfnet_l0 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.19it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3864.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our image embeddings shape is (3, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "image_embeddings1 = get_image_embeddings(image_paths.values, CFG.model_name1, CFG.model_path1, True)\n",
    "image_embeddings2 = get_image_embeddings(image_paths.values, CFG.model_name2, CFG.model_path2, False)\n",
    "image_embeddings3 = get_image_embeddings(image_paths.values, CFG.model_name3, CFG.model_path3, False)\n",
    "image_embeddings4 = get_image_embeddings(image_paths.values, CFG.model_name4, CFG.model_path4, True)\n",
    "#image_embeddings5 = get_image_embeddings(image_paths.values, CFG.model_name5, CFG.model_path5, True)\n",
    "\n",
    "image_embeddings = (image_embeddings1 + image_embeddings2 + image_embeddings3 + image_embeddings4 ) / 4\n",
    "image_predictions = get_image_predictions(df, image_embeddings, threshold = 0.36)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-allergy",
   "metadata": {
    "papermill": {
     "duration": 0.028912,
     "end_time": "2021-05-08T10:59:12.947671",
     "exception": false,
     "start_time": "2021-05-08T10:59:12.918759",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TEXT INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "younger-offense",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T10:59:13.012793Z",
     "iopub.status.busy": "2021-05-08T10:59:13.012099Z",
     "iopub.status.idle": "2021-05-08T11:00:17.847331Z",
     "shell.execute_reply": "2021-05-08T11:00:17.847769Z"
    },
    "papermill": {
     "duration": 64.871268,
     "end_time": "2021-05-08T11:00:17.847933",
     "exception": false,
     "start_time": "2021-05-08T10:59:12.976665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our text embeddings shape is (3, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our text embeddings shape is (3, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our text embeddings shape is (3, 768)\n",
      "Finding similar texts...for threshold : 0.85\n",
      "chunk 0 to 3\n",
      "Finding similar titles...\n",
      "chunk 0 to 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_params['model_name'] = transformer_model_1\n",
    "TOKENIZER = transformers.AutoTokenizer.from_pretrained(transformer_model_1)\n",
    "CONFIG = transformers.AutoConfig.from_pretrained(transformer_model_1)\n",
    "mask_tok = 250001\n",
    "text_embeddings_roberta_base = get_text_embeddings(df, model_params, TEXT_MODEL_PATH_1)\n",
    "\n",
    "# THIS WORKING \n",
    "# sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
    "model_params['model_name'] = transformer_model_2\n",
    "TOKENIZER = transformers.AutoTokenizer.from_pretrained(transformer_model_2)\n",
    "CONFIG = transformers.AutoConfig.from_pretrained(transformer_model_2)\n",
    "mask_tok = 250001\n",
    "text_embeddings_param = get_text_embeddings(df, model_params,TEXT_MODEL_PATH_2)\n",
    "\n",
    "model_params['model_name'] = transformer_model_3\n",
    "TOKENIZER = DistilBertTokenizer.from_pretrained(transformer_model_3)\n",
    "DistilBertModel.from_pretrained(transformer_model_3)\n",
    "CONFIG = transformers.AutoConfig.from_pretrained(transformer_model_3)\n",
    "mask_tok = 31999\n",
    "text_embeddings_distil_bert_indonesian = get_text_embeddings(df, model_params,TEXT_MODEL_PATH_3)\n",
    "\n",
    "text_embeddings = (text_embeddings_roberta_base + text_embeddings_param + text_embeddings_distil_bert_indonesian) / 3\n",
    "\n",
    "df,text_predictions_sbert = get_neighbours_cos_sim(df,text_embeddings)\n",
    "\n",
    "text_predictions_tfidf = get_text_predictions(df, max_features = 25_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-grove",
   "metadata": {
    "papermill": {
     "duration": 0.033499,
     "end_time": "2021-05-08T11:00:17.915218",
     "exception": false,
     "start_time": "2021-05-08T11:00:17.881719",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# W2V Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "constant-gasoline",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T11:00:17.988191Z",
     "iopub.status.busy": "2021-05-08T11:00:17.987614Z",
     "iopub.status.idle": "2021-05-08T11:00:41.536001Z",
     "shell.execute_reply": "2021-05-08T11:00:41.535457Z"
    },
    "papermill": {
     "duration": 23.587832,
     "end_time": "2021-05-08T11:00:41.536136",
     "exception": false,
     "start_time": "2021-05-08T11:00:17.948304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding similar texts...for threshold : 0.85\n",
      "chunk 0 to 3\n"
     ]
    }
   ],
   "source": [
    "corpus = tokenize_corpus(list(df['title']))\n",
    "w2v_model = Word2Vec.load(\"../input/word2vec-100-indonesian/idwiki_word2vec_100.model\")\n",
    "#w2v_model = np.load('../input/word2vec-100-indonesian/idwiki_word2vec_100.model.wv.vectors.npy')\n",
    "embeds = []\n",
    "for sentence in corpus:\n",
    "    words = [w for w in sentence if w in w2v_model.wv.index_to_key]\n",
    "    words_vector = np.array([w2v_model.wv[w] for w in words])\n",
    "    if len(words_vector)==0: \n",
    "        embeds.append(np.zeros((200), dtype='float32').tolist())\n",
    "    else:\n",
    "        embed = np.median(words_vector, axis=0).tolist()\n",
    "        embeds.append(embed)\n",
    "        \n",
    "df,text_predictions_w2v = get_neighbours_cos_sim(df,embeds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "funny-stevens",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T11:00:41.615450Z",
     "iopub.status.busy": "2021-05-08T11:00:41.614748Z",
     "iopub.status.idle": "2021-05-08T11:00:41.742093Z",
     "shell.execute_reply": "2021-05-08T11:00:41.741297Z"
    },
    "papermill": {
     "duration": 0.17038,
     "end_time": "2021-05-08T11:00:41.742225",
     "exception": false,
     "start_time": "2021-05-08T11:00:41.571845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not GET_CV:\n",
    "    df['text_predictions_w2v'] = text_predictions_w2v\n",
    "    df['text_predictions_sbert'] = text_predictions_sbert\n",
    "    df['image_predictions'] = image_predictions\n",
    "    df['text_predictions'] = text_predictions_tfidf\n",
    "    df['matches'] = df.apply(combine_predictions, axis = 1)\n",
    "    df[['posting_id', 'matches']].to_csv('submission.csv', index = False)\n",
    "else:\n",
    "    df['text_predictions_w2v'] = text_predictions_w2v\n",
    "    df['text_predictions_sbert'] = text_predictions_sbert\n",
    "    df['image_predictions'] = image_predictions\n",
    "    df['text_predictions'] = text_predictions_tfidf\n",
    "    df['matches'] = df.apply(combine_predictions, axis = 1)\n",
    "    df[['posting_id', 'matches']].to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-monitor",
   "metadata": {
    "papermill": {
     "duration": 0.034448,
     "end_time": "2021-05-08T11:00:41.812433",
     "exception": false,
     "start_time": "2021-05-08T11:00:41.777985",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Thanks you so much for reading this notebook. If you have any suggestions or ideas on ensembling models together then do let me know. 😁"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 127.66207,
   "end_time": "2021-05-08T11:00:43.960878",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-08T10:58:36.298808",
   "version": "2.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
