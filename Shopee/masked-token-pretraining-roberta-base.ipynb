{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "right-steal",
   "metadata": {
    "papermill": {
     "duration": 0.013975,
     "end_time": "2021-05-05T08:56:25.253428",
     "exception": false,
     "start_time": "2021-05-05T08:56:25.239453",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# About this Notebook\n",
    "\n",
    "In the Quest of generating of more relevant and better embeddings , here is another technique that I thought be useful. This notebooks describes <b> MASK token prediction BERT type pretraining of transformer models on our dataset </b> . My intuition was if we pretrain BERT or any other model using MASK word prediction task and then fine tune that model using arcface loss or simple classification task it might do a better job at creating good embeddings because it might have more idea about the words in the title. \n",
    "\n",
    "However it didnt give any significant boost in the performance of xlm-roberta , the reason which I feel for this is the test set is a whole lot different from train set . It was a good learning experience for me though , I am sharing it with the community as I feel many more can learn from it and also I would know if I have done anay mistake while implementing this\n",
    "\n",
    "Happy Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "graphic-roller",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-05-05T08:56:25.287986Z",
     "iopub.status.busy": "2021-05-05T08:56:25.287156Z",
     "iopub.status.idle": "2021-05-05T08:56:34.035437Z",
     "shell.execute_reply": "2021-05-05T08:56:34.034685Z"
    },
    "papermill": {
     "duration": 8.770598,
     "end_time": "2021-05-05T08:56:34.035650",
     "exception": false,
     "start_time": "2021-05-05T08:56:25.265052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preliminaries\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup,get_cosine_schedule_with_warmup\n",
    "from transformers import get_cosine_with_hard_restarts_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-african",
   "metadata": {
    "papermill": {
     "duration": 0.011582,
     "end_time": "2021-05-05T08:56:34.059735",
     "exception": false,
     "start_time": "2021-05-05T08:56:34.048153",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "grateful-criminal",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T08:56:34.093233Z",
     "iopub.status.busy": "2021-05-05T08:56:34.092312Z",
     "iopub.status.idle": "2021-05-05T08:56:34.689590Z",
     "shell.execute_reply": "2021-05-05T08:56:34.688334Z"
    },
    "papermill": {
     "duration": 0.61837,
     "end_time": "2021-05-05T08:56:34.689790",
     "exception": false,
     "start_time": "2021-05-05T08:56:34.071420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_WORKERS = 4\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "EPOCHS = 2\n",
    "SEED = 2020\n",
    "LR = 3e-5\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "################################################# MODEL ####################################################################\n",
    "\n",
    "transformer_model = '../input/sentence-transformer-models/stsb-roberta-base/0_Transformer'\n",
    "TOKENIZER = transformers.AutoTokenizer.from_pretrained(transformer_model)\n",
    "CONFIG = transformers.AutoConfig.from_pretrained(transformer_model)\n",
    "mask_tok = 50264\n",
    "############################################################################################################################\n",
    "if transformer_model == 'bert-base-uncased':\n",
    "    mask_tok = 103\n",
    "elif transformer_model == 'roberta-base':\n",
    "    mask_tok = 50264\n",
    "elif (transformer_model == 'xlm-roberta-base') or (transformer_model == 'sentence-transformers/paraphrase-xlm-r-multilingual-v1'):\n",
    "    mask_tok = 250001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-hardware",
   "metadata": {
    "papermill": {
     "duration": 0.012173,
     "end_time": "2021-05-05T08:56:34.714170",
     "exception": false,
     "start_time": "2021-05-05T08:56:34.701997",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "great-bicycle",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T08:56:34.751908Z",
     "iopub.status.busy": "2021-05-05T08:56:34.750820Z",
     "iopub.status.idle": "2021-05-05T08:56:34.754816Z",
     "shell.execute_reply": "2021-05-05T08:56:34.754136Z"
    },
    "papermill": {
     "duration": 0.028449,
     "end_time": "2021-05-05T08:56:34.754987",
     "exception": false,
     "start_time": "2021-05-05T08:56:34.726538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    \n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-aspect",
   "metadata": {
    "papermill": {
     "duration": 0.011794,
     "end_time": "2021-05-05T08:56:34.779312",
     "exception": false,
     "start_time": "2021-05-05T08:56:34.767518",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eight-cartridge",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T08:56:34.817960Z",
     "iopub.status.busy": "2021-05-05T08:56:34.815378Z",
     "iopub.status.idle": "2021-05-05T08:56:34.818710Z",
     "shell.execute_reply": "2021-05-05T08:56:34.819291Z"
    },
    "papermill": {
     "duration": 0.028203,
     "end_time": "2021-05-05T08:56:34.819450",
     "exception": false,
     "start_time": "2021-05-05T08:56:34.791247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ShopeeDataset(Dataset):\n",
    "    def __init__(self, csv):\n",
    "        self.csv = csv.reset_index()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.csv.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.csv.iloc[index]\n",
    "        text = row.title\n",
    "        \n",
    "        text = TOKENIZER(text,\n",
    "                         return_attention_mask=False,\n",
    "                         return_token_type_ids=False,\n",
    "                         padding='max_length',\n",
    "                         truncation=True,\n",
    "                         max_length=64)\n",
    "        \n",
    "        input_ids = text['input_ids']\n",
    "        \n",
    "        input_ids,labels = self.prepare_mlm_input_and_labels(np.array(input_ids))\n",
    "\n",
    "        input_ids = torch.tensor(input_ids,dtype=torch.long)\n",
    "        labels = torch.tensor(labels,dtype=torch.long)\n",
    "    \n",
    "        return input_ids,labels\n",
    "    \n",
    "    def prepare_mlm_input_and_labels(self,X):\n",
    "        # 15% BERT masking\n",
    "        inp_mask = np.random.rand(*X.shape)<0.15 \n",
    "        # do not mask special tokens\n",
    "        inp_mask[X<=2] = False\n",
    "        # set targets to -1 by default, it means ignore\n",
    "        labels = -100 * np.ones(X.shape, dtype=int)\n",
    "        # set labels for masked tokens\n",
    "        labels[inp_mask] = X[inp_mask]\n",
    "        \n",
    "        # prepare input\n",
    "        X_mlm = np.copy(X)\n",
    "        # set input to [MASK] which is the last token for the 90% of tokens\n",
    "        # this means leaving 10% unchanged\n",
    "        inp_mask_2mask = inp_mask  & (np.random.rand(*X.shape)<0.90)\n",
    "        X_mlm[inp_mask_2mask] = mask_tok\n",
    "\n",
    "        # set 10% to a random token\n",
    "        inp_mask_2random = inp_mask_2mask  & (np.random.rand(*X.shape) < 1/9)\n",
    "        X_mlm[inp_mask_2random] = np.random.randint(3, CONFIG.vocab_size, inp_mask_2random.sum())\n",
    "\n",
    "        return X_mlm, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-creature",
   "metadata": {
    "papermill": {
     "duration": 0.013946,
     "end_time": "2021-05-05T08:56:34.845876",
     "exception": false,
     "start_time": "2021-05-05T08:56:34.831930",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bored-bullet",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T08:56:34.877764Z",
     "iopub.status.busy": "2021-05-05T08:56:34.876883Z",
     "iopub.status.idle": "2021-05-05T08:56:34.881691Z",
     "shell.execute_reply": "2021-05-05T08:56:34.881107Z"
    },
    "papermill": {
     "duration": 0.023327,
     "end_time": "2021-05-05T08:56:34.881841",
     "exception": false,
     "start_time": "2021-05-05T08:56:34.858514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def masked_categorical_crossentropy(output,target):\n",
    "    y_true_masked = target[target!= -100]\n",
    "    y_pred_masked = output[target!= -100]\n",
    "    loss =  nn.CrossEntropyLoss()(y_pred_masked,y_true_masked)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-renewal",
   "metadata": {
    "papermill": {
     "duration": 0.012454,
     "end_time": "2021-05-05T08:56:34.907530",
     "exception": false,
     "start_time": "2021-05-05T08:56:34.895076",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unusual-focus",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T08:56:34.945486Z",
     "iopub.status.busy": "2021-05-05T08:56:34.943770Z",
     "iopub.status.idle": "2021-05-05T08:56:34.946303Z",
     "shell.execute_reply": "2021-05-05T08:56:34.946891Z"
    },
    "papermill": {
     "duration": 0.026566,
     "end_time": "2021-05-05T08:56:34.947063",
     "exception": false,
     "start_time": "2021-05-05T08:56:34.920497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_fn(dataloader,model,optimizer,device,scheduler,epoch):\n",
    "    model.train()\n",
    "    loss_score = AverageMeter()\n",
    "    \n",
    "    tk0 = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for bi,d in tk0:\n",
    "        \n",
    "        batch_size = d[0].shape[0]\n",
    "\n",
    "        input_ids = d[0]\n",
    "        targets = d[1]\n",
    "\n",
    "        input_ids = input_ids.to(device,dtype=torch.long)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(input_ids=input_ids,labels=targets)\n",
    "        \n",
    "        loss = output.loss       \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_score.update(loss.detach().item(), batch_size)\n",
    "        tk0.set_postfix(Train_Loss=loss_score.avg,Epoch=epoch,LR=optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        if scheduler is not None:\n",
    "                scheduler.step()\n",
    "        \n",
    "    return loss_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-uruguay",
   "metadata": {
    "papermill": {
     "duration": 0.012142,
     "end_time": "2021-05-05T08:56:34.972065",
     "exception": false,
     "start_time": "2021-05-05T08:56:34.959923",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "assisted-arlington",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T08:56:35.005336Z",
     "iopub.status.busy": "2021-05-05T08:56:35.004619Z",
     "iopub.status.idle": "2021-05-05T08:56:35.204982Z",
     "shell.execute_reply": "2021-05-05T08:56:35.204043Z"
    },
    "papermill": {
     "duration": 0.220502,
     "end_time": "2021-05-05T08:56:35.205169",
     "exception": false,
     "start_time": "2021-05-05T08:56:34.984667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../input/shopee-product-matching/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sound-transsexual",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T08:56:35.245007Z",
     "iopub.status.busy": "2021-05-05T08:56:35.242378Z",
     "iopub.status.idle": "2021-05-05T08:56:35.245849Z",
     "shell.execute_reply": "2021-05-05T08:56:35.246406Z"
    },
    "papermill": {
     "duration": 0.028316,
     "end_time": "2021-05-05T08:56:35.246622",
     "exception": false,
     "start_time": "2021-05-05T08:56:35.218306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    # Defining DataSet\n",
    "    train_dataset = ShopeeDataset(\n",
    "        csv=data\n",
    "    )\n",
    "        \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "        num_workers=NUM_WORKERS\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Defining Model for specific fold\n",
    "    model = AutoModelForMaskedLM.from_pretrained(transformer_model)\n",
    "    print(model)\n",
    "    model.to(device)\n",
    "\n",
    "        \n",
    "    # Defining Optimizer with weight decay to params other than bias and layer norms\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.0001},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "            ]  \n",
    "    \n",
    "    optimizer = AdamW(optimizer_parameters, lr=LR)\n",
    "    \n",
    "    #Defining LR SCheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=len(train_loader)*5, \n",
    "        num_training_steps=len(train_loader)*EPOCHS\n",
    "    )\n",
    "        \n",
    "    # THE ENGINE LOOP\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train_fn(train_loader, model,optimizer, device,scheduler=scheduler,epoch=epoch)\n",
    "        \n",
    "    model.save_pretrained('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "important-dialogue",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T08:56:35.278577Z",
     "iopub.status.busy": "2021-05-05T08:56:35.277059Z",
     "iopub.status.idle": "2021-05-05T09:08:10.017803Z",
     "shell.execute_reply": "2021-05-05T09:08:10.016433Z"
    },
    "papermill": {
     "duration": 694.758656,
     "end_time": "2021-05-05T09:08:10.017967",
     "exception": false,
     "start_time": "2021-05-05T08:56:35.259311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at ../input/sentence-transformer-models/stsb-roberta-base/0_Transformer and are newly initialized: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForMaskedLM(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): RobertaLMHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1070/1070 [05:35<00:00,  3.19it/s, Epoch=0, LR=5.99e-6, Train_Loss=9.8]\n",
      "100%|██████████| 1070/1070 [05:36<00:00,  3.18it/s, Epoch=1, LR=1.2e-5, Train_Loss=6.57]\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 715.35393,
   "end_time": "2021-05-05T09:08:14.231312",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-05T08:56:18.877382",
   "version": "2.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
