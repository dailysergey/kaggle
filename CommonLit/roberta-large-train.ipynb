{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "european-coaching",
   "metadata": {
    "papermill": {
     "duration": 0.016363,
     "end_time": "2021-07-19T08:08:20.242243",
     "exception": false,
     "start_time": "2021-07-19T08:08:20.225880",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Overview\n",
    "This is kernel is almost the same as [Lightweight Roberta solution in PyTorch](https://www.kaggle.com/andretugan/lightweight-roberta-solution-in-pytorch), but instead of \"roberta-base\", it starts from [Maunish's pre-trained model](https://www.kaggle.com/maunish/clrp-roberta-base).\n",
    "\n",
    "Acknowledgments: some ideas were taken from kernels by [Torch](https://www.kaggle.com/rhtsingh) and [Maunish](https://www.kaggle.com/maunish)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "attempted-flood",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-07-19T08:08:20.288482Z",
     "iopub.status.busy": "2021-07-19T08:08:20.287722Z",
     "iopub.status.idle": "2021-07-19T08:08:28.087226Z",
     "shell.execute_reply": "2021-07-19T08:08:28.086033Z",
     "shell.execute_reply.started": "2021-07-15T18:09:21.699559Z"
    },
    "papermill": {
     "duration": 7.828667,
     "end_time": "2021-07-19T08:08:28.087403",
     "exception": false,
     "start_time": "2021-07-19T08:08:20.258736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup,get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-audience",
   "metadata": {
    "papermill": {
     "duration": 0.015914,
     "end_time": "2021-07-19T08:08:28.120698",
     "exception": false,
     "start_time": "2021-07-19T08:08:28.104784",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# Version 1\n",
    "* SEED = 42\n",
    "* [2e-5,5e-5,1e-4]\n",
    "* Layers: [0, 261,309]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "tested-slide",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T08:08:28.196691Z",
     "iopub.status.busy": "2021-07-19T08:08:28.195798Z",
     "iopub.status.idle": "2021-07-19T08:08:28.198774Z",
     "shell.execute_reply": "2021-07-19T08:08:28.198354Z",
     "shell.execute_reply.started": "2021-07-15T18:09:29.591603Z"
    },
    "papermill": {
     "duration": 0.062986,
     "end_time": "2021-07-19T08:08:28.198894",
     "exception": false,
     "start_time": "2021-07-19T08:08:28.135908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_FOLDS = 5\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 12\n",
    "MAX_LEN = 250\n",
    "WORKERS = 4\n",
    "SEED = 42\n",
    "EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n",
    "ROBERTA_PATH = \"../input/robertalarge\"\n",
    "TOKENIZER_PATH = \"../input/robertalarge\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "thirty-telescope",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T08:08:28.235964Z",
     "iopub.status.busy": "2021-07-19T08:08:28.235053Z",
     "iopub.status.idle": "2021-07-19T08:08:28.238138Z",
     "shell.execute_reply": "2021-07-19T08:08:28.237534Z",
     "shell.execute_reply.started": "2021-07-15T18:09:29.640241Z"
    },
    "papermill": {
     "duration": 0.024666,
     "end_time": "2021-07-19T08:08:28.238263",
     "exception": false,
     "start_time": "2021-07-19T08:08:28.213597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_random_seed(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "optional-olympus",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T08:08:28.278132Z",
     "iopub.status.busy": "2021-07-19T08:08:28.277583Z",
     "iopub.status.idle": "2021-07-19T08:08:28.390017Z",
     "shell.execute_reply": "2021-07-19T08:08:28.388919Z",
     "shell.execute_reply.started": "2021-07-15T18:09:29.65245Z"
    },
    "papermill": {
     "duration": 0.135978,
     "end_time": "2021-07-19T08:08:28.390189",
     "exception": false,
     "start_time": "2021-07-19T08:08:28.254211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/train.csv\")\n",
    "\n",
    "# Remove incomplete entries if any.\n",
    "train_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n",
    "              inplace=True)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "test_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\n",
    "submission_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "finnish-elimination",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T08:08:28.428286Z",
     "iopub.status.busy": "2021-07-19T08:08:28.427648Z",
     "iopub.status.idle": "2021-07-19T08:08:28.651694Z",
     "shell.execute_reply": "2021-07-19T08:08:28.650743Z",
     "shell.execute_reply.started": "2021-07-15T18:09:29.784771Z"
    },
    "papermill": {
     "duration": 0.245301,
     "end_time": "2021-07-19T08:08:28.651853",
     "exception": false,
     "start_time": "2021-07-19T08:08:28.406552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-output",
   "metadata": {
    "papermill": {
     "duration": 0.014448,
     "end_time": "2021-07-19T08:08:28.680332",
     "exception": false,
     "start_time": "2021-07-19T08:08:28.665884",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "mechanical-shipping",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T08:08:28.717662Z",
     "iopub.status.busy": "2021-07-19T08:08:28.716827Z",
     "iopub.status.idle": "2021-07-19T08:08:28.719802Z",
     "shell.execute_reply": "2021-07-19T08:08:28.719188Z",
     "shell.execute_reply.started": "2021-07-15T18:09:29.981787Z"
    },
    "papermill": {
     "duration": 0.025276,
     "end_time": "2021-07-19T08:08:28.719944",
     "exception": false,
     "start_time": "2021-07-19T08:08:28.694668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LitDataset(Dataset):\n",
    "    def __init__(self, df, inference_only=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.df = df        \n",
    "        self.inference_only = inference_only\n",
    "        self.text = df.excerpt.tolist()\n",
    "        \n",
    "        if not self.inference_only:\n",
    "            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n",
    "    \n",
    "        self.encoded = tokenizer.batch_encode_plus(\n",
    "            self.text,\n",
    "            padding = 'max_length',            \n",
    "            max_length = MAX_LEN,\n",
    "            truncation = True,\n",
    "            return_attention_mask=True\n",
    "        )        \n",
    " \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):        \n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][index])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n",
    "        \n",
    "        if self.inference_only:\n",
    "            return (input_ids, attention_mask)            \n",
    "        else:\n",
    "            target = self.target[index]\n",
    "            return (input_ids, attention_mask, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-motel",
   "metadata": {
    "papermill": {
     "duration": 0.015789,
     "end_time": "2021-07-19T08:08:28.751810",
     "exception": false,
     "start_time": "2021-07-19T08:08:28.736021",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model\n",
    "The model is inspired by the one from [Maunish](https://www.kaggle.com/maunish/clrp-roberta-svm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "patient-winter",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T08:08:28.789846Z",
     "iopub.status.busy": "2021-07-19T08:08:28.788956Z",
     "iopub.status.idle": "2021-07-19T08:08:28.791588Z",
     "shell.execute_reply": "2021-07-19T08:08:28.791119Z",
     "shell.execute_reply.started": "2021-07-15T18:09:29.991783Z"
    },
    "papermill": {
     "duration": 0.024565,
     "end_time": "2021-07-19T08:08:28.791713",
     "exception": false,
     "start_time": "2021-07-19T08:08:28.767148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n",
    "        config.update({\"output_hidden_states\":True, \n",
    "                       \"hidden_dropout_prob\": 0.0,\n",
    "                       \"layer_norm_eps\": 1e-7})                       \n",
    "        \n",
    "        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n",
    "            \n",
    "        self.attention = nn.Sequential(            \n",
    "            nn.Linear(1024, 512),            \n",
    "            nn.Tanh(),                       \n",
    "            nn.Linear(512, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )        \n",
    "\n",
    "        self.regressor = nn.Sequential(                        \n",
    "            nn.Linear(1024, 1)                        \n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        roberta_output = self.roberta(input_ids=input_ids,\n",
    "                                      attention_mask=attention_mask)        \n",
    "\n",
    "        # There are a total of 13 layers of hidden states.\n",
    "        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n",
    "        # We take the hidden states from the last Roberta layer.\n",
    "        last_layer_hidden_states = roberta_output.hidden_states[-1]\n",
    "\n",
    "        # The number of cells is MAX_LEN.\n",
    "        # The size of the hidden state of each cell is 768 (for roberta-base).\n",
    "        # In order to condense hidden states of all cells to a context vector,\n",
    "        # we compute a weighted average of the hidden states of all cells.\n",
    "        # We compute the weight of each cell, using the attention neural network.\n",
    "        weights = self.attention(last_layer_hidden_states)\n",
    "                \n",
    "        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n",
    "        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n",
    "        # Now we compute context_vector as the weighted average.\n",
    "        # context_vector.shape is BATCH_SIZE x 768\n",
    "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n",
    "        \n",
    "        # Now we reduce the context vector to the prediction score.\n",
    "        return self.regressor(context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "smooth-macedonia",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T08:08:28.826155Z",
     "iopub.status.busy": "2021-07-19T08:08:28.825344Z",
     "iopub.status.idle": "2021-07-19T08:08:28.828015Z",
     "shell.execute_reply": "2021-07-19T08:08:28.827606Z",
     "shell.execute_reply.started": "2021-07-15T18:09:30.009479Z"
    },
    "papermill": {
     "duration": 0.022567,
     "end_time": "2021-07-19T08:08:28.828146",
     "exception": false,
     "start_time": "2021-07-19T08:08:28.805579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_mse(model, data_loader):\n",
    "    \"\"\"Evaluates the mean squared error of the |model| on |data_loader|\"\"\"\n",
    "    model.eval()            \n",
    "    mse_sum = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, (input_ids, attention_mask, target) in enumerate(data_loader):\n",
    "            \n",
    "            input_ids = input_ids.to(DEVICE)\n",
    "            attention_mask = attention_mask.to(DEVICE)                        \n",
    "            target = target.to(DEVICE)           \n",
    "            \n",
    "            pred = model(input_ids, attention_mask)                       \n",
    "\n",
    "            mse_sum += nn.MSELoss(reduction=\"sum\")(pred.flatten(), target).item()\n",
    "\n",
    "    return mse_sum / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "postal-rapid",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T08:08:28.868384Z",
     "iopub.status.busy": "2021-07-19T08:08:28.867386Z",
     "iopub.status.idle": "2021-07-19T08:08:28.870273Z",
     "shell.execute_reply": "2021-07-19T08:08:28.869805Z",
     "shell.execute_reply.started": "2021-07-15T18:09:30.022369Z"
    },
    "papermill": {
     "duration": 0.026214,
     "end_time": "2021-07-19T08:08:28.870417",
     "exception": false,
     "start_time": "2021-07-19T08:08:28.844203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    result = np.zeros(len(data_loader.dataset))    \n",
    "    index = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n",
    "            input_ids = input_ids.to(DEVICE)\n",
    "            attention_mask = attention_mask.to(DEVICE)\n",
    "                        \n",
    "            pred = model(input_ids, attention_mask)                        \n",
    "\n",
    "            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n",
    "            index += pred.shape[0]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "owned-coffee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T08:08:28.910530Z",
     "iopub.status.busy": "2021-07-19T08:08:28.909753Z",
     "iopub.status.idle": "2021-07-19T08:08:28.912432Z",
     "shell.execute_reply": "2021-07-19T08:08:28.912000Z",
     "shell.execute_reply.started": "2021-07-15T18:09:30.031788Z"
    },
    "papermill": {
     "duration": 0.027607,
     "end_time": "2021-07-19T08:08:28.912551",
     "exception": false,
     "start_time": "2021-07-19T08:08:28.884944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, model_path, train_loader, val_loader,\n",
    "          optimizer, scheduler=None, num_epochs=NUM_EPOCHS):    \n",
    "    best_val_rmse = None\n",
    "    best_epoch = 0\n",
    "    step = 0\n",
    "    last_eval_step = 0\n",
    "    eval_period = EVAL_SCHEDULE[0][1]    \n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):                           \n",
    "        val_rmse = None         \n",
    "\n",
    "        for batch_num, (input_ids, attention_mask, target) in enumerate(train_loader):\n",
    "            input_ids = input_ids.to(DEVICE)\n",
    "            attention_mask = attention_mask.to(DEVICE)            \n",
    "            target = target.to(DEVICE)                        \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            model.train()\n",
    "\n",
    "            pred = model(input_ids, attention_mask)\n",
    "                                                        \n",
    "            mse = nn.MSELoss(reduction=\"mean\")(pred.flatten(), target)\n",
    "                        \n",
    "            mse.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "            \n",
    "            if step >= last_eval_step + eval_period:\n",
    "                # Evaluate the model on val_loader.\n",
    "                elapsed_seconds = time.time() - start\n",
    "                num_steps = step - last_eval_step\n",
    "                print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n",
    "                last_eval_step = step\n",
    "                \n",
    "                val_rmse = math.sqrt(eval_mse(model, val_loader))                            \n",
    "\n",
    "                print(f\"Epoch: {epoch} batch_num: {batch_num}\", \n",
    "                      f\"val_rmse: {val_rmse:0.4}\")\n",
    "\n",
    "                for rmse, period in EVAL_SCHEDULE:\n",
    "                    if val_rmse >= rmse:\n",
    "                        eval_period = period\n",
    "                        break                               \n",
    "                \n",
    "                if not best_val_rmse or val_rmse < best_val_rmse:                    \n",
    "                    best_val_rmse = val_rmse\n",
    "                    best_epoch = epoch\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "                    print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n",
    "                else:       \n",
    "                    print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n",
    "                          f\"(from epoch {best_epoch})\")                                    \n",
    "                    \n",
    "                start = time.time()\n",
    "                                            \n",
    "            step += 1\n",
    "                        \n",
    "    \n",
    "    return best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "respective-newspaper",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T08:08:28.949314Z",
     "iopub.status.busy": "2021-07-19T08:08:28.948365Z",
     "iopub.status.idle": "2021-07-19T08:08:28.950715Z",
     "shell.execute_reply": "2021-07-19T08:08:28.951207Z",
     "shell.execute_reply.started": "2021-07-15T18:09:30.046098Z"
    },
    "papermill": {
     "duration": 0.024996,
     "end_time": "2021-07-19T08:08:28.951361",
     "exception": false,
     "start_time": "2021-07-19T08:08:28.926365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_optimizer(model):\n",
    "    named_parameters = list(model.named_parameters())    \n",
    "    \n",
    "    roberta_parameters = named_parameters[:389]    \n",
    "    attention_parameters = named_parameters[391:395]\n",
    "    regressor_parameters = named_parameters[395:]\n",
    "        \n",
    "    attention_group = [params for (name, params) in attention_parameters]\n",
    "    regressor_group = [params for (name, params) in regressor_parameters]\n",
    "\n",
    "    parameters = []\n",
    "    parameters.append({\"params\": attention_group})\n",
    "    parameters.append({\"params\": regressor_group})\n",
    "\n",
    "    for layer_num, (name, params) in enumerate(roberta_parameters):\n",
    "        weight_decay = 0.0 if \"bias\" in name else 0.01\n",
    "\n",
    "        lr = 2e-5\n",
    "        \n",
    "        if layer_num >= 261:        \n",
    "            lr = 5e-5\n",
    "\n",
    "        if layer_num >= 309:\n",
    "            lr = 7e-5\n",
    "            \n",
    "        parameters.append({\"params\": params,\n",
    "                           \"weight_decay\": weight_decay,\n",
    "                           \"lr\": lr})\n",
    "\n",
    "    return AdamW(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "featured-accountability",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T08:08:28.986741Z",
     "iopub.status.busy": "2021-07-19T08:08:28.985997Z",
     "iopub.status.idle": "2021-07-19T08:08:28.988642Z",
     "shell.execute_reply": "2021-07-19T08:08:28.988145Z",
     "shell.execute_reply.started": "2021-07-15T18:09:30.057531Z"
    },
    "papermill": {
     "duration": 0.021362,
     "end_time": "2021-07-19T08:08:28.988745",
     "exception": false,
     "start_time": "2021-07-19T08:08:28.967383",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = LitModel().to(DEVICE)\n",
    "#for _,layer in enumerate(model.named_parameters()):\n",
    "#    print(_,layer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "animated-plain",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T08:08:29.196912Z",
     "iopub.status.busy": "2021-07-19T08:08:29.196140Z",
     "iopub.status.idle": "2021-07-19T10:36:45.711747Z",
     "shell.execute_reply": "2021-07-19T10:36:45.711293Z"
    },
    "papermill": {
     "duration": 8896.709581,
     "end_time": "2021-07-19T10:36:45.711889",
     "exception": false,
     "start_time": "2021-07-19T08:08:29.002308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n",
      "\n",
      "16 steps took 17.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.033\n",
      "New best_val_rmse: 1.033\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7375\n",
      "New best_val_rmse: 0.7375\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6938\n",
      "New best_val_rmse: 0.6938\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.659\n",
      "New best_val_rmse: 0.659\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.9489\n",
      "Still best_val_rmse: 0.659 (from epoch 0)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6237\n",
      "New best_val_rmse: 0.6237\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6227\n",
      "New best_val_rmse: 0.6227\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6868\n",
      "Still best_val_rmse: 0.6227 (from epoch 0)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.7312\n",
      "Still best_val_rmse: 0.6227 (from epoch 0)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 160 val_rmse: 0.5932\n",
      "New best_val_rmse: 0.5932\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 176 val_rmse: 0.6101\n",
      "Still best_val_rmse: 0.5932 (from epoch 0)\n",
      "\n",
      "16 steps took 16.1 seconds\n",
      "Epoch: 1 batch_num: 4 val_rmse: 0.5317\n",
      "New best_val_rmse: 0.5317\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 20 val_rmse: 0.5525\n",
      "Still best_val_rmse: 0.5317 (from epoch 1)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 36 val_rmse: 0.5277\n",
      "New best_val_rmse: 0.5277\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 52 val_rmse: 0.5141\n",
      "New best_val_rmse: 0.5141\n",
      "\n",
      "16 steps took 15.8 seconds\n",
      "Epoch: 1 batch_num: 68 val_rmse: 0.5501\n",
      "Still best_val_rmse: 0.5141 (from epoch 1)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.5251\n",
      "Still best_val_rmse: 0.5141 (from epoch 1)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5253\n",
      "Still best_val_rmse: 0.5141 (from epoch 1)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.5007\n",
      "New best_val_rmse: 0.5007\n",
      "\n",
      "16 steps took 15.8 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.5172\n",
      "Still best_val_rmse: 0.5007 (from epoch 1)\n",
      "\n",
      "16 steps took 15.8 seconds\n",
      "Epoch: 1 batch_num: 148 val_rmse: 0.5057\n",
      "Still best_val_rmse: 0.5007 (from epoch 1)\n",
      "\n",
      "16 steps took 15.8 seconds\n",
      "Epoch: 1 batch_num: 164 val_rmse: 0.5069\n",
      "Still best_val_rmse: 0.5007 (from epoch 1)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 180 val_rmse: 0.5091\n",
      "Still best_val_rmse: 0.5007 (from epoch 1)\n",
      "\n",
      "16 steps took 15.9 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5034\n",
      "Still best_val_rmse: 0.5007 (from epoch 1)\n",
      "\n",
      "16 steps took 15.8 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4946\n",
      "New best_val_rmse: 0.4946\n",
      "\n",
      "8 steps took 7.9 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.5252\n",
      "Still best_val_rmse: 0.4946 (from epoch 2)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.5034\n",
      "Still best_val_rmse: 0.4946 (from epoch 2)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.5069\n",
      "Still best_val_rmse: 0.4946 (from epoch 2)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4932\n",
      "New best_val_rmse: 0.4932\n",
      "\n",
      "8 steps took 7.88 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5027\n",
      "Still best_val_rmse: 0.4932 (from epoch 2)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4892\n",
      "New best_val_rmse: 0.4892\n",
      "\n",
      "4 steps took 3.93 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4902\n",
      "Still best_val_rmse: 0.4892 (from epoch 2)\n",
      "\n",
      "8 steps took 7.84 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.5019\n",
      "Still best_val_rmse: 0.4892 (from epoch 2)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4967\n",
      "Still best_val_rmse: 0.4892 (from epoch 2)\n",
      "\n",
      "8 steps took 7.82 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4956\n",
      "Still best_val_rmse: 0.4892 (from epoch 2)\n",
      "\n",
      "8 steps took 7.83 seconds\n",
      "Epoch: 2 batch_num: 148 val_rmse: 0.4965\n",
      "Still best_val_rmse: 0.4892 (from epoch 2)\n",
      "\n",
      "8 steps took 7.9 seconds\n",
      "Epoch: 2 batch_num: 156 val_rmse: 0.4966\n",
      "Still best_val_rmse: 0.4892 (from epoch 2)\n",
      "\n",
      "8 steps took 7.84 seconds\n",
      "Epoch: 2 batch_num: 164 val_rmse: 0.4972\n",
      "Still best_val_rmse: 0.4892 (from epoch 2)\n",
      "\n",
      "8 steps took 7.82 seconds\n",
      "Epoch: 2 batch_num: 172 val_rmse: 0.497\n",
      "Still best_val_rmse: 0.4892 (from epoch 2)\n",
      "\n",
      "8 steps took 7.93 seconds\n",
      "Epoch: 2 batch_num: 180 val_rmse: 0.4971\n",
      "Still best_val_rmse: 0.4892 (from epoch 2)\n",
      "\n",
      "Performance estimates:\n",
      "[0.48920012553234143]\n",
      "Mean: 0.48920012553234143\n",
      "\n",
      "Fold 2/5\n",
      "\n",
      "16 steps took 17.0 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9222\n",
      "New best_val_rmse: 0.9222\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8232\n",
      "New best_val_rmse: 0.8232\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6832\n",
      "New best_val_rmse: 0.6832\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8383\n",
      "Still best_val_rmse: 0.6832 (from epoch 0)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6276\n",
      "New best_val_rmse: 0.6276\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6238\n",
      "New best_val_rmse: 0.6238\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.576\n",
      "New best_val_rmse: 0.576\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6091\n",
      "Still best_val_rmse: 0.576 (from epoch 0)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5275\n",
      "New best_val_rmse: 0.5275\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 160 val_rmse: 0.5561\n",
      "Still best_val_rmse: 0.5275 (from epoch 0)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 176 val_rmse: 0.6415\n",
      "Still best_val_rmse: 0.5275 (from epoch 0)\n",
      "\n",
      "16 steps took 16.2 seconds\n",
      "Epoch: 1 batch_num: 4 val_rmse: 0.631\n",
      "Still best_val_rmse: 0.5275 (from epoch 0)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 20 val_rmse: 0.5462\n",
      "Still best_val_rmse: 0.5275 (from epoch 0)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 36 val_rmse: 0.5158\n",
      "New best_val_rmse: 0.5158\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 52 val_rmse: 0.5094\n",
      "New best_val_rmse: 0.5094\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 68 val_rmse: 0.5404\n",
      "Still best_val_rmse: 0.5094 (from epoch 1)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.5154\n",
      "Still best_val_rmse: 0.5094 (from epoch 1)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5218\n",
      "Still best_val_rmse: 0.5094 (from epoch 1)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.6173\n",
      "Still best_val_rmse: 0.5094 (from epoch 1)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.5175\n",
      "Still best_val_rmse: 0.5094 (from epoch 1)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 148 val_rmse: 0.5112\n",
      "Still best_val_rmse: 0.5094 (from epoch 1)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 164 val_rmse: 0.494\n",
      "New best_val_rmse: 0.494\n",
      "\n",
      "8 steps took 7.86 seconds\n",
      "Epoch: 1 batch_num: 172 val_rmse: 0.4962\n",
      "Still best_val_rmse: 0.494 (from epoch 1)\n",
      "\n",
      "8 steps took 7.84 seconds\n",
      "Epoch: 1 batch_num: 180 val_rmse: 0.5061\n",
      "Still best_val_rmse: 0.494 (from epoch 1)\n",
      "\n",
      "16 steps took 16.1 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4987\n",
      "Still best_val_rmse: 0.494 (from epoch 1)\n",
      "\n",
      "8 steps took 7.84 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.519\n",
      "Still best_val_rmse: 0.494 (from epoch 1)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.496\n",
      "Still best_val_rmse: 0.494 (from epoch 1)\n",
      "\n",
      "8 steps took 7.84 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4918\n",
      "New best_val_rmse: 0.4918\n",
      "\n",
      "8 steps took 7.87 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4977\n",
      "Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "8 steps took 7.87 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4927\n",
      "Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "8 steps took 7.82 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4883\n",
      "New best_val_rmse: 0.4883\n",
      "\n",
      "4 steps took 3.94 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4894\n",
      "Still best_val_rmse: 0.4883 (from epoch 2)\n",
      "\n",
      "4 steps took 3.92 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4892\n",
      "Still best_val_rmse: 0.4883 (from epoch 2)\n",
      "\n",
      "4 steps took 3.91 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4879\n",
      "New best_val_rmse: 0.4879\n",
      "\n",
      "4 steps took 3.95 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4879\n",
      "New best_val_rmse: 0.4879\n",
      "\n",
      "4 steps took 3.96 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4893\n",
      "Still best_val_rmse: 0.4879 (from epoch 2)\n",
      "\n",
      "4 steps took 3.9 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4885\n",
      "Still best_val_rmse: 0.4879 (from epoch 2)\n",
      "\n",
      "4 steps took 3.94 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4902\n",
      "Still best_val_rmse: 0.4879 (from epoch 2)\n",
      "\n",
      "8 steps took 7.84 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.5083\n",
      "Still best_val_rmse: 0.4879 (from epoch 2)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4922\n",
      "Still best_val_rmse: 0.4879 (from epoch 2)\n",
      "\n",
      "8 steps took 7.86 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4916\n",
      "Still best_val_rmse: 0.4879 (from epoch 2)\n",
      "\n",
      "8 steps took 7.83 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4928\n",
      "Still best_val_rmse: 0.4879 (from epoch 2)\n",
      "\n",
      "8 steps took 7.89 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.493\n",
      "Still best_val_rmse: 0.4879 (from epoch 2)\n",
      "\n",
      "8 steps took 7.82 seconds\n",
      "Epoch: 2 batch_num: 148 val_rmse: 0.4922\n",
      "Still best_val_rmse: 0.4879 (from epoch 2)\n",
      "\n",
      "8 steps took 7.85 seconds\n",
      "Epoch: 2 batch_num: 156 val_rmse: 0.491\n",
      "Still best_val_rmse: 0.4879 (from epoch 2)\n",
      "\n",
      "8 steps took 7.89 seconds\n",
      "Epoch: 2 batch_num: 164 val_rmse: 0.4905\n",
      "Still best_val_rmse: 0.4879 (from epoch 2)\n",
      "\n",
      "8 steps took 7.82 seconds\n",
      "Epoch: 2 batch_num: 172 val_rmse: 0.4901\n",
      "Still best_val_rmse: 0.4879 (from epoch 2)\n",
      "\n",
      "8 steps took 7.85 seconds\n",
      "Epoch: 2 batch_num: 180 val_rmse: 0.49\n",
      "Still best_val_rmse: 0.4879 (from epoch 2)\n",
      "\n",
      "Performance estimates:\n",
      "[0.48920012553234143, 0.4878910677221497]\n",
      "Mean: 0.48854559662724556\n",
      "\n",
      "Fold 3/5\n",
      "\n",
      "16 steps took 17.0 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9445\n",
      "New best_val_rmse: 0.9445\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7525\n",
      "New best_val_rmse: 0.7525\n",
      "\n",
      "16 steps took 15.8 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6074\n",
      "New best_val_rmse: 0.6074\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6988\n",
      "Still best_val_rmse: 0.6074 (from epoch 0)\n",
      "\n",
      "16 steps took 15.6 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7342\n",
      "Still best_val_rmse: 0.6074 (from epoch 0)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5625\n",
      "New best_val_rmse: 0.5625\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6325\n",
      "Still best_val_rmse: 0.5625 (from epoch 0)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5222\n",
      "New best_val_rmse: 0.5222\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5275\n",
      "Still best_val_rmse: 0.5222 (from epoch 0)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 160 val_rmse: 0.5373\n",
      "Still best_val_rmse: 0.5222 (from epoch 0)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 176 val_rmse: 0.5125\n",
      "New best_val_rmse: 0.5125\n",
      "\n",
      "16 steps took 16.2 seconds\n",
      "Epoch: 1 batch_num: 4 val_rmse: 0.5254\n",
      "Still best_val_rmse: 0.5125 (from epoch 0)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 20 val_rmse: 0.5205\n",
      "Still best_val_rmse: 0.5125 (from epoch 0)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 36 val_rmse: 0.5829\n",
      "Still best_val_rmse: 0.5125 (from epoch 0)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 52 val_rmse: 0.5253\n",
      "Still best_val_rmse: 0.5125 (from epoch 0)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 68 val_rmse: 0.484\n",
      "New best_val_rmse: 0.484\n",
      "\n",
      "4 steps took 3.93 seconds\n",
      "Epoch: 1 batch_num: 72 val_rmse: 0.5372\n",
      "Still best_val_rmse: 0.484 (from epoch 1)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 88 val_rmse: 0.4931\n",
      "Still best_val_rmse: 0.484 (from epoch 1)\n",
      "\n",
      "8 steps took 7.81 seconds\n",
      "Epoch: 1 batch_num: 96 val_rmse: 0.4939\n",
      "Still best_val_rmse: 0.484 (from epoch 1)\n",
      "\n",
      "8 steps took 7.83 seconds\n",
      "Epoch: 1 batch_num: 104 val_rmse: 0.5074\n",
      "Still best_val_rmse: 0.484 (from epoch 1)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 120 val_rmse: 0.4866\n",
      "Still best_val_rmse: 0.484 (from epoch 1)\n",
      "\n",
      "4 steps took 3.94 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4937\n",
      "Still best_val_rmse: 0.484 (from epoch 1)\n",
      "\n",
      "8 steps took 7.85 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.5626\n",
      "Still best_val_rmse: 0.484 (from epoch 1)\n",
      "\n",
      "16 steps took 15.6 seconds\n",
      "Epoch: 1 batch_num: 148 val_rmse: 0.4733\n",
      "New best_val_rmse: 0.4733\n",
      "\n",
      "2 steps took 1.97 seconds\n",
      "Epoch: 1 batch_num: 150 val_rmse: 0.4919\n",
      "Still best_val_rmse: 0.4733 (from epoch 1)\n",
      "\n",
      "8 steps took 7.92 seconds\n",
      "Epoch: 1 batch_num: 158 val_rmse: 0.5559\n",
      "Still best_val_rmse: 0.4733 (from epoch 1)\n",
      "\n",
      "16 steps took 15.8 seconds\n",
      "Epoch: 1 batch_num: 174 val_rmse: 0.4915\n",
      "Still best_val_rmse: 0.4733 (from epoch 1)\n",
      "\n",
      "8 steps took 7.82 seconds\n",
      "Epoch: 1 batch_num: 182 val_rmse: 0.4923\n",
      "Still best_val_rmse: 0.4733 (from epoch 1)\n",
      "\n",
      "8 steps took 8.27 seconds\n",
      "Epoch: 2 batch_num: 2 val_rmse: 0.4766\n",
      "Still best_val_rmse: 0.4733 (from epoch 1)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4768\n",
      "Still best_val_rmse: 0.4733 (from epoch 1)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 6 val_rmse: 0.4792\n",
      "Still best_val_rmse: 0.4733 (from epoch 1)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4824\n",
      "Still best_val_rmse: 0.4733 (from epoch 1)\n",
      "\n",
      "4 steps took 3.91 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4853\n",
      "Still best_val_rmse: 0.4733 (from epoch 1)\n",
      "\n",
      "4 steps took 3.94 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.479\n",
      "Still best_val_rmse: 0.4733 (from epoch 1)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.4748\n",
      "Still best_val_rmse: 0.4733 (from epoch 1)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.475\n",
      "Still best_val_rmse: 0.4733 (from epoch 1)\n",
      "\n",
      "2 steps took 1.98 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4749\n",
      "Still best_val_rmse: 0.4733 (from epoch 1)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4742\n",
      "Still best_val_rmse: 0.4733 (from epoch 1)\n",
      "\n",
      "2 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4744\n",
      "Still best_val_rmse: 0.4733 (from epoch 1)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4753\n",
      "Still best_val_rmse: 0.4733 (from epoch 1)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4744\n",
      "Still best_val_rmse: 0.4733 (from epoch 1)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4721\n",
      "New best_val_rmse: 0.4721\n",
      "\n",
      "2 steps took 1.96 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4723\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.96 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4736\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.98 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4795\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.96 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4805\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "4 steps took 3.91 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4776\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4768\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4777\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.97 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4799\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4789\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4778\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4761\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.475\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4742\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4726\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.98 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4722\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4721\n",
      "New best_val_rmse: 0.4721\n",
      "\n",
      "2 steps took 1.97 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4726\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4732\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.98 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4733\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4748\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4776\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4785\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4784\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.99 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4782\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4767\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4748\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4739\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.96 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4737\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4736\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4742\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4744\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4746\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4741\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 2.02 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4736\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4737\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4739\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4742\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4735\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.98 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4726\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4722\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.96 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4721\n",
      "Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4721\n",
      "New best_val_rmse: 0.4721\n",
      "\n",
      "2 steps took 1.97 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4721\n",
      "New best_val_rmse: 0.4721\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.472\n",
      "New best_val_rmse: 0.472\n",
      "\n",
      "2 steps took 1.96 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4719\n",
      "New best_val_rmse: 0.4719\n",
      "\n",
      "2 steps took 1.97 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.472\n",
      "Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 1.96 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4721\n",
      "Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.472\n",
      "Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.472\n",
      "Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 1.98 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.472\n",
      "Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4721\n",
      "Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 1.98 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4722\n",
      "Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4723\n",
      "Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4723\n",
      "Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 1.99 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4724\n",
      "Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4725\n",
      "Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 1.96 seconds\n",
      "Epoch: 2 batch_num: 148 val_rmse: 0.4727\n",
      "Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 150 val_rmse: 0.4727\n",
      "Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 152 val_rmse: 0.4728\n",
      "Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 154 val_rmse: 0.4728\n",
      "Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 156 val_rmse: 0.4729\n",
      "Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 158 val_rmse: 0.4731\n",
      "Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 160 val_rmse: 0.473\n",
      "Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 162 val_rmse: 0.473\n",
      "Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 1.98 seconds\n",
      "Epoch: 2 batch_num: 164 val_rmse: 0.473\n",
      "Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 166 val_rmse: 0.473\n",
      "Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 1.96 seconds\n",
      "Epoch: 2 batch_num: 168 val_rmse: 0.4729\n",
      "Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 170 val_rmse: 0.4729\n",
      "Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 172 val_rmse: 0.4728\n",
      "Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 174 val_rmse: 0.4728\n",
      "Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 176 val_rmse: 0.4728\n",
      "Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 178 val_rmse: 0.4728\n",
      "Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 180 val_rmse: 0.4729\n",
      "Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 182 val_rmse: 0.4729\n",
      "Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 184 val_rmse: 0.4729\n",
      "Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 186 val_rmse: 0.4729\n",
      "Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "Performance estimates:\n",
      "[0.48920012553234143, 0.4878910677221497, 0.47193708101091125]\n",
      "Mean: 0.48300942475513414\n",
      "\n",
      "Fold 4/5\n",
      "\n",
      "16 steps took 17.1 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9465\n",
      "New best_val_rmse: 0.9465\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8389\n",
      "New best_val_rmse: 0.8389\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7239\n",
      "New best_val_rmse: 0.7239\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.78\n",
      "Still best_val_rmse: 0.7239 (from epoch 0)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.8456\n",
      "Still best_val_rmse: 0.7239 (from epoch 0)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7708\n",
      "Still best_val_rmse: 0.7239 (from epoch 0)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6559\n",
      "New best_val_rmse: 0.6559\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5863\n",
      "New best_val_rmse: 0.5863\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5863\n",
      "New best_val_rmse: 0.5863\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 160 val_rmse: 0.5374\n",
      "New best_val_rmse: 0.5374\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 176 val_rmse: 0.5313\n",
      "New best_val_rmse: 0.5313\n",
      "\n",
      "16 steps took 16.1 seconds\n",
      "Epoch: 1 batch_num: 4 val_rmse: 0.5294\n",
      "New best_val_rmse: 0.5294\n",
      "\n",
      "16 steps took 15.8 seconds\n",
      "Epoch: 1 batch_num: 20 val_rmse: 0.5428\n",
      "Still best_val_rmse: 0.5294 (from epoch 1)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 36 val_rmse: 0.56\n",
      "Still best_val_rmse: 0.5294 (from epoch 1)\n",
      "\n",
      "16 steps took 15.6 seconds\n",
      "Epoch: 1 batch_num: 52 val_rmse: 0.4825\n",
      "New best_val_rmse: 0.4825\n",
      "\n",
      "4 steps took 3.93 seconds\n",
      "Epoch: 1 batch_num: 56 val_rmse: 0.5007\n",
      "Still best_val_rmse: 0.4825 (from epoch 1)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 72 val_rmse: 0.5694\n",
      "Still best_val_rmse: 0.4825 (from epoch 1)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 88 val_rmse: 0.5114\n",
      "Still best_val_rmse: 0.4825 (from epoch 1)\n",
      "\n",
      "16 steps took 15.8 seconds\n",
      "Epoch: 1 batch_num: 104 val_rmse: 0.4989\n",
      "Still best_val_rmse: 0.4825 (from epoch 1)\n",
      "\n",
      "8 steps took 7.84 seconds\n",
      "Epoch: 1 batch_num: 112 val_rmse: 0.578\n",
      "Still best_val_rmse: 0.4825 (from epoch 1)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 128 val_rmse: 0.499\n",
      "Still best_val_rmse: 0.4825 (from epoch 1)\n",
      "\n",
      "8 steps took 7.84 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.4981\n",
      "Still best_val_rmse: 0.4825 (from epoch 1)\n",
      "\n",
      "8 steps took 7.82 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.5005\n",
      "Still best_val_rmse: 0.4825 (from epoch 1)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 160 val_rmse: 0.488\n",
      "Still best_val_rmse: 0.4825 (from epoch 1)\n",
      "\n",
      "4 steps took 3.92 seconds\n",
      "Epoch: 1 batch_num: 164 val_rmse: 0.4791\n",
      "New best_val_rmse: 0.4791\n",
      "\n",
      "2 steps took 1.98 seconds\n",
      "Epoch: 1 batch_num: 166 val_rmse: 0.4921\n",
      "Still best_val_rmse: 0.4791 (from epoch 1)\n",
      "\n",
      "8 steps took 7.82 seconds\n",
      "Epoch: 1 batch_num: 174 val_rmse: 0.4856\n",
      "Still best_val_rmse: 0.4791 (from epoch 1)\n",
      "\n",
      "4 steps took 3.92 seconds\n",
      "Epoch: 1 batch_num: 178 val_rmse: 0.5017\n",
      "Still best_val_rmse: 0.4791 (from epoch 1)\n",
      "\n",
      "16 steps took 16.2 seconds\n",
      "Epoch: 2 batch_num: 6 val_rmse: 0.5202\n",
      "Still best_val_rmse: 0.4791 (from epoch 1)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4777\n",
      "New best_val_rmse: 0.4777\n",
      "\n",
      "2 steps took 1.97 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4891\n",
      "Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "4 steps took 3.91 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.5058\n",
      "Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4954\n",
      "Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "8 steps took 7.83 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4784\n",
      "Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4791\n",
      "Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4832\n",
      "Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "4 steps took 3.9 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4824\n",
      "Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "4 steps took 3.91 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4796\n",
      "Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4794\n",
      "Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4766\n",
      "New best_val_rmse: 0.4766\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4761\n",
      "New best_val_rmse: 0.4761\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4728\n",
      "New best_val_rmse: 0.4728\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4716\n",
      "New best_val_rmse: 0.4716\n",
      "\n",
      "2 steps took 1.96 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4721\n",
      "Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4751\n",
      "Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4805\n",
      "Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "4 steps took 3.91 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4982\n",
      "Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "8 steps took 7.83 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4961\n",
      "Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "8 steps took 7.83 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4789\n",
      "Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4794\n",
      "Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4804\n",
      "Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "4 steps took 3.9 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4822\n",
      "Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "4 steps took 3.92 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4852\n",
      "Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "4 steps took 3.9 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4871\n",
      "Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "4 steps took 3.93 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4828\n",
      "Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "4 steps took 3.9 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4817\n",
      "Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "4 steps took 3.9 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4856\n",
      "Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "4 steps took 3.91 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4868\n",
      "Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "4 steps took 3.93 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4853\n",
      "Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "4 steps took 3.91 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4848\n",
      "Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "4 steps took 3.91 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4837\n",
      "Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "4 steps took 3.9 seconds\n",
      "Epoch: 2 batch_num: 148 val_rmse: 0.4824\n",
      "Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "4 steps took 3.93 seconds\n",
      "Epoch: 2 batch_num: 152 val_rmse: 0.4825\n",
      "Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "4 steps took 3.9 seconds\n",
      "Epoch: 2 batch_num: 156 val_rmse: 0.4836\n",
      "Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "4 steps took 3.91 seconds\n",
      "Epoch: 2 batch_num: 160 val_rmse: 0.484\n",
      "Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "4 steps took 3.92 seconds\n",
      "Epoch: 2 batch_num: 164 val_rmse: 0.4836\n",
      "Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "4 steps took 3.97 seconds\n",
      "Epoch: 2 batch_num: 168 val_rmse: 0.4829\n",
      "Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "4 steps took 3.9 seconds\n",
      "Epoch: 2 batch_num: 172 val_rmse: 0.4824\n",
      "Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "4 steps took 3.92 seconds\n",
      "Epoch: 2 batch_num: 176 val_rmse: 0.482\n",
      "Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "4 steps took 3.91 seconds\n",
      "Epoch: 2 batch_num: 180 val_rmse: 0.4819\n",
      "Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "4 steps took 3.94 seconds\n",
      "Epoch: 2 batch_num: 184 val_rmse: 0.4818\n",
      "Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "Performance estimates:\n",
      "[0.48920012553234143, 0.4878910677221497, 0.47193708101091125, 0.47155639213249717]\n",
      "Mean: 0.4801461665994749\n",
      "\n",
      "Fold 5/5\n",
      "\n",
      "16 steps took 17.0 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9401\n",
      "New best_val_rmse: 0.9401\n",
      "\n",
      "16 steps took 15.8 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7518\n",
      "New best_val_rmse: 0.7518\n",
      "\n",
      "16 steps took 15.8 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6527\n",
      "New best_val_rmse: 0.6527\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7696\n",
      "Still best_val_rmse: 0.6527 (from epoch 0)\n",
      "\n",
      "16 steps took 15.8 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7705\n",
      "Still best_val_rmse: 0.6527 (from epoch 0)\n",
      "\n",
      "16 steps took 15.8 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7363\n",
      "Still best_val_rmse: 0.6527 (from epoch 0)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6242\n",
      "New best_val_rmse: 0.6242\n",
      "\n",
      "16 steps took 15.9 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6186\n",
      "New best_val_rmse: 0.6186\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5987\n",
      "New best_val_rmse: 0.5987\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 160 val_rmse: 0.5546\n",
      "New best_val_rmse: 0.5546\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 176 val_rmse: 0.5378\n",
      "New best_val_rmse: 0.5378\n",
      "\n",
      "16 steps took 16.2 seconds\n",
      "Epoch: 1 batch_num: 4 val_rmse: 0.601\n",
      "Still best_val_rmse: 0.5378 (from epoch 0)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 20 val_rmse: 0.5449\n",
      "Still best_val_rmse: 0.5378 (from epoch 0)\n",
      "\n",
      "16 steps took 15.6 seconds\n",
      "Epoch: 1 batch_num: 36 val_rmse: 0.5398\n",
      "Still best_val_rmse: 0.5378 (from epoch 0)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 52 val_rmse: 0.5748\n",
      "Still best_val_rmse: 0.5378 (from epoch 0)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 68 val_rmse: 0.513\n",
      "New best_val_rmse: 0.513\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.4959\n",
      "New best_val_rmse: 0.4959\n",
      "\n",
      "8 steps took 7.84 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5143\n",
      "Still best_val_rmse: 0.4959 (from epoch 1)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.509\n",
      "Still best_val_rmse: 0.4959 (from epoch 1)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5244\n",
      "Still best_val_rmse: 0.4959 (from epoch 1)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5033\n",
      "Still best_val_rmse: 0.4959 (from epoch 1)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 156 val_rmse: 0.4883\n",
      "New best_val_rmse: 0.4883\n",
      "\n",
      "4 steps took 3.94 seconds\n",
      "Epoch: 1 batch_num: 160 val_rmse: 0.5154\n",
      "Still best_val_rmse: 0.4883 (from epoch 1)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 1 batch_num: 176 val_rmse: 0.5481\n",
      "Still best_val_rmse: 0.4883 (from epoch 1)\n",
      "\n",
      "16 steps took 16.3 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.5128\n",
      "Still best_val_rmse: 0.4883 (from epoch 1)\n",
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4805\n",
      "New best_val_rmse: 0.4805\n",
      "\n",
      "4 steps took 3.94 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4997\n",
      "Still best_val_rmse: 0.4805 (from epoch 2)\n",
      "\n",
      "8 steps took 7.82 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.483\n",
      "Still best_val_rmse: 0.4805 (from epoch 2)\n",
      "\n",
      "4 steps took 3.95 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4817\n",
      "Still best_val_rmse: 0.4805 (from epoch 2)\n",
      "\n",
      "4 steps took 3.91 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4802\n",
      "New best_val_rmse: 0.4802\n",
      "\n",
      "4 steps took 3.92 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4801\n",
      "New best_val_rmse: 0.4801\n",
      "\n",
      "4 steps took 3.98 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4824\n",
      "Still best_val_rmse: 0.4801 (from epoch 2)\n",
      "\n",
      "4 steps took 3.9 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4833\n",
      "Still best_val_rmse: 0.4801 (from epoch 2)\n",
      "\n",
      "4 steps took 3.9 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4809\n",
      "Still best_val_rmse: 0.4801 (from epoch 2)\n",
      "\n",
      "4 steps took 3.91 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4825\n",
      "Still best_val_rmse: 0.4801 (from epoch 2)\n",
      "\n",
      "4 steps took 3.91 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.482\n",
      "Still best_val_rmse: 0.4801 (from epoch 2)\n",
      "\n",
      "4 steps took 3.94 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4792\n",
      "New best_val_rmse: 0.4792\n",
      "\n",
      "2 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4795\n",
      "Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4813\n",
      "Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "4 steps took 3.91 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.482\n",
      "Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "4 steps took 3.95 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4824\n",
      "Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "4 steps took 3.92 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4829\n",
      "Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "4 steps took 3.9 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4817\n",
      "Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "4 steps took 3.91 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4827\n",
      "Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "4 steps took 3.92 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4799\n",
      "Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4791\n",
      "New best_val_rmse: 0.4791\n",
      "\n",
      "2 steps took 1.98 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4783\n",
      "New best_val_rmse: 0.4783\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4783\n",
      "New best_val_rmse: 0.4783\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4782\n",
      "New best_val_rmse: 0.4782\n",
      "\n",
      "2 steps took 1.97 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4781\n",
      "New best_val_rmse: 0.4781\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4785\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4788\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4791\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4793\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.97 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4798\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.98 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4799\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4798\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4795\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4793\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4791\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.479\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4789\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4789\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4789\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.97 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4791\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4792\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.96 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4791\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4792\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.96 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4796\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.96 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4799\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 148 val_rmse: 0.4799\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.96 seconds\n",
      "Epoch: 2 batch_num: 150 val_rmse: 0.48\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 152 val_rmse: 0.48\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "4 steps took 3.9 seconds\n",
      "Epoch: 2 batch_num: 156 val_rmse: 0.4801\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "4 steps took 3.91 seconds\n",
      "Epoch: 2 batch_num: 160 val_rmse: 0.48\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "4 steps took 3.94 seconds\n",
      "Epoch: 2 batch_num: 164 val_rmse: 0.4798\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 166 val_rmse: 0.4797\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 168 val_rmse: 0.4796\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 170 val_rmse: 0.4795\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 172 val_rmse: 0.4795\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 174 val_rmse: 0.4794\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 176 val_rmse: 0.4794\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 178 val_rmse: 0.4794\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 180 val_rmse: 0.4794\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 182 val_rmse: 0.4794\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.96 seconds\n",
      "Epoch: 2 batch_num: 184 val_rmse: 0.4794\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 186 val_rmse: 0.4794\n",
      "Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "Performance estimates:\n",
      "[0.48920012553234143, 0.4878910677221497, 0.47193708101091125, 0.47155639213249717, 0.47813402369272157]\n",
      "Mean: 0.47974373801812425\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "list_val_rmse = []\n",
    "\n",
    "kfold = KFold(n_splits=NUM_FOLDS, random_state=SEED, shuffle=True)\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(kfold.split(train_df)):    \n",
    "    print(f\"\\nFold {fold + 1}/{NUM_FOLDS}\")\n",
    "    model_path = f\"model_{fold + 1}.pth\"\n",
    "        \n",
    "    set_random_seed(SEED + fold)\n",
    "    \n",
    "    train_dataset = LitDataset(train_df.loc[train_indices])    \n",
    "    val_dataset = LitDataset(train_df.loc[val_indices])    \n",
    "        \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                              drop_last=True, shuffle=True, num_workers=WORKERS)    \n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
    "                            drop_last=False, shuffle=False, num_workers=WORKERS)    \n",
    "        \n",
    "    set_random_seed(SEED + fold)\n",
    "    \n",
    "    model = LitModel().to(DEVICE)\n",
    "    \n",
    "    optimizer = create_optimizer(model)                        \n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_training_steps=NUM_EPOCHS * len(train_loader), \n",
    "        num_warmup_steps=70)\n",
    "    \n",
    "    list_val_rmse.append(train(model, model_path, train_loader,\n",
    "                               val_loader, optimizer, scheduler=scheduler))\n",
    "\n",
    "    del model\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"\\nPerformance estimates:\")\n",
    "    print(list_val_rmse)\n",
    "    print(\"Mean:\", np.array(list_val_rmse).mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-sterling",
   "metadata": {
    "papermill": {
     "duration": 0.228109,
     "end_time": "2021-07-19T10:36:46.166264",
     "exception": false,
     "start_time": "2021-07-19T10:36:45.938155",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "convertible-affiliate",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T10:36:46.619105Z",
     "iopub.status.busy": "2021-07-19T10:36:46.617894Z",
     "iopub.status.idle": "2021-07-19T10:36:46.621816Z",
     "shell.execute_reply": "2021-07-19T10:36:46.621407Z"
    },
    "papermill": {
     "duration": 0.234352,
     "end_time": "2021-07-19T10:36:46.621932",
     "exception": false,
     "start_time": "2021-07-19T10:36:46.387580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = LitDataset(test_df, inference_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "loaded-passenger",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T10:36:47.087741Z",
     "iopub.status.busy": "2021-07-19T10:36:47.086711Z",
     "iopub.status.idle": "2021-07-19T10:38:43.644018Z",
     "shell.execute_reply": "2021-07-19T10:38:43.643481Z"
    },
    "papermill": {
     "duration": 116.804956,
     "end_time": "2021-07-19T10:38:43.644167",
     "exception": false,
     "start_time": "2021-07-19T10:36:46.839211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using model_1.pth\n",
      "\n",
      "Using model_2.pth\n",
      "\n",
      "Using model_3.pth\n",
      "\n",
      "Using model_4.pth\n",
      "\n",
      "Using model_5.pth\n"
     ]
    }
   ],
   "source": [
    "all_predictions = np.zeros((len(list_val_rmse), len(test_df)))\n",
    "\n",
    "test_dataset = LitDataset(test_df, inference_only=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                         drop_last=False, shuffle=False, num_workers=WORKERS)\n",
    "\n",
    "for index in range(len(list_val_rmse)):            \n",
    "    model_path = f\"model_{index + 1}.pth\"\n",
    "    print(f\"\\nUsing {model_path}\")\n",
    "                        \n",
    "    model = LitModel()\n",
    "    model.load_state_dict(torch.load(model_path))    \n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    all_predictions[index] = predict(model, test_loader)\n",
    "    \n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "concrete-reward",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T10:38:44.371296Z",
     "iopub.status.busy": "2021-07-19T10:38:44.370353Z",
     "iopub.status.idle": "2021-07-19T10:38:44.733513Z",
     "shell.execute_reply": "2021-07-19T10:38:44.732403Z"
    },
    "papermill": {
     "duration": 0.817546,
     "end_time": "2021-07-19T10:38:44.733643",
     "exception": false,
     "start_time": "2021-07-19T10:38:43.916097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id    target\n",
      "0  c0f722661 -0.348178\n",
      "1  f0953f0a5 -0.391592\n",
      "2  0df072751 -0.458750\n",
      "3  04caf4e0c -2.233941\n",
      "4  0e63f8bea -1.914501\n",
      "5  12537fe78 -1.273319\n",
      "6  965e592c0  0.352424\n"
     ]
    }
   ],
   "source": [
    "predictions = all_predictions.mean(axis=0)\n",
    "submission_df.target = predictions\n",
    "print(submission_df)\n",
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9034.754616,
   "end_time": "2021-07-19T10:38:48.133990",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-07-19T08:08:13.379374",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
