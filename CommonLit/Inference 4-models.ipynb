{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "comfortable-elements",
   "metadata": {
    "papermill": {
     "duration": 0.030851,
     "end_time": "2021-07-27T06:05:29.867314",
     "exception": false,
     "start_time": "2021-07-27T06:05:29.836463",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Overview\n",
    "\n",
    "This notebook combines two models:\n",
    "\n",
    "Score 0.467: [https://www.kaggle.com/andretugan/pre-trained-roberta-solution-in-pytorch](https://www.kaggle.com/andretugan/pre-trained-roberta-solution-in-pytorch)\n",
    "\n",
    "Score 0.468: [https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-infer-3](https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-infer-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-annual",
   "metadata": {
    "papermill": {
     "duration": 0.028492,
     "end_time": "2021-07-27T06:05:29.925802",
     "exception": false,
     "start_time": "2021-07-27T06:05:29.897310",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "metallic-showcase",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-07-27T06:05:29.997186Z",
     "iopub.status.busy": "2021-07-27T06:05:29.996259Z",
     "iopub.status.idle": "2021-07-27T06:05:33.066664Z",
     "shell.execute_reply": "2021-07-27T06:05:33.065586Z",
     "shell.execute_reply.started": "2021-07-27T05:53:54.068451Z"
    },
    "papermill": {
     "duration": 3.113869,
     "end_time": "2021-07-27T06:05:33.066837",
     "exception": false,
     "start_time": "2021-07-27T06:05:29.952968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoConfig\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import RandomSampler, SequentialSampler, Sampler\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "digital-vulnerability",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T06:05:33.165095Z",
     "iopub.status.busy": "2021-07-27T06:05:33.164274Z",
     "iopub.status.idle": "2021-07-27T06:05:33.167518Z",
     "shell.execute_reply": "2021-07-27T06:05:33.168318Z",
     "shell.execute_reply.started": "2021-07-27T05:53:56.838942Z"
    },
    "papermill": {
     "duration": 0.073833,
     "end_time": "2021-07-27T06:05:33.168536",
     "exception": false,
     "start_time": "2021-07-27T06:05:33.094703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 250\n",
    "EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n",
    "ROBERTA_PATH = \"/kaggle/input/roberta-base\"\n",
    "TOKENIZER_PATH = \"/kaggle/input/roberta-base\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "based-privacy",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T06:05:33.264333Z",
     "iopub.status.busy": "2021-07-27T06:05:33.259585Z",
     "iopub.status.idle": "2021-07-27T06:05:33.276568Z",
     "shell.execute_reply": "2021-07-27T06:05:33.275056Z",
     "shell.execute_reply.started": "2021-07-27T05:53:56.919158Z"
    },
    "papermill": {
     "duration": 0.076981,
     "end_time": "2021-07-27T06:05:33.276754",
     "exception": false,
     "start_time": "2021-07-27T06:05:33.199773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\n",
    "submission_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "arbitrary-kenya",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T06:05:33.378384Z",
     "iopub.status.busy": "2021-07-27T06:05:33.376451Z",
     "iopub.status.idle": "2021-07-27T06:05:33.685448Z",
     "shell.execute_reply": "2021-07-27T06:05:33.686175Z",
     "shell.execute_reply.started": "2021-07-27T05:53:56.944713Z"
    },
    "papermill": {
     "duration": 0.36591,
     "end_time": "2021-07-27T06:05:33.686413",
     "exception": false,
     "start_time": "2021-07-27T06:05:33.320503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "behind-publisher",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T06:05:33.789425Z",
     "iopub.status.busy": "2021-07-27T06:05:33.787907Z",
     "iopub.status.idle": "2021-07-27T06:05:33.791127Z",
     "shell.execute_reply": "2021-07-27T06:05:33.790662Z",
     "shell.execute_reply.started": "2021-07-27T05:53:57.141514Z"
    },
    "papermill": {
     "duration": 0.056734,
     "end_time": "2021-07-27T06:05:33.791254",
     "exception": false,
     "start_time": "2021-07-27T06:05:33.734520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_cell_magic\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "## define custom magic to save most useful classes and use them in inference notebook \n",
    "## instead of copying the code every time you have changes in the classes\n",
    "@register_cell_magic\n",
    "def write_and_run(line, cell):\n",
    "    argz = line.split()\n",
    "    file = argz[-1]\n",
    "    mode = 'w'\n",
    "    if len(argz) == 2 and argz[0] == '-a':\n",
    "        mode = 'a'\n",
    "    with open(file, mode) as f:\n",
    "        f.write(cell)\n",
    "    get_ipython().run_cell(cell)\n",
    "    \n",
    "Path('/kaggle/working/scripts').mkdir(exist_ok=True)\n",
    "models_dir = Path('/kaggle/working/models')\n",
    "models_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "complex-western",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T06:05:33.851297Z",
     "iopub.status.busy": "2021-07-27T06:05:33.850473Z",
     "iopub.status.idle": "2021-07-27T06:05:33.854928Z",
     "shell.execute_reply": "2021-07-27T06:05:33.854484Z",
     "shell.execute_reply.started": "2021-07-27T05:53:57.149766Z"
    },
    "papermill": {
     "duration": 0.035475,
     "end_time": "2021-07-27T06:05:33.855045",
     "exception": false,
     "start_time": "2021-07-27T06:05:33.819570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%write_and_run scripts/config.py\n",
    "\n",
    "class Config:\n",
    "    model_name = 'roberta-large'\n",
    "    output_hidden_states = True\n",
    "    epochs = 5\n",
    "#     evaluate_interval = 40\n",
    "    batch_size = 8\n",
    "    device = 'cuda'\n",
    "    seed = 42\n",
    "    max_len = 256\n",
    "    lr = 1e-5\n",
    "    wd = 0.01\n",
    "#     eval_schedule = [(float('inf'), 40), (0.5, 30), (0.49, 20), (0.48, 10), (0.47, 3), (0, 0)]\n",
    "    eval_schedule = [(float('inf'), 40), (0.47, 20), (0.46, 10), (0, 0)]\n",
    "\n",
    "    gradient_accumulation = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-paragraph",
   "metadata": {
    "papermill": {
     "duration": 0.030351,
     "end_time": "2021-07-27T06:05:33.915219",
     "exception": false,
     "start_time": "2021-07-27T06:05:33.884868",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "individual-kennedy",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T06:05:33.979188Z",
     "iopub.status.busy": "2021-07-27T06:05:33.978237Z",
     "iopub.status.idle": "2021-07-27T06:05:33.981103Z",
     "shell.execute_reply": "2021-07-27T06:05:33.980640Z",
     "shell.execute_reply.started": "2021-07-27T05:53:57.162107Z"
    },
    "papermill": {
     "duration": 0.038345,
     "end_time": "2021-07-27T06:05:33.981224",
     "exception": false,
     "start_time": "2021-07-27T06:05:33.942879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LitDataset(Dataset):\n",
    "    def __init__(self, df, inference_only=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.df = df        \n",
    "        self.inference_only = inference_only\n",
    "        self.text = df.excerpt.tolist()\n",
    "        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n",
    "        \n",
    "        if not self.inference_only:\n",
    "            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n",
    "    \n",
    "        self.encoded = tokenizer.batch_encode_plus(\n",
    "            self.text,\n",
    "            padding = 'max_length',            \n",
    "            max_length = MAX_LEN,\n",
    "            truncation = True,\n",
    "            return_attention_mask=True\n",
    "        )        \n",
    " \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):        \n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][index])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n",
    "        \n",
    "        if self.inference_only:\n",
    "            return (input_ids, attention_mask)            \n",
    "        else:\n",
    "            target = self.target[index]\n",
    "            return (input_ids, attention_mask, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-damage",
   "metadata": {
    "papermill": {
     "duration": 0.027119,
     "end_time": "2021-07-27T06:05:34.039357",
     "exception": false,
     "start_time": "2021-07-27T06:05:34.012238",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model\n",
    "The model is inspired by the one from [Maunish](https://www.kaggle.com/maunish/clrp-roberta-svm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "large-variation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T06:05:34.105371Z",
     "iopub.status.busy": "2021-07-27T06:05:34.104653Z",
     "iopub.status.idle": "2021-07-27T06:05:34.108247Z",
     "shell.execute_reply": "2021-07-27T06:05:34.107741Z",
     "shell.execute_reply.started": "2021-07-27T05:53:57.174953Z"
    },
    "papermill": {
     "duration": 0.04215,
     "end_time": "2021-07-27T06:05:34.108367",
     "exception": false,
     "start_time": "2021-07-27T06:05:34.066217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n",
    "        config.update({\"output_hidden_states\":True, \n",
    "                       \"hidden_dropout_prob\": 0.0,\n",
    "                       \"layer_norm_eps\": 1e-7})                       \n",
    "        \n",
    "        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n",
    "            \n",
    "        self.attention = nn.Sequential(            \n",
    "            nn.Linear(768, 512),            \n",
    "            nn.Tanh(),                       \n",
    "            nn.Linear(512, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )        \n",
    "\n",
    "        self.regressor = nn.Sequential(                        \n",
    "            nn.Linear(768, 1)                        \n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        roberta_output = self.roberta(input_ids=input_ids,\n",
    "                                      attention_mask=attention_mask)        \n",
    "\n",
    "        # There are a total of 13 layers of hidden states.\n",
    "        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n",
    "        # We take the hidden states from the last Roberta layer.\n",
    "        last_layer_hidden_states = roberta_output.hidden_states[-1]\n",
    "\n",
    "        # The number of cells is MAX_LEN.\n",
    "        # The size of the hidden state of each cell is 768 (for roberta-base).\n",
    "        # In order to condense hidden states of all cells to a context vector,\n",
    "        # we compute a weighted average of the hidden states of all cells.\n",
    "        # We compute the weight of each cell, using the attention neural network.\n",
    "        weights = self.attention(last_layer_hidden_states)\n",
    "                \n",
    "        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n",
    "        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n",
    "        # Now we compute context_vector as the weighted average.\n",
    "        # context_vector.shape is BATCH_SIZE x 768\n",
    "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n",
    "        \n",
    "        # Now we reduce the context vector to the prediction score.\n",
    "        return self.regressor(context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "commercial-metabolism",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T06:05:34.170661Z",
     "iopub.status.busy": "2021-07-27T06:05:34.170081Z",
     "iopub.status.idle": "2021-07-27T06:05:34.173376Z",
     "shell.execute_reply": "2021-07-27T06:05:34.173799Z",
     "shell.execute_reply.started": "2021-07-27T05:53:57.186428Z"
    },
    "papermill": {
     "duration": 0.037268,
     "end_time": "2021-07-27T06:05:34.173929",
     "exception": false,
     "start_time": "2021-07-27T06:05:34.136661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    result = np.zeros(len(data_loader.dataset))    \n",
    "    index = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n",
    "            input_ids = input_ids.to(DEVICE)\n",
    "            attention_mask = attention_mask.to(DEVICE)\n",
    "                        \n",
    "            pred = model(input_ids, attention_mask)                        \n",
    "\n",
    "            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n",
    "            index += pred.shape[0]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-bradley",
   "metadata": {
    "papermill": {
     "duration": 0.029247,
     "end_time": "2021-07-27T06:05:34.233014",
     "exception": false,
     "start_time": "2021-07-27T06:05:34.203767",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "binary-melbourne",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T06:05:34.298065Z",
     "iopub.status.busy": "2021-07-27T06:05:34.297292Z",
     "iopub.status.idle": "2021-07-27T06:05:34.313544Z",
     "shell.execute_reply": "2021-07-27T06:05:34.313050Z",
     "shell.execute_reply.started": "2021-07-27T05:53:57.202532Z"
    },
    "papermill": {
     "duration": 0.053292,
     "end_time": "2021-07-27T06:05:34.313679",
     "exception": false,
     "start_time": "2021-07-27T06:05:34.260387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = LitDataset(test_df, inference_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "young-interface",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T06:05:34.380279Z",
     "iopub.status.busy": "2021-07-27T06:05:34.379136Z",
     "iopub.status.idle": "2021-07-27T06:06:23.902269Z",
     "shell.execute_reply": "2021-07-27T06:06:23.901797Z",
     "shell.execute_reply.started": "2021-07-27T05:53:57.230144Z"
    },
    "papermill": {
     "duration": 49.55929,
     "end_time": "2021-07-27T06:06:23.902401",
     "exception": false,
     "start_time": "2021-07-27T06:05:34.343111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using ../input/commonlit-roberta-0467/model_1.pth\n",
      "\n",
      "Using ../input/commonlit-roberta-0467/model_2.pth\n",
      "\n",
      "Using ../input/commonlit-roberta-0467/model_3.pth\n",
      "\n",
      "Using ../input/commonlit-roberta-0467/model_4.pth\n",
      "\n",
      "Using ../input/commonlit-roberta-0467/model_5.pth\n"
     ]
    }
   ],
   "source": [
    "NUM_MODELS = 5\n",
    "\n",
    "all_predictions = np.zeros((NUM_MODELS, len(test_df)))\n",
    "\n",
    "\n",
    "\n",
    "test_dataset = LitDataset(test_df, inference_only=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                         drop_last=False, shuffle=False, num_workers=2)\n",
    "\n",
    "for model_index in range(NUM_MODELS):            \n",
    "    model_path = f\"../input/commonlit-roberta-0467/model_{model_index + 1}.pth\"\n",
    "    print(f\"\\nUsing {model_path}\")\n",
    "                        \n",
    "    model = LitModel()\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))    \n",
    "    model.to(DEVICE)\n",
    "        \n",
    "    all_predictions[model_index] = predict(model, test_loader)\n",
    "            \n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "attended-above",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T06:06:23.994201Z",
     "iopub.status.busy": "2021-07-27T06:06:23.993444Z",
     "iopub.status.idle": "2021-07-27T06:06:23.996620Z",
     "shell.execute_reply": "2021-07-27T06:06:23.997048Z",
     "shell.execute_reply.started": "2021-07-27T05:54:51.146451Z"
    },
    "papermill": {
     "duration": 0.063893,
     "end_time": "2021-07-27T06:06:23.997198",
     "exception": false,
     "start_time": "2021-07-27T06:06:23.933305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model1_predictions = all_predictions.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "limiting-anger",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T06:06:24.062083Z",
     "iopub.status.busy": "2021-07-27T06:06:24.061443Z",
     "iopub.status.idle": "2021-07-27T06:06:24.065305Z",
     "shell.execute_reply": "2021-07-27T06:06:24.065801Z",
     "shell.execute_reply.started": "2021-07-27T05:54:51.153390Z"
    },
    "papermill": {
     "duration": 0.040202,
     "end_time": "2021-07-27T06:06:24.065959",
     "exception": false,
     "start_time": "2021-07-27T06:06:24.025757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROBERTA_LARGE_PATH = \"../input/clrp-roberta-large/clrp_roberta_large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "comprehensive-recruitment",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T06:06:24.134526Z",
     "iopub.status.busy": "2021-07-27T06:06:24.132755Z",
     "iopub.status.idle": "2021-07-27T06:06:24.135236Z",
     "shell.execute_reply": "2021-07-27T06:06:24.135704Z",
     "shell.execute_reply.started": "2021-07-27T05:54:51.163123Z"
    },
    "papermill": {
     "duration": 0.040274,
     "end_time": "2021-07-27T06:06:24.135846",
     "exception": false,
     "start_time": "2021-07-27T06:06:24.095572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LitModelBig(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(ROBERTA_LARGE_PATH)\n",
    "        config.update({\"output_hidden_states\":True, \n",
    "                       \"hidden_dropout_prob\": 0.0,\n",
    "                       \"layer_norm_eps\": 1e-7})                       \n",
    "        \n",
    "        self.roberta = AutoModel.from_pretrained(ROBERTA_LARGE_PATH, config=config)  \n",
    "            \n",
    "        self.attention = nn.Sequential(            \n",
    "            nn.Linear(1024, 512),            \n",
    "            nn.Tanh(),                       \n",
    "            nn.Linear(512, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )        \n",
    "\n",
    "        self.regressor = nn.Sequential(                        \n",
    "            nn.Linear(1024, 1)                        \n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        roberta_output = self.roberta(input_ids=input_ids,\n",
    "                                      attention_mask=attention_mask)        \n",
    "\n",
    "        # There are a total of 13 layers of hidden states.\n",
    "        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n",
    "        # We take the hidden states from the last Roberta layer.\n",
    "        last_layer_hidden_states = roberta_output.hidden_states[-1]\n",
    "\n",
    "        # The number of cells is MAX_LEN.\n",
    "        # The size of the hidden state of each cell is 768 (for roberta-base).\n",
    "        # In order to condense hidden states of all cells to a context vector,\n",
    "        # we compute a weighted average of the hidden states of all cells.\n",
    "        # We compute the weight of each cell, using the attention neural network.\n",
    "        weights = self.attention(last_layer_hidden_states)\n",
    "                \n",
    "        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n",
    "        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n",
    "        # Now we compute context_vector as the weighted average.\n",
    "        # context_vector.shape is BATCH_SIZE x 768\n",
    "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n",
    "        \n",
    "        # Now we reduce the context vector to the prediction score.\n",
    "        return self.regressor(context_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-affair",
   "metadata": {
    "papermill": {
     "duration": 0.029601,
     "end_time": "2021-07-27T06:06:24.197799",
     "exception": false,
     "start_time": "2021-07-27T06:06:24.168198",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "growing-agent",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T06:06:24.269238Z",
     "iopub.status.busy": "2021-07-27T06:06:24.267992Z",
     "iopub.status.idle": "2021-07-27T06:08:50.248281Z",
     "shell.execute_reply": "2021-07-27T06:08:50.247374Z",
     "shell.execute_reply.started": "2021-07-27T05:54:51.174613Z"
    },
    "papermill": {
     "duration": 146.021728,
     "end_time": "2021-07-27T06:08:50.248426",
     "exception": false,
     "start_time": "2021-07-27T06:06:24.226698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using ../input/roberta-large-250-seeds-2e55e57e5-0-261-309/model_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using ../input/roberta-large-250-seeds-2e55e57e5-0-261-309/model_2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using ../input/roberta-large-250-seeds-2e55e57e5-0-261-309/model_3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using ../input/roberta-large-250-seeds-2e55e57e5-0-261-309/model_4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using ../input/roberta-large-250-seeds-2e55e57e5-0-261-309/model_5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "NUM_MODELS = 5\n",
    "\n",
    "all_predictions = np.zeros((NUM_MODELS, len(test_df)))\n",
    "\n",
    "\n",
    "\n",
    "test_dataset = LitDataset(test_df, inference_only=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                         drop_last=False, shuffle=False, num_workers=2)\n",
    "\n",
    "for model_index in range(NUM_MODELS):            \n",
    "    model_path = f\"../input/roberta-large-250-seeds-2e55e57e5-0-261-309/model_{model_index + 1}.pth\"\n",
    "    print(f\"\\nUsing {model_path}\")\n",
    "                        \n",
    "    model = LitModelBig()\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))    \n",
    "    model.to(DEVICE)\n",
    "        \n",
    "    all_predictions[model_index] = predict(model, test_loader)\n",
    "            \n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "alpine-truck",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T06:08:50.315421Z",
     "iopub.status.busy": "2021-07-27T06:08:50.314048Z",
     "iopub.status.idle": "2021-07-27T06:08:50.316318Z",
     "shell.execute_reply": "2021-07-27T06:08:50.316807Z",
     "shell.execute_reply.started": "2021-07-27T05:57:21.871975Z"
    },
    "papermill": {
     "duration": 0.037329,
     "end_time": "2021-07-27T06:08:50.316950",
     "exception": false,
     "start_time": "2021-07-27T06:08:50.279621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model3_predictions = all_predictions.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-asthma",
   "metadata": {
    "papermill": {
     "duration": 0.03041,
     "end_time": "2021-07-27T06:08:50.377974",
     "exception": false,
     "start_time": "2021-07-27T06:08:50.347564",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# FineTune Roberta Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "virgin-swing",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T06:08:50.443842Z",
     "iopub.status.busy": "2021-07-27T06:08:50.443268Z",
     "iopub.status.idle": "2021-07-27T06:08:50.451520Z",
     "shell.execute_reply": "2021-07-27T06:08:50.451103Z",
     "shell.execute_reply.started": "2021-07-27T05:57:21.880162Z"
    },
    "papermill": {
     "duration": 0.043001,
     "end_time": "2021-07-27T06:08:50.451640",
     "exception": false,
     "start_time": "2021-07-27T06:08:50.408639",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%write_and_run scripts/dataset.py\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "def convert_examples_to_features(text, tokenizer, max_len):\n",
    "\n",
    "    tok = tokenizer.encode_plus(\n",
    "        text, \n",
    "        max_length=max_len, \n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "    )\n",
    "    return tok\n",
    "\n",
    "\n",
    "class CLRPDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len, is_test=False):\n",
    "        self.data = data\n",
    "        self.excerpts = self.data.excerpt.tolist()\n",
    "        if not is_test:\n",
    "            self.targets = self.data.target.tolist()\n",
    "            \n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_test = is_test\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        if not self.is_test:\n",
    "            excerpt = self.excerpts[item]\n",
    "            label = self.targets[item]\n",
    "            features = convert_examples_to_features(\n",
    "                excerpt, self.tokenizer, self.max_len\n",
    "            )\n",
    "            return {\n",
    "                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n",
    "                'label':torch.tensor(label, dtype=torch.float),\n",
    "            }\n",
    "        else:\n",
    "            excerpt = self.excerpts[item]\n",
    "            features = convert_examples_to_features(\n",
    "                excerpt, self.tokenizer, self.max_len\n",
    "            )\n",
    "            return {\n",
    "                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eastern-henry",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T06:08:50.517983Z",
     "iopub.status.busy": "2021-07-27T06:08:50.517306Z",
     "iopub.status.idle": "2021-07-27T06:08:50.520683Z",
     "shell.execute_reply": "2021-07-27T06:08:50.520281Z",
     "shell.execute_reply.started": "2021-07-27T05:57:21.893897Z"
    },
    "papermill": {
     "duration": 0.038558,
     "end_time": "2021-07-27T06:08:50.520789",
     "exception": false,
     "start_time": "2021-07-27T06:08:50.482231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_dataloader(data, tokenizer, is_train=True):\n",
    "    dataset = CLRPDataset(data, tokenizer=tokenizer, max_len=Config.max_len,is_test=True)\n",
    "    if is_train:\n",
    "        sampler = RandomSampler(dataset)\n",
    "    else:\n",
    "        sampler = SequentialSampler(dataset)\n",
    "\n",
    "    batch_dataloader = DataLoader(dataset, sampler=sampler, batch_size=Config.batch_size, pin_memory=True, collate_fn=DynamicPadCollate())\n",
    "    return batch_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "structured-mexico",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T06:08:50.591231Z",
     "iopub.status.busy": "2021-07-27T06:08:50.589963Z",
     "iopub.status.idle": "2021-07-27T06:08:50.592276Z",
     "shell.execute_reply": "2021-07-27T06:08:50.592689Z",
     "shell.execute_reply.started": "2021-07-27T05:57:21.906989Z"
    },
    "papermill": {
     "duration": 0.041752,
     "end_time": "2021-07-27T06:08:50.592810",
     "exception": false,
     "start_time": "2021-07-27T06:08:50.551058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DynamicPadCollate:\n",
    "    def __call__(self,batch):\n",
    "                \n",
    "        out = {'input_ids' :[],\n",
    "               'attention_mask':[],\n",
    "                'label':[]\n",
    "        }\n",
    "        \n",
    "        for i in batch:\n",
    "            for k,v in i.items():\n",
    "                out[k].append(v)\n",
    "                \n",
    "        max_pad =0\n",
    "\n",
    "        for p in out['input_ids']:\n",
    "            if max_pad < len(p):\n",
    "                max_pad = len(p)\n",
    "                    \n",
    "\n",
    "        for i in range(len(batch)):\n",
    "            \n",
    "            input_id = out['input_ids'][i]\n",
    "            att_mask = out['attention_mask'][i]\n",
    "            text_len = len(input_id)\n",
    "            \n",
    "            out['input_ids'][i] = (out['input_ids'][i].tolist() + [1] * (max_pad - text_len))[:max_pad]\n",
    "            out['attention_mask'][i] = (out['attention_mask'][i].tolist() + [0] * (max_pad - text_len))[:max_pad]\n",
    "        \n",
    "        out['input_ids'] = torch.tensor(out['input_ids'],dtype=torch.long)\n",
    "        out['attention_mask'] = torch.tensor(out['attention_mask'],dtype=torch.long)\n",
    "        out['label'] = torch.tensor(out['label'],dtype=torch.float)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "differential-alliance",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T06:08:50.665606Z",
     "iopub.status.busy": "2021-07-27T06:08:50.658145Z",
     "iopub.status.idle": "2021-07-27T06:08:50.669025Z",
     "shell.execute_reply": "2021-07-27T06:08:50.668630Z",
     "shell.execute_reply.started": "2021-07-27T05:57:21.920084Z"
    },
    "papermill": {
     "duration": 0.045856,
     "end_time": "2021-07-27T06:08:50.669140",
     "exception": false,
     "start_time": "2021-07-27T06:08:50.623284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%write_and_run scripts/model.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, h_size, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(h_size, hidden_dim)\n",
    "        self.V = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, features):\n",
    "        att = torch.tanh(self.W(features))\n",
    "        score = self.V(att)\n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = torch.sum(context_vector, dim=1)\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "class CLRPModel(nn.Module):\n",
    "    def __init__(self,transformer,config):\n",
    "        super(CLRPModel,self).__init__()\n",
    "        self.h_size = config.hidden_size\n",
    "        self.transformer = transformer\n",
    "        self.head = AttentionHead(self.h_size*4)\n",
    "        self.linear = nn.Linear(self.h_size*2, 1)\n",
    "        self.linear_out = nn.Linear(self.h_size*8, 1)\n",
    "\n",
    "              \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        transformer_out = self.transformer(input_ids, attention_mask)\n",
    "       \n",
    "        all_hidden_states = torch.stack(transformer_out.hidden_states)\n",
    "        cat_over_last_layers = torch.cat(\n",
    "            (all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]),-1\n",
    "        )\n",
    "        \n",
    "        cls_pooling = cat_over_last_layers[:, 0]   \n",
    "        head_logits = self.head(cat_over_last_layers)\n",
    "        y_hat = self.linear_out(torch.cat([head_logits, cls_pooling], -1))\n",
    "        \n",
    "        return y_hat\n",
    "def create_optimizer(model):\n",
    "    named_parameters = list(model.named_parameters())    \n",
    "    \n",
    "    roberta_parameters = named_parameters[:389]    \n",
    "    attention_parameters = named_parameters[391:395]\n",
    "    regressor_parameters = named_parameters[395:]\n",
    "        \n",
    "    attention_group = [params for (name, params) in attention_parameters]\n",
    "    regressor_group = [params for (name, params) in regressor_parameters]\n",
    "\n",
    "    parameters = []\n",
    "    parameters.append({\"params\": attention_group})\n",
    "    parameters.append({\"params\": regressor_group})\n",
    "    \n",
    "    # increase lr every second layer\n",
    "    increase_lr_every_k_layer = 1\n",
    "    lrs = np.linspace(1, 5, 24 // increase_lr_every_k_layer)\n",
    "    for layer_num, (name, params) in enumerate(roberta_parameters):\n",
    "        weight_decay = 0.0 if \"bias\" in name else 0.01\n",
    "        splitted_name = name.split('.')\n",
    "        lr = Config.lr\n",
    "        if len(splitted_name) >= 4 and str.isdigit(splitted_name[3]):\n",
    "            layer_num = int(splitted_name[3])\n",
    "            lr = lrs[layer_num // increase_lr_every_k_layer] * Config.lr \n",
    "\n",
    "        parameters.append({\"params\": params,\n",
    "                           \"weight_decay\": weight_decay,\n",
    "                           \"lr\": lr})\n",
    "\n",
    "    return optim.AdamW(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "indonesian-trial",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T06:08:50.737145Z",
     "iopub.status.busy": "2021-07-27T06:08:50.735776Z",
     "iopub.status.idle": "2021-07-27T06:08:50.738217Z",
     "shell.execute_reply": "2021-07-27T06:08:50.738637Z",
     "shell.execute_reply.started": "2021-07-27T05:57:21.936043Z"
    },
    "papermill": {
     "duration": 0.039034,
     "end_time": "2021-07-27T06:08:50.738762",
     "exception": false,
     "start_time": "2021-07-27T06:08:50.699728",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_finetuned(model, data_loader):\n",
    "    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    result = np.zeros(len(data_loader.dataset))    \n",
    "    index = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, batch in enumerate(data_loader):\n",
    "            input_ids, attention_mask = batch['input_ids'], batch['attention_mask']\n",
    "            #(input_ids, attention_mask) \n",
    "            input_ids = input_ids.to(DEVICE)\n",
    "            attention_mask = attention_mask.to(DEVICE)\n",
    "                        \n",
    "            pred = model(input_ids, attention_mask)                        \n",
    "\n",
    "            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n",
    "            index += pred.shape[0]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ranking-science",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T06:08:50.806619Z",
     "iopub.status.busy": "2021-07-27T06:08:50.805834Z",
     "iopub.status.idle": "2021-07-27T06:10:29.259076Z",
     "shell.execute_reply": "2021-07-27T06:10:29.258522Z",
     "shell.execute_reply.started": "2021-07-27T05:57:21.949222Z"
    },
    "papermill": {
     "duration": 98.490148,
     "end_time": "2021-07-27T06:10:29.259230",
     "exception": false,
     "start_time": "2021-07-27T06:08:50.769082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using ../input/finetune-roberta-large-public/models/best_model_0.pth\n",
      "\n",
      "Using ../input/finetune-roberta-large-public/models/best_model_1.pth\n",
      "\n",
      "Using ../input/finetune-roberta-large-public/models/best_model_2.pth\n",
      "\n",
      "Using ../input/finetune-roberta-large-public/models/best_model_3.pth\n",
      "\n",
      "Using ../input/finetune-roberta-large-public/models/best_model_4.pth\n"
     ]
    }
   ],
   "source": [
    "NUM_MODELS = 5\n",
    "import copy\n",
    "all_predictions = np.zeros((NUM_MODELS, len(test_df)))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ROBERTA_LARGE_PATH)\n",
    "config = AutoConfig.from_pretrained(ROBERTA_LARGE_PATH)\n",
    "config.update({\n",
    "            \"hidden_dropout_prob\": 0.0,\n",
    "            \"layer_norm_eps\": 1e-7,\n",
    "            \"output_hidden_states\": True\n",
    "            }) \n",
    "test_dl = make_dataloader(test_df, tokenizer, is_train=False)\n",
    "transformer = AutoModel.from_pretrained(ROBERTA_LARGE_PATH, config=config)  \n",
    "\n",
    "for model_index in range(NUM_MODELS):            \n",
    "    model_path = f\"../input/finetune-roberta-large-public/models/best_model_{model_index}.pth\" #../input/finetune-roberta-large-public/models/best_model_0.pth\n",
    "    print(f\"\\nUsing {model_path}\")\n",
    "                        \n",
    "    model = CLRPModel(transformer, config)\n",
    "    #model.load_state_dict(torch.load(model_path, map_location=DEVICE))    \n",
    "    model.load_state_dict(torch.load(model_path,DEVICE))\n",
    "    model.to(DEVICE)\n",
    "        \n",
    "    all_predictions[model_index] = predict_finetuned(model, test_dl)\n",
    "            \n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cloudy-surge",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T06:10:29.329863Z",
     "iopub.status.busy": "2021-07-27T06:10:29.329296Z",
     "iopub.status.idle": "2021-07-27T06:10:29.333327Z",
     "shell.execute_reply": "2021-07-27T06:10:29.332881Z",
     "shell.execute_reply.started": "2021-07-27T05:58:35.111333Z"
    },
    "papermill": {
     "duration": 0.040318,
     "end_time": "2021-07-27T06:10:29.333435",
     "exception": false,
     "start_time": "2021-07-27T06:10:29.293117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model5_predictions = all_predictions.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-consciousness",
   "metadata": {
    "papermill": {
     "duration": 0.032838,
     "end_time": "2021-07-27T06:10:29.403022",
     "exception": false,
     "start_time": "2021-07-27T06:10:29.370184",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 2\n",
    "Imported from [https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-infer-3](https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-infer-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "clinical-fancy",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T06:10:29.510820Z",
     "iopub.status.busy": "2021-07-27T06:10:29.491201Z",
     "iopub.status.idle": "2021-07-27T06:10:34.290465Z",
     "shell.execute_reply": "2021-07-27T06:10:34.289579Z",
     "shell.execute_reply.started": "2021-07-27T05:58:35.120661Z"
    },
    "papermill": {
     "duration": 4.855389,
     "end_time": "2021-07-27T06:10:34.290627",
     "exception": false,
     "start_time": "2021-07-27T06:10:29.435238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = test_df\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import (\n",
    "    Dataset, DataLoader, \n",
    "    SequentialSampler, RandomSampler\n",
    ")\n",
    "from transformers import RobertaConfig\n",
    "from transformers import (\n",
    "    get_cosine_schedule_with_warmup, \n",
    "    get_cosine_with_hard_restarts_schedule_with_warmup\n",
    ")\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import RobertaModel\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "def convert_examples_to_features(data, tokenizer, max_len, is_test=False):\n",
    "    data = data.replace('\\n', '')\n",
    "    tok = tokenizer.encode_plus(\n",
    "        data, \n",
    "        max_length=max_len, \n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=True\n",
    "    )\n",
    "    curr_sent = {}\n",
    "    padding_length = max_len - len(tok['input_ids'])\n",
    "    curr_sent['input_ids'] = tok['input_ids'] + ([0] * padding_length)\n",
    "    curr_sent['token_type_ids'] = tok['token_type_ids'] + \\\n",
    "        ([0] * padding_length)\n",
    "    curr_sent['attention_mask'] = tok['attention_mask'] + \\\n",
    "        ([0] * padding_length)\n",
    "    return curr_sent\n",
    "\n",
    "class DatasetRetriever(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len, is_test=False):\n",
    "        self.data = data\n",
    "        self.excerpts = self.data.excerpt.values.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_test = is_test\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        if not self.is_test:\n",
    "            excerpt, label = self.excerpts[item], self.targets[item]\n",
    "            features = convert_examples_to_features(\n",
    "                excerpt, self.tokenizer, \n",
    "                self.max_len, self.is_test\n",
    "            )\n",
    "            return {\n",
    "                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n",
    "                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n",
    "                'label':torch.tensor(label, dtype=torch.double),\n",
    "            }\n",
    "        else:\n",
    "            excerpt = self.excerpts[item]\n",
    "            features = convert_examples_to_features(\n",
    "                excerpt, self.tokenizer, \n",
    "                self.max_len, self.is_test\n",
    "            )\n",
    "            return {\n",
    "                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n",
    "                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n",
    "            }\n",
    "\n",
    "class CommonLitModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name, \n",
    "        config,  \n",
    "        multisample_dropout=False,\n",
    "        output_hidden_states=False\n",
    "    ):\n",
    "        super(CommonLitModel, self).__init__()\n",
    "        self.config = config\n",
    "        self.roberta = RobertaModel.from_pretrained(\n",
    "            model_name, \n",
    "            output_hidden_states=output_hidden_states\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size)\n",
    "        if multisample_dropout:\n",
    "            self.dropouts = nn.ModuleList([\n",
    "                nn.Dropout(0.5) for _ in range(5)\n",
    "            ])\n",
    "        else:\n",
    "            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n",
    "        self.regressor = nn.Linear(config.hidden_size, 1)\n",
    "        self._init_weights(self.layer_norm)\n",
    "        self._init_weights(self.regressor)\n",
    " \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    " \n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        labels=None\n",
    "    ):\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        sequence_output = outputs[1]\n",
    "        sequence_output = self.layer_norm(sequence_output)\n",
    " \n",
    "        # multi-sample dropout\n",
    "        for i, dropout in enumerate(self.dropouts):\n",
    "            if i == 0:\n",
    "                logits = self.regressor(dropout(sequence_output))\n",
    "            else:\n",
    "                logits += self.regressor(dropout(sequence_output))\n",
    "        \n",
    "        logits /= len(self.dropouts)\n",
    " \n",
    "        # calculate loss\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = torch.nn.MSELoss()\n",
    "            logits = logits.view(-1).to(labels.dtype)\n",
    "            loss = torch.sqrt(loss_fn(logits, labels.view(-1)))\n",
    "        \n",
    "        output = (logits,) + outputs[1:]\n",
    "        return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "def make_model(model_name, num_labels=1):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "    config = RobertaConfig.from_pretrained(model_name)\n",
    "    config.update({'num_labels':num_labels})\n",
    "    model = CommonLitModel(model_name, config=config)\n",
    "    return model, tokenizer\n",
    "\n",
    "def make_loader(\n",
    "    data, \n",
    "    tokenizer, \n",
    "    max_len,\n",
    "    batch_size,\n",
    "):\n",
    "    \n",
    "    test_dataset = DatasetRetriever(data, tokenizer, max_len, is_test=True)\n",
    "    test_sampler = SequentialSampler(test_dataset)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size // 2, \n",
    "        sampler=test_sampler, \n",
    "        pin_memory=False, \n",
    "        drop_last=False, \n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    return test_loader\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, model, scalar=None):\n",
    "        self.model = model\n",
    "        self.scalar = scalar\n",
    "\n",
    "    def evaluate(self, data_loader, tokenizer):\n",
    "        preds = []\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch_data in enumerate(data_loader):\n",
    "                input_ids, attention_mask, token_type_ids = batch_data['input_ids'], \\\n",
    "                    batch_data['attention_mask'], batch_data['token_type_ids']\n",
    "                input_ids, attention_mask, token_type_ids = input_ids.cuda(), \\\n",
    "                    attention_mask.cuda(), token_type_ids.cuda()\n",
    "                \n",
    "                if self.scalar is not None:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = self.model(\n",
    "                            input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids\n",
    "                        )\n",
    "                else:\n",
    "                    outputs = self.model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids\n",
    "                    )\n",
    "                \n",
    "                logits = outputs[0].detach().cpu().numpy().squeeze().tolist()\n",
    "                preds += logits\n",
    "        return preds\n",
    "\n",
    "def config(fold, model_name, load_model_path):\n",
    "    torch.manual_seed(2021)\n",
    "    torch.cuda.manual_seed(2021)\n",
    "    torch.cuda.manual_seed_all(2021)\n",
    "    \n",
    "    max_len = 250\n",
    "    batch_size = 8\n",
    "\n",
    "    model, tokenizer = make_model(\n",
    "        model_name=model_name, \n",
    "        num_labels=1\n",
    "    )\n",
    "    model.load_state_dict(\n",
    "        torch.load(f'{load_model_path}/model{fold}.bin')\n",
    "    )\n",
    "    test_loader = make_loader(\n",
    "        test, tokenizer, max_len=max_len,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    if torch.cuda.device_count() >= 1:\n",
    "        print('Model pushed to {} GPU(s), type {}.'.format(\n",
    "            torch.cuda.device_count(), \n",
    "            torch.cuda.get_device_name(0))\n",
    "        )\n",
    "        model = model.cuda() \n",
    "    else:\n",
    "        raise ValueError('CPU training is not supported')\n",
    "\n",
    "    # scaler = torch.cuda.amp.GradScaler()\n",
    "    scaler = None\n",
    "    return (\n",
    "        model, tokenizer, \n",
    "        test_loader, scaler\n",
    "    )\n",
    "\n",
    "def run(fold=0, model_name=None, load_model_path=None):\n",
    "    model, tokenizer, \\\n",
    "        test_loader, scaler = config(fold, model_name, load_model_path)\n",
    "    \n",
    "    import time\n",
    "\n",
    "    evaluator = Evaluator(model, scaler)\n",
    "\n",
    "    test_time_list = []\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    tic1 = time.time()\n",
    "\n",
    "    preds = evaluator.evaluate(test_loader, tokenizer)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    tic2 = time.time() \n",
    "    test_time_list.append(tic2 - tic1)\n",
    "    \n",
    "    del model, tokenizer, test_loader, scaler\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return preds\n",
    "\n",
    "#pred_df1 = pd.DataFrame()\n",
    "#pred_df2 = pd.DataFrame()\n",
    "#pred_df3 = pd.DataFrame()\n",
    "#for fold in tqdm(range(5)):\n",
    "    #pred_df1[f'fold{fold}'] = run(fold, '../input/roberta-base/', '../input/commonlit-roberta-base-i/')\n",
    "    #pred_df2[f'fold{fold+5}'] = run(fold, '../input/robertalarge/', '../input/roberta-large-itptfit/')\n",
    "    #pred_df3[f'fold{fold+10}'] = run(fold, '../input/robertalarge/', '../input/commonlit-roberta-large-ii/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "optical-product",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T06:10:34.367180Z",
     "iopub.status.busy": "2021-07-27T06:10:34.366140Z",
     "iopub.status.idle": "2021-07-27T06:10:34.368341Z",
     "shell.execute_reply": "2021-07-27T06:10:34.368741Z",
     "shell.execute_reply.started": "2021-07-27T05:58:39.957775Z"
    },
    "papermill": {
     "duration": 0.039885,
     "end_time": "2021-07-27T06:10:34.368870",
     "exception": false,
     "start_time": "2021-07-27T06:10:34.328985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pred_df1 = np.array(pred_df1)\n",
    "#pred_df2 = np.array(pred_df2)\n",
    "#pred_df3 = np.array(pred_df3)\n",
    "#model2_predictions = pred_df2.mean(axis=1)# + (pred_df1.mean(axis=1)*5) + (pred_df3.mean(axis=1) * 9)) / 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-chamber",
   "metadata": {
    "papermill": {
     "duration": 0.036862,
     "end_time": "2021-07-27T06:10:34.438589",
     "exception": false,
     "start_time": "2021-07-27T06:10:34.401727",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "thermal-court",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T06:10:34.513403Z",
     "iopub.status.busy": "2021-07-27T06:10:34.512653Z",
     "iopub.status.idle": "2021-07-27T06:10:35.523417Z",
     "shell.execute_reply": "2021-07-27T06:10:35.522507Z",
     "shell.execute_reply.started": "2021-07-27T05:58:39.965428Z"
    },
    "papermill": {
     "duration": 1.052231,
     "end_time": "2021-07-27T06:10:35.523577",
     "exception": false,
     "start_time": "2021-07-27T06:10:34.471346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numba import cuda \n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras import backend as K\n",
    "from transformers import RobertaTokenizer, TFRobertaModel\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from sklearn.model_selection import KFold,StratifiedKFold,train_test_split\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.layers import Input,LSTM,Bidirectional,Embedding,Dense, Conv1D, Dropout , MaxPool1D , MaxPooling1D, GlobalAveragePooling2D , GlobalAveragePooling1D , GlobalMaxPooling1D , concatenate , Flatten\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.models import Model,load_model,save_model , model_from_json\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau,ModelCheckpoint, EarlyStopping ,LearningRateScheduler\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from transformers import TFBertModel, BertTokenizerFast , BertTokenizer , RobertaTokenizerFast , TFRobertaModel , RobertaConfig , TFAutoModel , AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "quantitative-affect",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T06:10:35.632484Z",
     "iopub.status.busy": "2021-07-27T06:10:35.610388Z",
     "iopub.status.idle": "2021-07-27T06:14:32.598674Z",
     "shell.execute_reply": "2021-07-27T06:14:32.599854Z",
     "shell.execute_reply.started": "2021-07-27T05:58:40.908583Z"
    },
    "papermill": {
     "duration": 237.041941,
     "end_time": "2021-07-27T06:14:32.600082",
     "exception": false,
     "start_time": "2021-07-27T06:10:35.558141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization\n",
      "generating train and test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../input/huggingface-roberta-variants/roberta-base/roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/huggingface-roberta-variants/roberta-base/roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights\n",
      "Extracting Features from train data\n",
      "178/178 [==============================] - 34s 177ms/step\n",
      "Extracting Features from train data\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "loading weights\n",
      "Extracting Features from train data\n",
      "178/178 [==============================] - 34s 177ms/step\n",
      "Extracting Features from train data\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "loading weights\n",
      "Extracting Features from train data\n",
      "178/178 [==============================] - 35s 178ms/step\n",
      "Extracting Features from train data\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "loading weights\n",
      "Extracting Features from train data\n",
      "178/178 [==============================] - 34s 178ms/step\n",
      "Extracting Features from train data\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "loading weights\n",
      "Extracting Features from train data\n",
      "178/178 [==============================] - 35s 178ms/step\n",
      "Extracting Features from train data\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "***********predicting***********\n",
      "running iteration 1\n",
      "Fold 1 , rmse score: 0.43823865620064634\n",
      "running iteration 2\n",
      "Fold 2 , rmse score: 0.4224735057234672\n",
      "running iteration 3\n",
      "Fold 3 , rmse score: 0.4252029095120738\n",
      "running iteration 4\n",
      "Fold 4 , rmse score: 0.453694748774298\n",
      "running iteration 5\n",
      "Fold 5 , rmse score: 0.4039424727361166\n",
      "the average rmse is 0.42871045858932033\n",
      "***********predicting***********\n",
      "running iteration 1\n",
      "Fold 1 , rmse score: 0.3477601550995168\n",
      "running iteration 2\n",
      "Fold 2 , rmse score: 0.3448146631637942\n",
      "running iteration 3\n",
      "Fold 3 , rmse score: 0.34647520717666536\n",
      "running iteration 4\n",
      "Fold 4 , rmse score: 0.35213643913099674\n",
      "running iteration 5\n",
      "Fold 5 , rmse score: 0.33096691987949656\n",
      "the average rmse is 0.34443067689009393\n",
      "***********predicting***********\n",
      "running iteration 1\n",
      "Fold 1 , rmse score: 0.33444067056289306\n",
      "running iteration 2\n",
      "Fold 2 , rmse score: 0.3249573858762401\n",
      "running iteration 3\n",
      "Fold 3 , rmse score: 0.32974692712283726\n",
      "running iteration 4\n",
      "Fold 4 , rmse score: 0.34390115086600864\n",
      "running iteration 5\n",
      "Fold 5 , rmse score: 0.3243613397805638\n",
      "the average rmse is 0.3314814948417086\n",
      "***********predicting***********\n",
      "running iteration 1\n",
      "Fold 1 , rmse score: 0.4108145912825745\n",
      "running iteration 2\n",
      "Fold 2 , rmse score: 0.40084371435112404\n",
      "running iteration 3\n",
      "Fold 3 , rmse score: 0.3899026936726255\n",
      "running iteration 4\n",
      "Fold 4 , rmse score: 0.4206343882127504\n",
      "running iteration 5\n",
      "Fold 5 , rmse score: 0.3908282521710825\n",
      "the average rmse is 0.40260472793803137\n",
      "***********predicting***********\n",
      "running iteration 1\n",
      "Fold 1 , rmse score: 0.3750453155713853\n",
      "running iteration 2\n",
      "Fold 2 , rmse score: 0.3721358427470201\n",
      "running iteration 3\n",
      "Fold 3 , rmse score: 0.37372482503883436\n",
      "running iteration 4\n",
      "Fold 4 , rmse score: 0.3783391696959681\n",
      "running iteration 5\n",
      "Fold 5 , rmse score: 0.3573790820089833\n",
      "the average rmse is 0.37132484701243823\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu,True)\n",
    "        \n",
    "max_len = 250\n",
    "batch_size = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "MODEL=['bert-base-uncased' , 'roberta-base']\n",
    "\n",
    "model_name = MODEL[1]\n",
    "\n",
    "path=[\n",
    "    \"../input/commonlitreadabilityprize/sample_submission.csv\",\n",
    "    \"../input/commonlitreadabilityprize/test.csv\",\n",
    "    \"../input/commonlitreadabilityprize/train.csv\"\n",
    "]\n",
    "\n",
    "df_train = pd.read_csv(path[2])\n",
    "df_test = pd.read_csv(path[1])\n",
    "df_ss = pd.read_csv(path[0])\n",
    "                         \n",
    "df_train = df_train.drop(['url_legal','license','standard_error'],axis='columns')\n",
    "df_test = df_test.drop(['url_legal','license'],axis='columns')\n",
    "X= df_train['excerpt']\n",
    "y=df_train['target'].values\n",
    "\n",
    "X_test = df_test['excerpt']\n",
    "\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(\"../input/huggingface-roberta-variants/roberta-base/roberta-base\")\n",
    "\n",
    "print('tokenization')\n",
    "train_embeddings = tokenizer1(X.to_list(), truncation = True , padding = 'max_length' , max_length=max_len)\n",
    "test_embeddings = tokenizer1(X_test.to_list() , truncation = True , padding = 'max_length' , max_length = max_len)\n",
    "                         \n",
    "@tf.function\n",
    "def map_function(encodings):\n",
    "    input_ids = encodings['input_ids']\n",
    "    \n",
    "    return {'input_word_ids': input_ids}\n",
    "\n",
    "print(\"generating train and test\")    \n",
    "train = tf.data.Dataset.from_tensor_slices((train_embeddings))\n",
    "train = (\n",
    "            train\n",
    "            .map(map_function, num_parallel_calls=AUTOTUNE)\n",
    "            .batch(16)\n",
    "            .prefetch(AUTOTUNE)\n",
    "        )\n",
    "\n",
    "\n",
    "test = tf.data.Dataset.from_tensor_slices((test_embeddings))\n",
    "test = (\n",
    "        test\n",
    "        .map(map_function, num_parallel_calls = AUTOTUNE)\n",
    "        .batch(16)\n",
    "        .prefetch(AUTOTUNE)\n",
    "    )\n",
    "                         \n",
    "                         \n",
    "def build_roberta_base_model(max_len=max_len ):\n",
    "    \n",
    "    transformer = TFAutoModel.from_pretrained(\"../input/huggingface-roberta-variants/roberta-base/roberta-base\")\n",
    "    \n",
    "    input_word_ids = tf.keras.layers.Input(shape = (max_len, ), dtype = tf.int32, name = 'input_word_ids')\n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "    \n",
    "    # We only need the cls_token, resulting in a 2d array\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "    output = tf.keras.layers.Dense(1, activation = 'linear', dtype = 'float32')(cls_token)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs = [input_word_ids], outputs = output)\n",
    "    \n",
    "    return model\n",
    "                         \n",
    "ragnar_model = build_roberta_base_model()\n",
    "def feature_extractor(path):\n",
    "    print(\"loading weights\")\n",
    "    ragnar_model.load_weights(path)\n",
    "    x= ragnar_model.layers[-3].output\n",
    "    model = Model(inputs = ragnar_model.inputs , outputs = x)\n",
    "    return model\n",
    "                         \n",
    "def get_preds(model,train,test):\n",
    "    print(\"Extracting Features from train data\")\n",
    "    train_features = model.predict( train , verbose =1)\n",
    "    train_features = train_features.last_hidden_state\n",
    "    train_features = train_features[: , 0 , :]\n",
    "    print(\"Extracting Features from train data\")\n",
    "    test_features = model.predict( test , verbose =1)\n",
    "    test_features = test_features.last_hidden_state\n",
    "    test_features = test_features[: , 0 , :]\n",
    "    \n",
    "    return np.array(train_features , dtype= np.float16) , np.array(test_features , dtype= np.float16) \n",
    "                         \n",
    "#model weight paths\n",
    "paths=[\"../input/commonlit-readability-roberta-base/Roberta_Base_123_1.h5\",\n",
    "       \"../input/commonlit-readability-roberta-base/Roberta_Base_123_2.h5\",\n",
    "       \"../input/commonlit-readability-roberta-base/Roberta_Base_123_3.h5\",\n",
    "       \"../input/commonlit-readability-roberta-base/Roberta_Base_123_4.h5\",\n",
    "       \"../input/commonlit-readability-roberta-base/Roberta_Base_123_5.h5\"\n",
    "      ]\n",
    "                         \n",
    "#1\n",
    "extraction_model = feature_extractor(paths[0])\n",
    "train_embeddings1 , test_embeddings1 = get_preds(extraction_model , train , test)\n",
    "                         \n",
    "#2\n",
    "extraction_model = feature_extractor(paths[1])\n",
    "train_embeddings2 , test_embeddings2 = get_preds(extraction_model , train , test)\n",
    "                         \n",
    "#3\n",
    "extraction_model = feature_extractor(paths[2])\n",
    "train_embeddings3 , test_embeddings3 = get_preds(extraction_model , train , test)\n",
    "                         \n",
    "#4\n",
    "extraction_model = feature_extractor(paths[3])\n",
    "train_embeddings4 , test_embeddings4 = get_preds(extraction_model , train , test)\n",
    "                         \n",
    "#5\n",
    "extraction_model = feature_extractor(paths[4])\n",
    "train_embeddings5 , test_embeddings5 = get_preds(extraction_model , train , test)\n",
    "                         \n",
    "def get_preds(train_embeddings , test_embeddings):\n",
    "    scores=[]\n",
    "    kfold = KFold(n_splits=5, shuffle= True , random_state=2021)\n",
    "    iteration=1\n",
    "    preds = np.zeros((test_embeddings.shape[0]))\n",
    "    for train_idx, test_idx in kfold.split(train_embeddings,y):\n",
    "        print(f'running iteration {iteration}')\n",
    "        X_train = train_embeddings[train_idx]\n",
    "        X_test = train_embeddings[test_idx]\n",
    "        y_train = y[train_idx]\n",
    "        y_test = y[test_idx]\n",
    "\n",
    "        regression_model = Ridge()\n",
    "        \n",
    "        regression_model.fit(X_train,y_train)\n",
    "        y_pred = regression_model.predict(X_test)\n",
    "\n",
    "        score = np.sqrt(mse(y_pred,y_test))\n",
    "        scores.append(score)\n",
    "        print(f'Fold {iteration} , rmse score: {score}')\n",
    "        y_preds = regression_model.predict(test_embeddings)\n",
    "        y_preds=y_preds.reshape(-1)\n",
    "        preds+=y_preds  \n",
    "        iteration += 1\n",
    "\n",
    "    print(f\"the average rmse is {np.mean(scores)}\")\n",
    "    return np.array(preds)/5  \n",
    "                         \n",
    "                         \n",
    "print(\"***********predicting***********\")\n",
    "preds1 = get_preds(train_embeddings1,test_embeddings1)\n",
    "print(\"***********predicting***********\")\n",
    "preds2 = get_preds(train_embeddings2,test_embeddings2)\n",
    "print(\"***********predicting***********\")\n",
    "preds3 = get_preds(train_embeddings3,test_embeddings3)\n",
    "print(\"***********predicting***********\")\n",
    "preds4 = get_preds(train_embeddings4,test_embeddings4)\n",
    "print(\"***********predicting***********\")\n",
    "preds5 = get_preds(train_embeddings5,test_embeddings5)\n",
    "\n",
    "preds_ragnar=(preds1+preds2+preds3+preds4+preds5)/5\n",
    "#preds_ragnar = preds.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-strengthening",
   "metadata": {
    "papermill": {
     "duration": 0.270686,
     "end_time": "2021-07-27T06:14:33.321294",
     "exception": false,
     "start_time": "2021-07-27T06:14:33.050608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "pacific-undergraduate",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T06:14:33.868272Z",
     "iopub.status.busy": "2021-07-27T06:14:33.867394Z",
     "iopub.status.idle": "2021-07-27T06:14:33.870297Z",
     "shell.execute_reply": "2021-07-27T06:14:33.869881Z",
     "shell.execute_reply.started": "2021-07-27T06:02:43.565584Z"
    },
    "papermill": {
     "duration": 0.277938,
     "end_time": "2021-07-27T06:14:33.870404",
     "exception": false,
     "start_time": "2021-07-27T06:14:33.592466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model5_predictions * 0.6 + model1_predictions * 0.3 + preds_ragnar * 0.1 # model3_predictions * 0.3 + model2_predictions * 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "nuclear-columbus",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T06:14:34.417686Z",
     "iopub.status.busy": "2021-07-27T06:14:34.417083Z",
     "iopub.status.idle": "2021-07-27T06:14:34.422005Z",
     "shell.execute_reply": "2021-07-27T06:14:34.422405Z",
     "shell.execute_reply.started": "2021-07-27T06:03:54.923939Z"
    },
    "papermill": {
     "duration": 0.280657,
     "end_time": "2021-07-27T06:14:34.422550",
     "exception": false,
     "start_time": "2021-07-27T06:14:34.141893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.47250219, -0.43566774, -0.4106286 , -2.45019254, -1.84634897,\n",
       "       -1.24610696,  0.1523464 ])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "danish-exchange",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T06:14:34.978143Z",
     "iopub.status.busy": "2021-07-27T06:14:34.977433Z",
     "iopub.status.idle": "2021-07-27T06:14:34.981006Z",
     "shell.execute_reply": "2021-07-27T06:14:34.981405Z",
     "shell.execute_reply.started": "2021-07-27T06:04:03.358688Z"
    },
    "papermill": {
     "duration": 0.280303,
     "end_time": "2021-07-27T06:14:34.981549",
     "exception": false,
     "start_time": "2021-07-27T06:14:34.701246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.4725, -0.4357, -0.4106, -2.4502, -1.8463, -1.2461,  0.1523])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(predictions, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "tribal-builder",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T06:14:35.540979Z",
     "iopub.status.busy": "2021-07-27T06:14:35.540235Z",
     "iopub.status.idle": "2021-07-27T06:14:36.247126Z",
     "shell.execute_reply": "2021-07-27T06:14:36.246182Z",
     "shell.execute_reply.started": "2021-07-27T06:04:28.507203Z"
    },
    "papermill": {
     "duration": 0.990295,
     "end_time": "2021-07-27T06:14:36.247271",
     "exception": false,
     "start_time": "2021-07-27T06:14:35.256976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id  target\n",
      "0  c0f722661 -0.4725\n",
      "1  f0953f0a5 -0.4357\n",
      "2  0df072751 -0.4106\n",
      "3  04caf4e0c -2.4502\n",
      "4  0e63f8bea -1.8463\n",
      "5  12537fe78 -1.2461\n",
      "6  965e592c0  0.1523\n"
     ]
    }
   ],
   "source": [
    "submission_df.target = np.round(predictions,4)\n",
    "print(submission_df)\n",
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 556.885746,
   "end_time": "2021-07-27T06:14:39.786025",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-07-27T06:05:22.900279",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
