{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a04452f7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-07-15T08:24:05.428071Z",
     "iopub.status.busy": "2021-07-15T08:24:05.427350Z",
     "iopub.status.idle": "2021-07-15T08:24:12.271795Z",
     "shell.execute_reply": "2021-07-15T08:24:12.272245Z",
     "shell.execute_reply.started": "2021-07-14T10:07:21.002286Z"
    },
    "papermill": {
     "duration": 6.87744,
     "end_time": "2021-07-15T08:24:12.272566",
     "exception": false,
     "start_time": "2021-07-15T08:24:05.395126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup,get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abdf36c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T08:24:12.353001Z",
     "iopub.status.busy": "2021-07-15T08:24:12.352465Z",
     "iopub.status.idle": "2021-07-15T08:24:12.356406Z",
     "shell.execute_reply": "2021-07-15T08:24:12.355778Z",
     "shell.execute_reply.started": "2021-07-14T10:07:28.27969Z"
    },
    "papermill": {
     "duration": 0.068841,
     "end_time": "2021-07-15T08:24:12.356523",
     "exception": false,
     "start_time": "2021-07-15T08:24:12.287682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_FOLDS = 5\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 12\n",
    "MAX_LEN = 248\n",
    "SEED = 1000\n",
    "WORKERS = 4\n",
    "EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n",
    "ROBERTA_LARGE_PATH = \"../input/clrp-roberta-large/clrp_roberta_large\"\n",
    "TOKENIZER_LARGET_PATH = \"../input/clrp-roberta-large/clrp_roberta_large\"\n",
    "ROBERTA_PATH = \"/kaggle/input/roberta-base\"\n",
    "TOKENIZER_PATH = \"/kaggle/input/roberta-base\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6fba106",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T08:24:12.388823Z",
     "iopub.status.busy": "2021-07-15T08:24:12.388193Z",
     "iopub.status.idle": "2021-07-15T08:24:12.793268Z",
     "shell.execute_reply": "2021-07-15T08:24:12.792744Z",
     "shell.execute_reply.started": "2021-07-14T10:07:28.342585Z"
    },
    "papermill": {
     "duration": 0.421864,
     "end_time": "2021-07-15T08:24:12.793407",
     "exception": false,
     "start_time": "2021-07-15T08:24:12.371543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer_base = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "tokenizer_large = AutoTokenizer.from_pretrained(TOKENIZER_LARGET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03433aac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T08:24:12.827719Z",
     "iopub.status.busy": "2021-07-15T08:24:12.827131Z",
     "iopub.status.idle": "2021-07-15T08:24:12.831050Z",
     "shell.execute_reply": "2021-07-15T08:24:12.830639Z",
     "shell.execute_reply.started": "2021-07-14T10:07:28.795951Z"
    },
    "papermill": {
     "duration": 0.023506,
     "end_time": "2021-07-15T08:24:12.831157",
     "exception": false,
     "start_time": "2021-07-15T08:24:12.807651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_random_seed(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a02347b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T08:24:12.866130Z",
     "iopub.status.busy": "2021-07-15T08:24:12.865495Z",
     "iopub.status.idle": "2021-07-15T08:24:12.983128Z",
     "shell.execute_reply": "2021-07-15T08:24:12.982603Z",
     "shell.execute_reply.started": "2021-07-14T10:07:28.804328Z"
    },
    "papermill": {
     "duration": 0.138023,
     "end_time": "2021-07-15T08:24:12.983269",
     "exception": false,
     "start_time": "2021-07-15T08:24:12.845246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/train.csv\")\n",
    "\n",
    "# Remove incomplete entries if any.\n",
    "train_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n",
    "              inplace=True)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "test_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\n",
    "submission_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdeda64",
   "metadata": {
    "papermill": {
     "duration": 0.014511,
     "end_time": "2021-07-15T08:24:13.014619",
     "exception": false,
     "start_time": "2021-07-15T08:24:13.000108",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b573165",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T08:24:13.052065Z",
     "iopub.status.busy": "2021-07-15T08:24:13.051338Z",
     "iopub.status.idle": "2021-07-15T08:24:13.053977Z",
     "shell.execute_reply": "2021-07-15T08:24:13.053577Z",
     "shell.execute_reply.started": "2021-07-09T06:57:14.30942Z"
    },
    "papermill": {
     "duration": 0.024703,
     "end_time": "2021-07-15T08:24:13.054081",
     "exception": false,
     "start_time": "2021-07-15T08:24:13.029378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LitDataset(Dataset):\n",
    "    def __init__(self, df, inference_only=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.df = df        \n",
    "        self.inference_only = inference_only\n",
    "        self.text = df.excerpt.tolist()\n",
    "        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n",
    "        \n",
    "        if not self.inference_only:\n",
    "            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n",
    "    \n",
    "        self.encoded = tokenizer_base.batch_encode_plus(\n",
    "            self.text,\n",
    "            padding = 'max_length',            \n",
    "            max_length = MAX_LEN,\n",
    "            truncation = True,\n",
    "            return_attention_mask=True\n",
    "        )        \n",
    " \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):        \n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][index])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n",
    "        \n",
    "        if self.inference_only:\n",
    "            return (input_ids, attention_mask)            \n",
    "        else:\n",
    "            target = self.target[index]\n",
    "            return (input_ids, attention_mask, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e1d152",
   "metadata": {
    "papermill": {
     "duration": 0.016536,
     "end_time": "2021-07-15T08:24:13.085392",
     "exception": false,
     "start_time": "2021-07-15T08:24:13.068856",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model\n",
    "The model is inspired by the one from [Maunish](https://www.kaggle.com/maunish/clrp-roberta-svm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5d9fbd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T08:24:13.124800Z",
     "iopub.status.busy": "2021-07-15T08:24:13.124087Z",
     "iopub.status.idle": "2021-07-15T08:24:13.126824Z",
     "shell.execute_reply": "2021-07-15T08:24:13.126424Z",
     "shell.execute_reply.started": "2021-07-09T06:57:14.320364Z"
    },
    "papermill": {
     "duration": 0.025047,
     "end_time": "2021-07-15T08:24:13.126926",
     "exception": false,
     "start_time": "2021-07-15T08:24:13.101879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n",
    "        config.update({\"output_hidden_states\":True, \n",
    "                       \"hidden_dropout_prob\": 0.0,\n",
    "                       \"layer_norm_eps\": 1e-7})                       \n",
    "        \n",
    "        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n",
    "            \n",
    "        self.attention = nn.Sequential(            \n",
    "            nn.Linear(768, 512),            \n",
    "            nn.Tanh(),                       \n",
    "            nn.Linear(512, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )        \n",
    "\n",
    "        self.regressor = nn.Sequential(                        \n",
    "            nn.Linear(768, 1)                        \n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        roberta_output = self.roberta(input_ids=input_ids,\n",
    "                                      attention_mask=attention_mask)        \n",
    "\n",
    "        # There are a total of 13 layers of hidden states.\n",
    "        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n",
    "        # We take the hidden states from the last Roberta layer.\n",
    "        last_layer_hidden_states = roberta_output.hidden_states[-1]\n",
    "\n",
    "        # The number of cells is MAX_LEN.\n",
    "        # The size of the hidden state of each cell is 768 (for roberta-base).\n",
    "        # In order to condense hidden states of all cells to a context vector,\n",
    "        # we compute a weighted average of the hidden states of all cells.\n",
    "        # We compute the weight of each cell, using the attention neural network.\n",
    "        weights = self.attention(last_layer_hidden_states)\n",
    "                \n",
    "        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n",
    "        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n",
    "        # Now we compute context_vector as the weighted average.\n",
    "        # context_vector.shape is BATCH_SIZE x 768\n",
    "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n",
    "        \n",
    "        # Now we reduce the context vector to the prediction score.\n",
    "        return self.regressor(context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8706fac0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T08:24:13.163443Z",
     "iopub.status.busy": "2021-07-15T08:24:13.162789Z",
     "iopub.status.idle": "2021-07-15T08:24:13.166282Z",
     "shell.execute_reply": "2021-07-15T08:24:13.166733Z",
     "shell.execute_reply.started": "2021-07-09T06:57:14.335375Z"
    },
    "papermill": {
     "duration": 0.025001,
     "end_time": "2021-07-15T08:24:13.166866",
     "exception": false,
     "start_time": "2021-07-15T08:24:13.141865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LitModelBig(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(ROBERTA_LARGE_PATH)\n",
    "        config.update({\"output_hidden_states\":True, \n",
    "                       \"hidden_dropout_prob\": 0.0,\n",
    "                       \"layer_norm_eps\": 1e-7})                       \n",
    "        \n",
    "        self.roberta = AutoModel.from_pretrained(ROBERTA_LARGE_PATH, config=config)  \n",
    "            \n",
    "        self.attention = nn.Sequential(            \n",
    "            nn.Linear(1024, 512),            \n",
    "            nn.Tanh(),                       \n",
    "            nn.Linear(512, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )        \n",
    "\n",
    "        self.regressor = nn.Sequential(                        \n",
    "            nn.Linear(1024, 1)                        \n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        roberta_output = self.roberta(input_ids=input_ids,\n",
    "                                      attention_mask=attention_mask)        \n",
    "\n",
    "        # There are a total of 13 layers of hidden states.\n",
    "        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n",
    "        # We take the hidden states from the last Roberta layer.\n",
    "        last_layer_hidden_states = roberta_output.hidden_states[-1]\n",
    "\n",
    "        # The number of cells is MAX_LEN.\n",
    "        # The size of the hidden state of each cell is 768 (for roberta-base).\n",
    "        # In order to condense hidden states of all cells to a context vector,\n",
    "        # we compute a weighted average of the hidden states of all cells.\n",
    "        # We compute the weight of each cell, using the attention neural network.\n",
    "        weights = self.attention(last_layer_hidden_states)\n",
    "                \n",
    "        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n",
    "        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n",
    "        # Now we compute context_vector as the weighted average.\n",
    "        # context_vector.shape is BATCH_SIZE x 768\n",
    "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n",
    "        \n",
    "        # Now we reduce the context vector to the prediction score.\n",
    "        return self.regressor(context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "112fe85b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T08:24:13.210164Z",
     "iopub.status.busy": "2021-07-15T08:24:13.208494Z",
     "iopub.status.idle": "2021-07-15T08:24:13.211063Z",
     "shell.execute_reply": "2021-07-15T08:24:13.211588Z",
     "shell.execute_reply.started": "2021-07-09T06:57:14.347762Z"
    },
    "papermill": {
     "duration": 0.029287,
     "end_time": "2021-07-15T08:24:13.211715",
     "exception": false,
     "start_time": "2021-07-15T08:24:13.182428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_mse(model, data_loader, file_indx):\n",
    "    \"\"\"Evaluates the mean squared error of the |model| on |data_loader|\"\"\"\n",
    "    model.eval()            \n",
    "    mse_sum = 0\n",
    "    tempoof= pd.DataFrame(columns=['id','pred','target'])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for batch_num, (input_ids, attention_mask, target) in enumerate(data_loader):\n",
    "            file_ids_batch_start = (batch_num) * BATCH_SIZE\n",
    "            file_ids_batch_end = (batch_num) * BATCH_SIZE + BATCH_SIZE \n",
    "            input_ids = input_ids.to(DEVICE)\n",
    "\n",
    "            attention_mask = attention_mask.to(DEVICE)                        \n",
    "            target = target.to(DEVICE)\n",
    "            \n",
    "            pred = model(input_ids, attention_mask)                       \n",
    "            \n",
    "            numpy_preds = torch.reshape(pred, (-1,)).cpu()\n",
    "            numpy_target = torch.reshape(target, (-1,)).cpu()\n",
    "\n",
    "            \n",
    "            ids = train_df.loc[file_indx].id[file_ids_batch_start:file_ids_batch_end].values\n",
    "            preds = numpy_preds.cpu().detach().numpy()\n",
    "            targets = numpy_target.cpu().detach().numpy()\n",
    "            \n",
    "            tmp = pd.DataFrame({\"id\":ids, 'pred' : preds, 'targets': targets})\n",
    "            tempoof = pd.concat([tempoof,tmp])\n",
    "            mse_sum += nn.MSELoss(reduction=\"sum\")(pred.flatten(), target).item()\n",
    "            \n",
    "\n",
    "    return mse_sum / len(data_loader.dataset), tempoof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "588dbf98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T08:24:13.246908Z",
     "iopub.status.busy": "2021-07-15T08:24:13.246159Z",
     "iopub.status.idle": "2021-07-15T08:24:13.249578Z",
     "shell.execute_reply": "2021-07-15T08:24:13.249983Z",
     "shell.execute_reply.started": "2021-07-09T06:57:14.365118Z"
    },
    "papermill": {
     "duration": 0.02369,
     "end_time": "2021-07-15T08:24:13.250108",
     "exception": false,
     "start_time": "2021-07-15T08:24:13.226418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    result = np.zeros(len(data_loader.dataset))    \n",
    "    index = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n",
    "            input_ids = input_ids.to(DEVICE)\n",
    "            attention_mask = attention_mask.to(DEVICE)\n",
    "                        \n",
    "            pred = model(input_ids, attention_mask)                        \n",
    "\n",
    "            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n",
    "            index += pred.shape[0]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4314a4e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T08:24:13.287106Z",
     "iopub.status.busy": "2021-07-15T08:24:13.286199Z",
     "iopub.status.idle": "2021-07-15T08:24:13.288743Z",
     "shell.execute_reply": "2021-07-15T08:24:13.289164Z",
     "shell.execute_reply.started": "2021-07-09T06:57:14.377142Z"
    },
    "papermill": {
     "duration": 0.025136,
     "end_time": "2021-07-15T08:24:13.289296",
     "exception": false,
     "start_time": "2021-07-15T08:24:13.264160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "oof = pd.DataFrame(columns=['id','target'])\n",
    "oof.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feaa51d",
   "metadata": {
    "papermill": {
     "duration": 0.016295,
     "end_time": "2021-07-15T08:24:13.322035",
     "exception": false,
     "start_time": "2021-07-15T08:24:13.305740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Commonlit-roberta-0467     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3704bf14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T08:24:13.359615Z",
     "iopub.status.busy": "2021-07-15T08:24:13.358969Z",
     "iopub.status.idle": "2021-07-15T08:25:44.739092Z",
     "shell.execute_reply": "2021-07-15T08:25:44.738584Z"
    },
    "papermill": {
     "duration": 91.402552,
     "end_time": "2021-07-15T08:25:44.739225",
     "exception": false,
     "start_time": "2021-07-15T08:24:13.336673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n",
      "\n",
      "Using ../input/commonlit-roberta-0467/model_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /kaggle/input/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 2/5\n",
      "\n",
      "Using ../input/commonlit-roberta-0467/model_2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /kaggle/input/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 3/5\n",
      "\n",
      "Using ../input/commonlit-roberta-0467/model_3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /kaggle/input/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 4/5\n",
      "\n",
      "Using ../input/commonlit-roberta-0467/model_4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /kaggle/input/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 5/5\n",
      "\n",
      "Using ../input/commonlit-roberta-0467/model_5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /kaggle/input/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "list_val_rmse = []\n",
    "\n",
    "kfold = KFold(n_splits=NUM_FOLDS, random_state=SEED, shuffle=True)\n",
    "\n",
    "model_name = 'roberta-0467'\n",
    "for fold, (train_indices, val_indices) in enumerate(kfold.split(train_df)):    \n",
    "    print(f\"\\nFold {fold + 1}/{NUM_FOLDS}\")\n",
    "\n",
    "    model_path = f\"../input/commonlit-roberta-0467/model_{fold + 1}.pth\"\n",
    "    \n",
    "    print(f\"\\nUsing {model_path}\")\n",
    "    \n",
    "    set_random_seed(SEED + fold)\n",
    "    \n",
    "    \n",
    "    train_dataset = LitDataset(train_df.loc[train_indices])  \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                              drop_last=True, shuffle=True, num_workers=WORKERS)\n",
    "    val_dataset = LitDataset(train_df.loc[val_indices])  \n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,drop_last=False, shuffle=False, num_workers=WORKERS)    \n",
    "    \n",
    "    \n",
    "    model = LitModel()\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))    \n",
    "    model.to(DEVICE)\n",
    "    \n",
    "\n",
    "    crnt_val_scor,oof_tmp = eval_mse(model, val_loader,val_indices)\n",
    "    val_rmse = math.sqrt(crnt_val_scor)\n",
    "    oof = pd.concat([oof,oof_tmp])\n",
    "    #oof.merge(oof_tmp, how='left',on='id')\n",
    "oof.to_csv('oof_score_roberta-0467.csv',index=False)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b839665",
   "metadata": {
    "papermill": {
     "duration": 0.016664,
     "end_time": "2021-07-15T08:25:44.773177",
     "exception": false,
     "start_time": "2021-07-15T08:25:44.756513",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### commonlit-roberta-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5309038",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T08:25:44.818964Z",
     "iopub.status.busy": "2021-07-15T08:25:44.818168Z",
     "iopub.status.idle": "2021-07-15T08:29:02.498410Z",
     "shell.execute_reply": "2021-07-15T08:29:02.497381Z"
    },
    "papermill": {
     "duration": 197.708883,
     "end_time": "2021-07-15T08:29:02.498574",
     "exception": false,
     "start_time": "2021-07-15T08:25:44.789691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n",
      "\n",
      "Using ../input/robertalarge/model_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 2/5\n",
      "\n",
      "Using ../input/robertalarge/model_2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 3/5\n",
      "\n",
      "Using ../input/robertalarge/model_3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 4/5\n",
      "\n",
      "Using ../input/robertalarge/model_4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 5/5\n",
      "\n",
      "Using ../input/robertalarge/model_5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "list_val_rmse = []\n",
    "\n",
    "kfold = KFold(n_splits=NUM_FOLDS, random_state=SEED, shuffle=True)\n",
    "oof = pd.DataFrame(columns=['id','target'])\n",
    "oof.set_index('id', inplace=True)\n",
    "model_name = 'roberta_large_self_trained'\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(kfold.split(train_df)):\n",
    "    print(f\"\\nFold {fold + 1}/{NUM_FOLDS}\")\n",
    "\n",
    "    model_path = f\"../input/robertalarge/model_{fold + 1}.pth\"\n",
    "    \n",
    "    print(f\"\\nUsing {model_path}\")\n",
    "    \n",
    "    set_random_seed(SEED + fold)\n",
    "    \n",
    "    \n",
    "    train_dataset = LitDataset(train_df.loc[train_indices])  \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                              drop_last=True, shuffle=True, num_workers=WORKERS)\n",
    "    val_dataset = LitDataset(train_df.loc[val_indices])  \n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,drop_last=False, shuffle=False, num_workers=WORKERS)    \n",
    "    \n",
    "    \n",
    "    model = LitModelBig()\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))    \n",
    "    model.to(DEVICE)\n",
    "    \n",
    "\n",
    "    crnt_val_scor,oof_tmp = eval_mse(model, val_loader,val_indices)\n",
    "    val_rmse = math.sqrt(crnt_val_scor)\n",
    "    \n",
    "    #oof.merge(oof_tmp, how='left',on='id')\n",
    "    oof = pd.concat([oof,oof_tmp])\n",
    "oof.to_csv('oof_score_roberta_large_self_trained.csv',index=False)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "732f246b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T08:29:02.581396Z",
     "iopub.status.busy": "2021-07-15T08:29:02.575569Z",
     "iopub.status.idle": "2021-07-15T08:29:02.589941Z",
     "shell.execute_reply": "2021-07-15T08:29:02.589528Z"
    },
    "papermill": {
     "duration": 0.069423,
     "end_time": "2021-07-15T08:29:02.590051",
     "exception": false,
     "start_time": "2021-07-15T08:29:02.520628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = train_df #test_df\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import (\n",
    "    Dataset, DataLoader, \n",
    "    SequentialSampler, RandomSampler\n",
    ")\n",
    "from transformers import RobertaConfig\n",
    "from transformers import (\n",
    "    get_cosine_schedule_with_warmup, \n",
    "    get_cosine_with_hard_restarts_schedule_with_warmup\n",
    ")\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import RobertaModel\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "def convert_examples_to_features(data, tokenizer, max_len, is_test=False):\n",
    "    data = data.replace('\\n', '')\n",
    "    tok = tokenizer.encode_plus(\n",
    "        data, \n",
    "        max_length=max_len, \n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=True\n",
    "    )\n",
    "    curr_sent = {}\n",
    "    padding_length = max_len - len(tok['input_ids'])\n",
    "    curr_sent['input_ids'] = tok['input_ids'] + ([0] * padding_length)\n",
    "    curr_sent['token_type_ids'] = tok['token_type_ids'] + \\\n",
    "        ([0] * padding_length)\n",
    "    curr_sent['attention_mask'] = tok['attention_mask'] + \\\n",
    "        ([0] * padding_length)\n",
    "    return curr_sent\n",
    "\n",
    "class DatasetRetriever(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len, is_test=False):\n",
    "        self.data = data\n",
    "        if 'excerpt' in self.data.columns:\n",
    "            self.excerpts = self.data.excerpt.values.tolist()\n",
    "        else:\n",
    "            self.excerpts = self.data.text.values.tolist()\n",
    "        self.targets = self.data.target.values.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_test = is_test\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        if not self.is_test:\n",
    "            excerpt, label = self.excerpts[item], self.targets[item]\n",
    "            features = convert_examples_to_features(\n",
    "                excerpt, self.tokenizer, \n",
    "                self.max_len, self.is_test\n",
    "            )\n",
    "            return {\n",
    "                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n",
    "                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n",
    "                'label':torch.tensor(label, dtype=torch.double),\n",
    "            }\n",
    "        else:\n",
    "            excerpt = self.excerpts[item]\n",
    "            features = convert_examples_to_features(\n",
    "                excerpt, self.tokenizer, \n",
    "                self.max_len, self.is_test\n",
    "            )\n",
    "            return {\n",
    "                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n",
    "                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n",
    "            }\n",
    "\n",
    "class CommonLitModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name, \n",
    "        config,  \n",
    "        multisample_dropout=False,\n",
    "        output_hidden_states=False\n",
    "    ):\n",
    "        super(CommonLitModel, self).__init__()\n",
    "        self.config = config\n",
    "        self.roberta = RobertaModel.from_pretrained(\n",
    "            model_name, \n",
    "            output_hidden_states=output_hidden_states\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size)\n",
    "        if multisample_dropout:\n",
    "            self.dropouts = nn.ModuleList([\n",
    "                nn.Dropout(0.5) for _ in range(5)\n",
    "            ])\n",
    "        else:\n",
    "            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n",
    "        self.regressor = nn.Linear(config.hidden_size, 1)\n",
    "        self._init_weights(self.layer_norm)\n",
    "        self._init_weights(self.regressor)\n",
    " \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    " \n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        labels=None\n",
    "    ):\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        sequence_output = outputs[1]\n",
    "        sequence_output = self.layer_norm(sequence_output)\n",
    " \n",
    "        # multi-sample dropout\n",
    "        for i, dropout in enumerate(self.dropouts):\n",
    "            if i == 0:\n",
    "                logits = self.regressor(dropout(sequence_output))\n",
    "            else:\n",
    "                logits += self.regressor(dropout(sequence_output))\n",
    "        \n",
    "        logits /= len(self.dropouts)\n",
    " \n",
    "        # calculate loss\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = torch.nn.MSELoss()\n",
    "            logits = logits.view(-1).to(labels.dtype)\n",
    "            loss = torch.sqrt(loss_fn(logits, labels.view(-1)))\n",
    "        \n",
    "        output = (logits,) + outputs[1:]\n",
    "        return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "def make_model(model_name, num_labels=1):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "    config = RobertaConfig.from_pretrained(model_name)\n",
    "    config.update({'num_labels':num_labels})\n",
    "    model = CommonLitModel(model_name, config=config)\n",
    "    return model, tokenizer\n",
    "\n",
    "def make_loader(data, tokenizer, max_len, batch_size,is_test):\n",
    "    \n",
    "    test_dataset = DatasetRetriever(data, tokenizer, max_len, is_test=is_test)\n",
    "    test_sampler = SequentialSampler(test_dataset)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        sampler=test_sampler, \n",
    "        pin_memory=False, \n",
    "        drop_last=False, \n",
    "        num_workers=WORKERS\n",
    "    )\n",
    "\n",
    "    return test_loader\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, model, scalar=None,files_ids=None):\n",
    "        self.model = model\n",
    "        self.scalar = scalar\n",
    "        self.files_ids = files_ids\n",
    "\n",
    "    def evaluate(self, data_loader, tokenizer):\n",
    "        preds = []\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        \n",
    "        tempoof= pd.DataFrame(columns=['id','pred','target'])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch_data in enumerate(data_loader):\n",
    "                \n",
    "                file_ids_batch_start = (batch_idx) * BATCH_SIZE\n",
    "                file_ids_batch_end = (batch_idx) * BATCH_SIZE + BATCH_SIZE \n",
    "                \n",
    "                input_ids, attention_mask, token_type_ids,labels = batch_data['input_ids'], \\\n",
    "                    batch_data['attention_mask'], batch_data['token_type_ids'], batch_data['label']\n",
    "                \n",
    "                input_ids, attention_mask, token_type_ids, labels = input_ids.cuda(), \\\n",
    "                    attention_mask.cuda(), token_type_ids.cuda(), labels.cuda()\n",
    "                \n",
    "                ids = train_df.loc[self.files_ids].id[file_ids_batch_start:file_ids_batch_end].values\n",
    "                \n",
    "                \n",
    "                if self.scalar is not None:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = self.model(\n",
    "                            input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            labels=labels\n",
    "                        )\n",
    "                else:\n",
    "                    outputs = self.model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids\n",
    "                    )\n",
    "                \n",
    "                logits = outputs[0].detach().cpu().numpy().squeeze().tolist()\n",
    "                \n",
    "                preds += logits\n",
    "\n",
    "                #numpy_preds = torch.reshape(preds, (-1,)).cpu()\n",
    "                numpy_target = torch.reshape(labels, (-1,)).cpu()\n",
    "                temp_preds = logits\n",
    "                targets = numpy_target.cpu().detach().numpy()\n",
    "                #print(\"IDS\", ids)\n",
    "                #print(\"Targets\", targets)\n",
    "                #print(\"Preds\",temp_preds)\n",
    "                \n",
    "                tmp = pd.DataFrame({\"id\":ids, 'pred' : temp_preds, 'target': targets})\n",
    "                tempoof = pd.concat([tempoof,tmp])\n",
    "        return preds, tempoof\n",
    "\n",
    "def config(fold, model_name, load_model_path):\n",
    "    \n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    \n",
    "    max_len = 250\n",
    "    batch_size = BATCH_SIZE\n",
    "\n",
    "    model, tokenizer = make_model(model_name=model_name, num_labels=1)\n",
    "    model.load_state_dict(\n",
    "        torch.load(f'{load_model_path}/model{fold}.bin')\n",
    "    )\n",
    "    test_loader = make_loader(\n",
    "        test, tokenizer, max_len=max_len,\n",
    "        batch_size=batch_size,\n",
    "        is_test=False\n",
    "    )\n",
    "\n",
    "    if torch.cuda.device_count() >= 1:\n",
    "        print('Model pushed to {} GPU(s), type {}.'.format(\n",
    "            torch.cuda.device_count(), \n",
    "            torch.cuda.get_device_name(0))\n",
    "        )\n",
    "        model = model.cuda() \n",
    "    else:\n",
    "        raise ValueError('CPU training is not supported')\n",
    "\n",
    "    # scaler = torch.cuda.amp.GradScaler()\n",
    "    scaler = None\n",
    "    return (\n",
    "        model, tokenizer, \n",
    "        test_loader, scaler\n",
    "    )\n",
    "\n",
    "def run(fold=0, model_name=None, load_model_path=None, files_ids=None):\n",
    "    \n",
    "    model, tokenizer, test_loader, scaler = config(fold, model_name, load_model_path)\n",
    "    \n",
    "    evaluator = Evaluator(model, scaler, files_ids)\n",
    "\n",
    "    test_time_list = []\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    tic1 = time.time()\n",
    "    \n",
    "    preds, tempoof = evaluator.evaluate(test_loader, tokenizer)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    tic2 = time.time() \n",
    "    test_time_list.append(tic2 - tic1)\n",
    "    \n",
    "    del model, tokenizer, test_loader, scaler\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return preds, tempoof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2aee36af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T08:29:02.647012Z",
     "iopub.status.busy": "2021-07-15T08:29:02.646050Z",
     "iopub.status.idle": "2021-07-15T08:36:06.189847Z",
     "shell.execute_reply": "2021-07-15T08:36:06.188813Z"
    },
    "papermill": {
     "duration": 423.578531,
     "end_time": "2021-07-15T08:36:06.190031",
     "exception": false,
     "start_time": "2021-07-15T08:29:02.611500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using commonlit-roberta-base-i\n",
      "\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/roberta-base/ were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "\n",
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/roberta-base/ were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "\n",
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/roberta-base/ were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "\n",
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/roberta-base/ were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "\n",
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/roberta-base/ were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Using roberta-large-itptfit\n",
      "\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large/ were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large/ and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "\n",
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large/ were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large/ and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "\n",
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large/ were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large/ and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "\n",
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large/ were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large/ and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "\n",
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large/ were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large/ and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Using commonlit-roberta-large-ii\n",
      "\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large/ were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large/ and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "\n",
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large/ were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large/ and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "\n",
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large/ were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large/ and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "\n",
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large/ were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large/ and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "\n",
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large/ were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/clrp-roberta-large/clrp_roberta_large/ and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=NUM_FOLDS, random_state=SEED, shuffle=True)\n",
    "\n",
    "oof = pd.DataFrame(columns=['id','target'])\n",
    "oof.set_index('id', inplace=True)\n",
    "\n",
    "model_names = ['commonlit-roberta-base-i','roberta-large-itptfit','commonlit-roberta-large-ii' ]\n",
    "print(\"Using\",model_names[0])\n",
    "for fold, (train_indices, val_indices) in enumerate(kfold.split(train_df)):\n",
    "    print(f\"\\nFold {fold + 1}/{NUM_FOLDS}\")\n",
    "    \n",
    "    set_random_seed(SEED + fold)\n",
    "    \n",
    "    test = train_df.loc[val_indices]\n",
    "\n",
    "    preds, tempoof = run(fold, '../input/roberta-base/', '../input/commonlit-roberta-base-i/',val_indices) \n",
    "    oof = pd.concat([oof,tempoof])\n",
    "\n",
    "oof.to_csv(f'oof_{model_names[0]}.csv',index=False)\n",
    "\n",
    "oof = pd.DataFrame(columns=['id','target'])\n",
    "oof.set_index('id', inplace=True)\n",
    "\n",
    "print(\"Using\",model_names[1])\n",
    "for fold, (train_indices, val_indices) in enumerate(kfold.split(train_df)):\n",
    "    print(f\"\\nFold {fold + 1}/{NUM_FOLDS}\")\n",
    "    \n",
    "    set_random_seed(SEED + fold)\n",
    "    \n",
    "    test = train_df.loc[val_indices]\n",
    "\n",
    "    preds, tempoof = run(fold, '../input/clrp-roberta-large/clrp_roberta_large/', '../input/roberta-large-itptfit/',val_indices) \n",
    "    oof = pd.concat([oof,tempoof])\n",
    "\n",
    "oof.to_csv(f'oof_{model_names[1]}.csv',index=False)\n",
    "\n",
    "oof = pd.DataFrame(columns=['id','target'])\n",
    "oof.set_index('id', inplace=True)\n",
    "\n",
    "print(\"Using\",model_names[2])\n",
    "for fold, (train_indices, val_indices) in enumerate(kfold.split(train_df)):\n",
    "    print(f\"\\nFold {fold + 1}/{NUM_FOLDS}\")\n",
    "    \n",
    "    set_random_seed(SEED + fold)\n",
    "    \n",
    "    test = train_df.loc[val_indices]\n",
    "\n",
    "    preds, tempoof = run(fold, '../input/clrp-roberta-large/clrp_roberta_large/', '../input/roberta-large-itptfit/',val_indices) \n",
    "    oof = pd.concat([oof,tempoof])\n",
    "\n",
    "oof.to_csv(f'oof_{model_names[2]}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cccb6e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T08:36:06.276824Z",
     "iopub.status.busy": "2021-07-15T08:36:06.276248Z",
     "iopub.status.idle": "2021-07-15T08:36:06.303846Z",
     "shell.execute_reply": "2021-07-15T08:36:06.304393Z",
     "shell.execute_reply.started": "2021-07-14T10:28:37.013285Z"
    },
    "papermill": {
     "duration": 0.075664,
     "end_time": "2021-07-15T08:36:06.304549",
     "exception": false,
     "start_time": "2021-07-15T08:36:06.228885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.054013</td>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>-1.149684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.247197</td>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>0.450214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.952325</td>\n",
       "      <td>0a43a07f1</td>\n",
       "      <td>-0.978375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.081337</td>\n",
       "      <td>c57b50918</td>\n",
       "      <td>-2.172103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.245806</td>\n",
       "      <td>587502a70</td>\n",
       "      <td>0.441140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.680656</td>\n",
       "      <td>7382b7a7a</td>\n",
       "      <td>-1.853938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.120458</td>\n",
       "      <td>9c5ff50d5</td>\n",
       "      <td>0.073447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.747775</td>\n",
       "      <td>25f93b2f6</td>\n",
       "      <td>0.577809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.189476</td>\n",
       "      <td>2c26db523</td>\n",
       "      <td>0.350102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.300779</td>\n",
       "      <td>5b990ba77</td>\n",
       "      <td>0.413244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2833 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target         id      pred\n",
       "0  -1.054013  dd1000b26 -1.149684\n",
       "1   0.247197  37c1b32fb  0.450214\n",
       "2  -0.952325  0a43a07f1 -0.978375\n",
       "3  -3.081337  c57b50918 -2.172103\n",
       "4   0.245806  587502a70  0.441140\n",
       "..       ...        ...       ...\n",
       "9  -1.680656  7382b7a7a -1.853938\n",
       "10  0.120458  9c5ff50d5  0.073447\n",
       "11  0.747775  25f93b2f6  0.577809\n",
       "0   0.189476  2c26db523  0.350102\n",
       "1   0.300779  5b990ba77  0.413244\n",
       "\n",
       "[2833 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec7ad55c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T08:36:06.385818Z",
     "iopub.status.busy": "2021-07-15T08:36:06.384640Z",
     "iopub.status.idle": "2021-07-15T08:36:06.387027Z",
     "shell.execute_reply": "2021-07-15T08:36:06.387438Z",
     "shell.execute_reply.started": "2021-07-09T06:43:16.440878Z"
    },
    "papermill": {
     "duration": 0.04186,
     "end_time": "2021-07-15T08:36:06.387567",
     "exception": false,
     "start_time": "2021-07-15T08:36:06.345707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from sklearn.metrics import mean_squared_error\n",
    "#mean_squared_error(oof.targets, oof['roberta-0467'],squared=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 731.987983,
   "end_time": "2021-07-15T08:36:10.127686",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-07-15T08:23:58.139703",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
